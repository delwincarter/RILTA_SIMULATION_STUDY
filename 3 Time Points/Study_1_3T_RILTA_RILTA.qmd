---
title: "Study 1 (k = 2) RILTA Generated, RILTA Analyzed; Three Timepoints"
format:
  html:
    code-fold: true
editor: visual
author: "Delwin Carter"
page-layout: full
fig-format: svg
knitr:
  opts_chunk:
    out.width: "90%"
    fig.align: center
---

```{r, message=FALSE, warning=FALSE}

#| label: "load-libraries"
#| echo: true
#| message: false
#| warning: false


library(tidyverse)
library(glue)
library(MplusAutomation)
library(here)
library(gt)
library(janitor)
library(parallel)
library(tools)
library(stringr)
```

# Study 1 (k = 2): RILTA Generated, RILTA Analyzed

![](images/clipboard-3927155885.png){width="350"}

# Model 1:

### Conditions:

Sample Size: N = 500, 1000, 2000, and 4000

Transition logit (probability): TPs = 1.385 (.8), .85 (.7), .41 (.6), -.41 (.4), -.85 (.3) , and -1.385 (.2)

![](images/clipboard-4066126347.png){width="591"}

RI Loadings: lambda = 0, 1, 1.5, 2, 2.5, and 3

![](images/clipboard-3119439446.png){width="351"}

```{r}
#| label: "simulation-conditions"
#| echo: true
#| message: false
#| warning: false


p1 <- expand.grid(N = c(500, 1000, 2000, 4000),
TPs = c(1.385, .85, .41, -.41, -.85, -1.385),
lambda = c(0, 1, 1.5, 2, 2.5, 3))
       
# Display the matrix using gt
p1 %>%
  gt() %>%
  tab_header(
    title = "Simulation Conditions Matrix",
    subtitle = "Combinations of Sample Sizes, Transition Probabilities, and Mixtures"
  ) %>%
  cols_align(
    align = "center",
    columns = everything() # Centers all columns
  )
```

```{r,message=FALSE, warning=FALSE, eval = FALSE}

#| label: "rilta-rilta-simulation"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

library(parallel)
# Step 1: Create the cluster for parallel processing
num_cores <- detectCores() - 1  # Detect the number of available cores (minus 1)
cl <- makeCluster(num_cores, type = "PSOCK")  # Create the PSOCK cluster

rilta_rilta_func <- function(N, TPs, lambda) {
  
  RILTA_RILTA <- mplusObject(
    TITLE = glue("Generate RILTA_RILTA_N_{N}_TP = {TPs}_TH_1_lambda_{lambda}"),

    MONTECARLO =
      glue("NAMES = u11-u15 u21-u25 u31-u35;
      GENERATE = u11-u15 u21-u25 u31-u35(1);
      CATEGORICAL = u11-u15 u21-u25 u31-u35;
      GENCLASSES = c1(2) c2(2) c3(2);
      CLASSES = c1(2) c2(2) c3(2);
      NOBSERVATIONS = {N};
      SEED = 07252005;
      NREPS = 500;
      !!SAVE = repM1*.dat;
      RESULTS = RILTA_RILTA_N_{N}_TP_{TPs}_TH_1_lambda_{lambda}.csv;"),

    ANALYSIS =
      "TYPE = MIXTURE;
      algorithm = integration;
      processors = 24;
      starts = 50 10;
      logcriterion=0.00001;
      mconv=0.00001;",

    MODELPOPULATION = glue("	
        %OVERALL%

        [c1#1-c3#1*0];
        c2#1 on c1#1*{TPs};
        c3#1 on c2#1*0;
      	
          f by u11-u15*{lambda} (p1-p5)
               u21-u25*{lambda} (p1-p5)
               u31-u35*{lambda} (p1-p5);

        f@1;
        [f@0];
        
      MODEL POPULATION-c1:
        %c1#1%
     [u11$1*1 u12$1*1 u13$1*1 u14$1*1 u15$1*1] (p111-p115);

        %c1#2%
     [u11$1*-1 u12$1*-1 u13$1*-1 u14$1*-1 u15$1*-1] (p121-p125);

      MODEL POPULATION-c2:  
        %c2#1%
     [u21$1*1 u22$1*1 u23$1*1 u24$1*1 u25$1*1] (p111-p115);

        %c2#2%
     [u21$1*-1 u22$1*-1 u23$1*-1 u24$1*-1 u25$1*-1] (p121-p125);
     
      MODEL POPULATION-c3:  
        %c3#1%
     [u31$1*1 u32$1*1 u33$1*1 u34$1*1 u35$1*1] (p111-p115);

        %c3#2%
     [u31$1*-1 u32$1*-1 u33$1*-1 u34$1*-1 u35$1*-1] (p121-p125);
       "),
     

    MODEL =
      glue("	
        %OVERALL%
          [c1#1-c3#1*0](par1-par3);
        	c2#1 on c1#1*{TPs} (par11);
        	c3#1 on c2#1*0;
        	
          f by u11-u15*{lambda} (p1-p5)
               u21-u25*{lambda} (p1-p5)
               u31-u35*{lambda} (p1-p5);
               
      	f@1;
        [f@0];

     MODEL c1:
        %c1#1%
     [u11$1*1 u12$1*1 u13$1*1 u14$1*1 u15$1*1] (p111-p115);

        %c1#2%
     [u11$1*-1 u12$1*-1 u13$1*-1 u14$1*-1 u15$1*-1] (p121-p125);

    MODEL c2: 	
        %c2#1%
     [u21$1*1 u22$1*1 u23$1*1 u24$1*1 u25$1*1] (p111-p115);

        %c2#2%
     [u21$1*-1 u22$1*-1 u23$1*-1 u24$1*-1 u25$1*-1] (p121-p125);
     
    MODEL c3:  
        %c3#1%
     [u31$1*1 u32$1*1 u33$1*1 u34$1*1 u35$1*1] (p111-p115);

        %c3#2%
     [u31$1*-1 u32$1*-1 u33$1*-1 u34$1*-1 u35$1*-1] (p121-p125);
	      "),
      

    MODELCONSTRAINT =
      if (TPs == 1.385) {
        glue("
        New(
        trans11*.80 trans12*.20 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.65 prob22*.35);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
      } 
             else if (TPs == .85) {
        glue("
        New(
        trans11*.70 trans12*.30 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.60 prob22*.4);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
            else  if (TPs == .41) {
        glue("
        New(
        trans11*.60 trans12*.40 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.55 prob22*.45);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
             else if (TPs == -.41) {
        glue("
        New(
        trans11*.40 trans12*.60 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.45 prob22*.55);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
             else if (TPs == -.85) {
        glue("
        New(
        trans11*.30 trans12*.70 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.40 prob22*.60);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
        
        else if (TPs == -1.385) {
        glue("
         New(
        trans11*.20 trans12*.80 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.35 prob22*.65);

        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;")
      }
  )

  # Run Mplus model
  RILTA_RILTA_Model<- mplusModeler(RILTA_RILTA, 
                                   dataout = here("7. 3T RILTA GEN RILTA ANALYZED", glue("RILTA_RILTA_N_{N}_TP_{TPs}_TH_1_lambda_{lambda}.dat")),
                                   modelout = glue(here("7. 3T RILTA GEN RILTA ANALYZED", "RILTA_RILTA_N_{N}_TP_{TPs}_TH_1_lambda_{lambda}.inp")),
                                   check = TRUE, run = TRUE, hashfilename = FALSE)
return(RILTA_RILTA_Model)
}

# Step 3: Export necessary objects to the cluster
clusterExport(cl, c("rilta_rilta_func", "p1", "here", "glue", "mplusModeler", "mplusObject"))

# Ensure necessary libraries are loaded on each cluster node
clusterEvalQ(cl, {
  library(MplusAutomation)
  library(glue)
  library(here)
})

# Step 4: Run the simulation in parallel using the cluster
result_list <- parLapply(cl, 1:nrow(p1), function(i) {
  rilta_rilta_func(p1$N[i], p1$TPs[i],  p1$lambda[i])
})

# Step 5: Stop the cluster after the simulation
stopCluster(cl)



```

# CHECK FOR LABEL SWITCHING

### Step 1: Combine All CSV Files into One Data Frame

**Objective:** 

*Load all CSV files and combine them into a single data frame.*

```{r}
#| label: "combine-csv-files-parallel"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 1: Set the correct CSV directory
csv_directory <- here('3 Time Points', '8_3T_RILTA_GEN_RILTA_ANALYZED')

# Step 2: Source the child document
source(here('Child_Docs', 'data_scraping.R'))
```

### Step 2: Scrape Rows and Process Data

**Objective:** 

Extract data from the appropriate rows from each 9-row chunk and prepare the data for further processing.

```{r}
#| label: "scrape-rows-process-data-parallel"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 2: Process the data using the child script
source(here('Child_Docs', 'step2_3t_RILTA.R'))
```

### Step 3: Convert Logits to Probabilities and Add Actual (Population) Values

**Objective:** 

Convert the logits to probabilities and add the known actual values to each row.

```{r}
#| label: "convert-logits-to-probabilities"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 3 and 4: Process the data and return results
source(here('Child_Docs', 'step_3.R'))

# The objects `final_data_with_actuals` and `violators` should now be in the global environment
```

### **Step 5: Plot Random Sample of Violators for Visual Inspection**

#### **Objective**

*Generate plots of randomly sampled violators for visual inspection using parallel processing.*

```{r, eval=FALSE}
#| label: "plot-violators"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Set plot width and height
plot_width <- 8
plot_height <- 6

# Take a random sample of up to 250 violators (ensure not to exceed the total number of violators)
set.seed(123)  # For reproducibility
sample_size <- min(nrow(violators), 250)  # Handle cases where fewer than 250 violators exist
sampled_violators <- violators[sample(nrow(violators), sample_size), ]

# Define the function to create plots sequentially
plot_violator <- function(i) {
  row_data <- sampled_violators[i, ]
  
  # Extract the file name from the current row
  file_name <- row_data$FileName

  # Extract probability values for EC1 and EC2 (estimated probabilities) and AC1 and AC2 (actuals)
  estimated_probabilities <- c(
    as.numeric(row_data[c("Ec1u1", "Ec1u2", "Ec1u3", "Ec1u4", "Ec1u5")]),
    as.numeric(row_data[c("Ec2u1", "Ec2u2", "Ec2u3", "Ec2u4", "Ec2u5")])
  )
  
  actual_values <- c(
    as.numeric(row_data[c("Ac1u1", "Ac1u2", "Ac1u3", "Ac1u4", "Ac1u5")]),
    as.numeric(row_data[c("Ac2u1", "Ac2u2", "Ac2u3", "Ac2u4", "Ac2u5")])
  )
  
  # Create labels for the legend with actual values directly from the dataset
  labels <- c(
    paste0("EC1: (", round(row_data$Ec1u1, 3), ", ", round(row_data$Ec1u2, 3), ", ", round(row_data$Ec1u3, 3), ", ", round(row_data$Ec1u4, 3), ", ", round(row_data$Ec1u5, 3), ")"),
    paste0("EC2: (", round(row_data$Ec2u1, 3), ", ", round(row_data$Ec2u2, 3), ", ", round(row_data$Ec2u3, 3), ", ", round(row_data$Ec2u4, 3), ", ", round(row_data$Ec2u5, 3), ")"),
    paste0("AC1: (", round(row_data$Ac1u1, 3), ", ", round(row_data$Ac1u2, 3), ", ", round(row_data$Ac1u3, 3), ", ", round(row_data$Ac1u4, 3), ", ", round(row_data$Ac1u5, 3), ")"),
    paste0("AC2: (", round(row_data$Ac2u1, 3), ", ", round(row_data$Ac2u2, 3), ", ", round(row_data$Ac2u3, 3), ", ", round(row_data$Ac2u4, 3), ", ", round(row_data$Ac2u5, 3), ")")
  )

  # Step 6: Create a data frame for plotting
  plot_data <- data.frame(
    Items = rep(1:5, 4),
    Probabilities = c(estimated_probabilities, actual_values),
    Class = rep(labels, each = 5)
  )

  # Step 7: Create the plot with the file name in the title
  p <- ggplot(plot_data, aes(x = Items, y = Probabilities, color = Class, group = Class)) +
    geom_line(size = 1.2) +
    geom_point(size = 3) +
    labs(title = file_name, x = "Items", y = "Probabilities") +  # Only the file name in the title
    theme_minimal(base_size = 16) +
    theme(panel.background = element_rect(fill = "white"),
          plot.background = element_rect(fill = "white"),
          plot.title = element_text(size = 14, hjust = 0.5)) +  # Adjust title size and center
    scale_color_manual(values = c(
      "darkblue", "darkgreen",  # EC1 and EC2 (Estimated Probabilities)
      "lightblue", "lightgreen"  # AC1 and AC2 (Actual Values)
    ))

  ggsave(filename = file.path("z3t_rilta_rilta_violator_plots", paste0("violator_plot_", i, "_", file_name, ".png")),
         plot = p, width = plot_width, height = plot_height)
}

# Apply the function to generate plots sequentially (without parallelization)
invisible(lapply(1:sample_size, plot_violator))

```

### **Step 6: Summarize Violations**

#### **Objective**

*Calculate the percentage of violations for each file, handling missing values appropriately.*

```{r}

#| label: "summarize-errors-and-violations"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true


library(parallel)

# Step 1: Extract Errors, Warnings, and Replication Status
extract_errors_and_replications <- function(filepath) {
  lines <- readLines(filepath)
  results <- list()
  current_replication <- NULL

  # Extract requested and completed replications
  replication_info_line <- grep("Number of replications", lines, value = TRUE)
  requested_replications <- as.integer(str_extract(lines[which(grepl("Requested", lines))], "\\d+"))
  completed_replications <- as.integer(str_extract(lines[which(grepl("Completed", lines))], "\\d+"))

  for (line in lines) {
    replication_match <- str_match(line, "REPLICATION (\\d+):")
    if (!is.na(replication_match[1])) {
      current_replication <- as.integer(replication_match[2])
      results <- append(results, list(
        tibble(
          FileName = basename(filepath),
          Replication = current_replication,
          Message = "None",
          MessageType = "None",
          Requested = requested_replications,
          Completed = completed_replications
        )
      ))
    } else if (!is.null(current_replication) && (grepl("FIXED TO AVOID SINGULARITY", line, ignore.case = TRUE) || grepl("MAY NOT BE TRUSTWORTHY", line, ignore.case = TRUE))) {
      message_type <- ifelse(grepl("FIXED TO AVOID SINGULARITY", line, ignore.case = TRUE), "Singularity Fix", "Non-Positive Definite")
      results <- append(results, list(
        tibble(
          FileName = basename(filepath),
          Replication = current_replication,
          Message = str_trim(line),
          MessageType = message_type,
          Requested = requested_replications,
          Completed = completed_replications
        )
      ))
    }
  }

  if (length(results) == 0) {
    results <- list(tibble(
      FileName = basename(filepath),
      Replication = NA,
      Message = "No Errors or Warnings",
      MessageType = "None",
      Requested = requested_replications,
      Completed = completed_replications
    ))
  }

  bind_rows(results)
}



# The parallelized processing setup and the summarization steps follow the same structure as previously described,
# focusing on handling the adjustment to the error count per replication.


# Step 2: Parallelized Processing
output_folder <- "7. 3T RILTA GEN RILTA ANALYZED_REP"  # Updated folder name with _REP
file_list <- list.files(output_folder, pattern = "\\.out$", full.names = TRUE)
cluster_type <- ifelse(.Platform$OS.type == "windows", "PSOCK", "FORK")
num_cores <- detectCores() - 1  # Use all but one core
cl <- makeCluster(num_cores, type = cluster_type)

# Export necessary functions and libraries to the cluster
clusterExport(cl, c("extract_errors_and_replications", "readLines", "str_match", "str_extract", "basename", "tibble", "bind_rows", "grepl", "str_trim"))
clusterEvalQ(cl, {
  library(tidyverse)
  library(stringr)
})

# Step 3: Execute Parallel Processing
replication_details <- bind_rows(parLapply(cl, file_list, extract_errors_and_replications))
stopCluster(cl)

# Step 4: Calculate Replication Summary with Error Rate
replication_summary <- replication_details %>%
  group_by(FileName) %>%
  summarise(
    Requested = first(Requested),
    Completed = first(Completed),
    ErrorReplications = sum(MessageType != "None"),
    GoodReplications = Completed - ErrorReplications,
    ErrorRate = if_else(Completed > 0, (ErrorReplications / Completed) * 100, 0)
  ) %>%
  select(FileName, Requested, Completed, ErrorReplications, GoodReplications, ErrorRate)

# Step 5: Display the Table or Notify
if (any(replication_summary$ErrorReplications > 0)) {
  replication_summary_table <- replication_summary %>%
    gt() %>%
    tab_header(
      title = "Replication Summary",
      subtitle = paste0("Folder: ", output_folder)
    ) %>%
    fmt_number(columns = c("Requested", "Completed", "ErrorReplications", "GoodReplications", "ErrorRate"), decimals = 2) %>%
    cols_label(
      FileName = "File Name",
      Requested = "Requested Replications",
      Completed = "Completed Replications",
      ErrorReplications = "Replications with Errors",
      GoodReplications = "Good Replications",
      ErrorRate = "Error Rate (%)"
    ) %>%
    tab_options(
      table.font.size = "small",
      heading.title.font.size = "medium",
      heading.subtitle.font.size = "small"
    )

  # Display the table
  print(replication_summary_table)
} else {
  message("No errors detected. No table to display.")
}

```

#### Step 6: Part 1 Create Column Names from the Filename

```{r}

#| label: "create-column-names-from-filename"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Add new columns based on the information in the FileName and set factors
final_data_with_actuals <- final_data_with_actuals %>%
  mutate(
    # Extract the sample size (N) from the FileName with the correct values
    N = case_when(
      grepl("N_4000", FileName) ~ 4,  # Correct value for N_4000
      grepl("N_500", FileName) ~ 1,   
      grepl("N_1000", FileName) ~ 2,  
      grepl("N_2000", FileName) ~ 3,  
      TRUE ~ NA_integer_
    ),
    # Map the TPs from the FileName to the appropriate Population labels
    Population = case_when(
      grepl("TP_1.385", FileName) ~ ".800",   # Use ".800" instead of "0.800"
      grepl("TP_0.85", FileName) ~ ".700",    # Use ".700" instead of "0.700"
      grepl("TP_0.41", FileName) ~ ".600",    # Use ".600" instead of "0.600"
      grepl("TP_-0.41", FileName) ~ ".400",   # Use ".400" instead of "0.400"
      grepl("TP_-0.85", FileName) ~ ".300",   # Use ".300" instead of "0.300"
      grepl("TP_-1.385", FileName) ~ ".200",  # Use ".200" instead of "0.200"
      TRUE ~ NA_character_
    ),
Lambda = case_when(
  grepl("a_1$", FileName) ~ "1",        # Match 'a_1' at the end of the string
  grepl("a_1\\.5", FileName) ~ "1.5",   # Match 'a_1.5' (escaping the decimal point)
  grepl("a_2$", FileName) ~ "2",        # Match 'a_2' at the end of the string
  grepl("a_2\\.5", FileName) ~ "2.5",   # Match 'a_2.5' (escaping the decimal point)
  grepl("a_3$", FileName) ~ "3",        # Match 'a_3' at the end of the string
  grepl("a_0", FileName) ~ "0",         # Match 'a_0'
  TRUE ~ NA_character_
),


    # Create the Transitions variable based on Population values before Population is a factor
    Transitions = case_when(
      Population %in% c(".200", ".300", ".400") ~ 1,  # Assign 1 for Population .200, .300, .400
      Population %in% c(".600", ".700", ".800") ~ 2,  # Assign 2 for Population .600, .700, .800
      TRUE ~ NA_integer_
    )
  ) %>%
  # Convert columns to factors, ordering N_4000 first in the factor levels
  mutate(
    N = factor(N, levels = c(4, 1, 2, 3), labels = c("N = 4000", "N = 500", "N = 1000", "N = 2000")),
    Population = factor(Population, levels = c(".800", ".700", ".600", ".400", ".300", ".200")),
    Transitions = factor(Transitions, levels = c(1, 2), labels = c("Mover", "Stayer"))
  )

```

#### Step 6: Part 2 Calculate Violation Percentages per Condition

```{r}

#| label: "calculate-violation-percentages"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true


# Ensure no missing values in Any_Violation and create violation summary, including Population and Lambda
violation_summary <- final_data_with_actuals %>%
  mutate(Any_Violation = ifelse(is.na(Any_Violation), 0, Any_Violation)) %>%
  group_by(FileName, Population, N, Lambda) %>%  # Include Lambda in group_by
  summarize(
    Total_Rows = n(),  # Total rows per group
    Total_Violations = sum(Any_Violation, na.rm = TRUE),  # Total violations
    Percentage_Violations = (Total_Violations / Total_Rows) * 100  # Percentage of violations
  ) %>%
  ungroup() %>%
  select(-FileName)  # Remove FileName after ungrouping

# Ensure that all numeric columns are rounded and suppress scientific notation
options(scipen = 999)
violation_summary <- violation_summary %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)))

```

#### Step 6: Part 3 Summarize & Visualize Label Switching Percentage Results

```{r}
#| label: "summarize-violations"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Function to calculate the number of replications needed a priori, handling N and Population appropriately
calculate_needed_reps <- function(data) {
  data %>%
    # Convert N from character to numeric if it contains "N =" format
    mutate(
      N_numeric = as.numeric(gsub("N = ", "", N)),  # Remove "N = " prefix and convert to numeric
      Additional_Runs = (500 + Total_Violations) * (Percentage_Violations / 100),  # Correct calculation for additional runs
      Replications_Needed = ceiling(500 + Total_Violations + Additional_Runs + 20),  # Total replications needed + buffer of 20
      Replications_Needed = if_else(Replications_Needed < 500, 500, Replications_Needed)  # Ensure a minimum of 500 replications
    )
}

# Apply the updated function to calculate replications
violation_summary <- calculate_needed_reps(violation_summary)

# Ensure correct column names and assign TPs (logits) based on Population (for display and MplusAutomation)
violation_summary <- violation_summary %>%
  mutate(
    TPs = case_when(
      Population == ".800" ~ 1.385,
      Population == ".700" ~ 0.85,
      Population == ".600" ~ 0.41,
      Population == ".400" ~ -0.41,
      Population == ".300" ~ -0.85,
      Population == ".200" ~ -1.385,
      TRUE ~ NA_real_
    )
  ) %>%
  rename(
    `Transition Probability` = Population  # Rename Population to Transition Probability for display purposes
  )

# Now, create the table with formatted output
create_model_table <- function(data) {
  data %>%
    select(
      `Transition Probability`,  # Display transition probabilities
      TPs,  # Logit values for MplusAutomation
      `Sample Size` = N_numeric,  # Use the numeric version of N for clarity
      Lambda,  # Lambda column
      `Total Mplus Runs` = Total_Rows,  # Rename Total_Rows to Total Mplus Runs
      `Total Violations` = Total_Violations,  # Rename Total Violations to Total Violations
      `% of Violations` = Percentage_Violations,  # Rename Percentage Violations to % of Violations
      `Replications Needed` = Replications_Needed  # Add the column for replications needed
    ) %>%
    gt() %>%
    tab_header(
      title = "Monte Carlo Results:",
      subtitle = "Percentage of Cases with Label Switching and Replications Needed"
    ) %>%
    cols_align(
      align = "center",  # Center all columns
      columns = everything()
    ) %>%
    tab_options(
      data_row.padding = px(4)  # Set padding between rows
    ) %>%
    tab_style(
      style = cell_text(align = "center"),  # Center align the headers only
      locations = cells_column_labels(everything())  # Apply to headers only
    )
}

# Arrange the data by Transition Probability and Sample Size (numeric N)
violation_summary <- violation_summary %>%
  arrange(`Transition Probability`, N_numeric)

# Create and display the table
final_table <- create_model_table(violation_summary)

# Display the final table
final_table

```

#### Part 2

```{r,message=FALSE, warning=FALSE, eval = FALSE}

#| label: "rilta-rilta2-simulation"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true



  # Define the Mplus object with the dynamic replications
rilta_rilta_func <- function(N_numeric, TPs, Lambda, Replications_Needed) {
  
  RILTA_RILTA <- mplusObject(
    TITLE = glue("Generate RILTA_RILTA_N_{N_numeric}_TP_{TPs}_TH_1_Lambda_{Lambda}"),

    MONTECARLO =
      glue("NAMES = u11-u15 u21-u25 u31-u35;
      GENERATE = u11-u15 u21-u25 u31-u35(1);
      CATEGORICAL = u11-u15 u21-u25 u31-u35;
      GENCLASSES = c1(2) c2(2) c3(2);
      CLASSES = c1(2) c2(2) c3(2);
      NOBSERVATIONS = {N_numeric};
      SEED = 07252005;
      NREPS = {Replications_Needed};
      !!SAVE = repM1*.dat;
      RESULTS = RILTA_RILTA_N_{N_numeric}_TP_{TPs}_TH_1_Lambda_{Lambda}.csv;"),

    ANALYSIS =
      "TYPE = MIXTURE;
      algorithm = integration;
      processors = 24;
      starts = 50 10;
      logcriterion=0.00001;
      mconv=0.00001;",

    MODELPOPULATION = glue("	
        %OVERALL%

        [c1#1-c3#1*0];
        c2#1 on c1#1*{TPs};
        c3#1 on c2#1*0;
      	
          f by u11-u15*{Lambda} (p1-p5)
               u21-u25*{Lambda} (p1-p5)
               u31-u35*{Lambda} (p1-p5);

        f@1;
        [f@0];
        
      MODEL POPULATION-c1:
        %c1#1%
     [u11$1*1 u12$1*1 u13$1*1 u14$1*1 u15$1*1] (p111-p115);

        %c1#2%
     [u11$1*-1 u12$1*-1 u13$1*-1 u14$1*-1 u15$1*-1] (p121-p125);

      MODEL POPULATION-c2:  
        %c2#1%
     [u21$1*1 u22$1*1 u23$1*1 u24$1*1 u25$1*1] (p111-p115);

        %c2#2%
     [u21$1*-1 u22$1*-1 u23$1*-1 u24$1*-1 u25$1*-1] (p121-p125);
     
      MODEL POPULATION-c3:  
        %c3#1%
     [u31$1*1 u32$1*1 u33$1*1 u34$1*1 u35$1*1] (p111-p115);

        %c3#2%
     [u31$1*-1 u32$1*-1 u33$1*-1 u34$1*-1 u35$1*-1] (p121-p125);
       "),
     

    MODEL =
      glue("	
        %OVERALL%
          [c1#1-c3#1*0](par1-par3);
        	c2#1 on c1#1*{TPs} (par11);
        	c3#1 on c2#1*0;
        	
          f by u11-u15*{Lambda} (p1-p5)
               u21-u25*{Lambda} (p1-p5)
               u31-u35*{Lambda} (p1-p5);
               
      	f@1;
        [f@0];

     MODEL c1:
        %c1#1%
     [u11$1*1 u12$1*1 u13$1*1 u14$1*1 u15$1*1] (p111-p115);

        %c1#2%
     [u11$1*-1 u12$1*-1 u13$1*-1 u14$1*-1 u15$1*-1] (p121-p125);

    MODEL c2: 	
        %c2#1%
     [u21$1*1 u22$1*1 u23$1*1 u24$1*1 u25$1*1] (p111-p115);

        %c2#2%
     [u21$1*-1 u22$1*-1 u23$1*-1 u24$1*-1 u25$1*-1] (p121-p125);
     
    MODEL c3:  
        %c3#1%
     [u31$1*1 u32$1*1 u33$1*1 u34$1*1 u35$1*1] (p111-p115);

        %c3#2%
     [u31$1*-1 u32$1*-1 u33$1*-1 u34$1*-1 u35$1*-1] (p121-p125);
	      "),
      

    MODELCONSTRAINT =
      if (TPs == 1.385) {
        glue("
        New(
        trans11*.80 trans12*.20 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.65 prob22*.35);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
      } 
             else if (TPs == .85) {
        glue("
        New(
        trans11*.70 trans12*.30 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.60 prob22*.4);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
            else  if (TPs == .41) {
        glue("
        New(
        trans11*.60 trans12*.40 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.55 prob22*.45);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
             else if (TPs == -.41) {
        glue("
        New(
        trans11*.40 trans12*.60 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.45 prob22*.55);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
             else if (TPs == -.85) {
        glue("
        New(
        trans11*.30 trans12*.70 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.40 prob22*.60);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
        
        else if (TPs == -1.385) {
        glue("
         New(
        trans11*.20 trans12*.80 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.35 prob22*.65);

        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;")
      }
  )

  # Run Mplus model
  RILTA_RILTA_Model<- mplusModeler(RILTA_RILTA, 
                                   dataout = here("7. 3T RILTA GEN RILTA ANALYZED_REP", glue("RILTA_RILTA_N_{N_numeric}_TP_{TPs}_TH_1_Lambda_{Lambda}.dat")),
                                   modelout = glue(here("7. 3T RILTA GEN RILTA ANALYZED_REP", "RILTA_RILTA_N_{N_numeric}_TP_{TPs}_TH_1_Lambda_{Lambda}.inp")),
                                   check = TRUE, run = TRUE, hashfilename = FALSE)
return(RILTA_RILTA_Model)
}

# Start the cluster
num_cores <- detectCores() - 1

# Step 2: Select the cluster type based on the system (PSOCK for Windows, FORK for macOS/Linux)
cluster_type <- ifelse(.Platform$OS.type == "windows", "PSOCK", "FORK")


cl <- makeCluster(num_cores, type = cluster_type)

# Export necessary objects to the cluster
clusterExport(cl, c("rilta_rilta_func", "violation_summary", "here", "glue", "mplusModeler", "mplusObject"))

# Ensure required libraries are loaded on each node
clusterEvalQ(cl, {
  library(MplusAutomation)
  library(glue)
  library(here)
})

result_list <- parLapply(cl, 1:nrow(violation_summary), function(i) {
  rilta_rilta_func(
    violation_summary$N_numeric[i], 
    violation_summary$TPs[i],  
    violation_summary$Lambda[i], 
    violation_summary$Replications_Needed[i]
  )
})

# Stop the cluster after the simulation
stopCluster(cl)



```

# CHECK FOR LABEL SWITCHING

### Step 1: Combine All CSV Files into One Data Frame

**Objective:** 

*Load all CSV files and combine them into a single data frame.*

```{r}
#| label: "combine-csv-files-parallel2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 1: Set the correct CSV directory
csv_directory <- here('3 Time Points', '8_3T_RILTA_GEN_RILTA_ANALYZED_REP')

# Step 2: Source the child document
source(here('Child_Docs', 'data_scraping.R'))
```

### Step 2: Scrape Rows and Process Data

**Objective:** 

Extract data from the appropriate rows from each 9-row chunk and prepare the data for further processing.

```{r}
#| label: "scrape-rows-process-data-parallel2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 2: Process the data using the child script
source(here('Child_Docs', 'step2_3t_RILTA.R'))
```

### Step 3: Convert Logits to Probabilities and Add Actual (Population) Values

**Objective:** 

Convert the logits to probabilities and add the known actual values to each row.

```{r}
#| label: "convert-logits-to-probabilities2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 3 and 4: Process the data and return results
source(here('Child_Docs', 'step_3.R'))

# The objects `final_data_with_actuals` and `violators` should now be in the global environment
```

### **Step 6: Summarize Violations**

#### **Objective**

*Calculate the percentage of violations for each file, handling missing values appropriately.*

```{r}

#| label: "summarize-errors-and-violations2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true


library(parallel)

# Step 1: Extract Errors, Warnings, and Replication Status
extract_errors_and_replications <- function(filepath) {
  lines <- readLines(filepath)
  results <- list()
  current_replication <- NULL

  # Extract requested and completed replications
  replication_info_line <- grep("Number of replications", lines, value = TRUE)
  requested_replications <- as.integer(str_extract(lines[which(grepl("Requested", lines))], "\\d+"))
  completed_replications <- as.integer(str_extract(lines[which(grepl("Completed", lines))], "\\d+"))

  for (line in lines) {
    replication_match <- str_match(line, "REPLICATION (\\d+):")
    if (!is.na(replication_match[1])) {
      current_replication <- as.integer(replication_match[2])
      results <- append(results, list(
        tibble(
          FileName = basename(filepath),
          Replication = current_replication,
          Message = "None",
          MessageType = "None",
          Requested = requested_replications,
          Completed = completed_replications
        )
      ))
    } else if (!is.null(current_replication) && (grepl("FIXED TO AVOID SINGULARITY", line, ignore.case = TRUE) || grepl("MAY NOT BE TRUSTWORTHY", line, ignore.case = TRUE))) {
      message_type <- ifelse(grepl("FIXED TO AVOID SINGULARITY", line, ignore.case = TRUE), "Singularity Fix", "Non-Positive Definite")
      results <- append(results, list(
        tibble(
          FileName = basename(filepath),
          Replication = current_replication,
          Message = str_trim(line),
          MessageType = message_type,
          Requested = requested_replications,
          Completed = completed_replications
        )
      ))
    }
  }

  if (length(results) == 0) {
    results <- list(tibble(
      FileName = basename(filepath),
      Replication = NA,
      Message = "No Errors or Warnings",
      MessageType = "None",
      Requested = requested_replications,
      Completed = completed_replications
    ))
  }

  bind_rows(results)
}



# The parallelized processing setup and the summarization steps follow the same structure as previously described,
# focusing on handling the adjustment to the error count per replication.


# Step 2: Parallelized Processing
output_folder <- "7. 3T RILTA GEN RILTA ANALYZED_REP"  # Updated folder name with _REP
file_list <- list.files(output_folder, pattern = "\\.out$", full.names = TRUE)
cluster_type <- ifelse(.Platform$OS.type == "windows", "PSOCK", "FORK")
num_cores <- detectCores() - 1  # Use all but one core
cl <- makeCluster(num_cores, type = cluster_type)

# Export necessary functions and libraries to the cluster
clusterExport(cl, c("extract_errors_and_replications", "readLines", "str_match", "str_extract", "basename", "tibble", "bind_rows", "grepl", "str_trim"))
clusterEvalQ(cl, {
  library(tidyverse)
  library(stringr)
})

# Step 3: Execute Parallel Processing
replication_details <- bind_rows(parLapply(cl, file_list, extract_errors_and_replications))
stopCluster(cl)

# Step 4: Calculate Replication Summary with Error Rate
replication_summary <- replication_details %>%
  group_by(FileName) %>%
  summarise(
    Requested = first(Requested),
    Completed = first(Completed),
    ErrorReplications = sum(MessageType != "None"),
    GoodReplications = Completed - ErrorReplications,
    ErrorRate = if_else(Completed > 0, (ErrorReplications / Completed) * 100, 0)
  ) %>%
  select(FileName, Requested, Completed, ErrorReplications, GoodReplications, ErrorRate)

# Step 5: Display the Table or Notify
if (any(replication_summary$ErrorReplications > 0)) {
  replication_summary_table <- replication_summary %>%
    gt() %>%
    tab_header(
      title = "Replication Summary",
      subtitle = paste0("Folder: ", output_folder)
    ) %>%
    fmt_number(columns = c("Requested", "Completed", "ErrorReplications", "GoodReplications", "ErrorRate"), decimals = 2) %>%
    cols_label(
      FileName = "File Name",
      Requested = "Requested Replications",
      Completed = "Completed Replications",
      ErrorReplications = "Replications with Errors",
      GoodReplications = "Good Replications",
      ErrorRate = "Error Rate (%)"
    ) %>%
    tab_options(
      table.font.size = "small",
      heading.title.font.size = "medium",
      heading.subtitle.font.size = "small"
    )

  # Display the table
  print(replication_summary_table)
} else {
  message("No errors detected. No table to display.")
}

```

#### Step 6: Part 1 Create Column Names from the Filename

```{r}

#| label: "create-column-names-from-filename2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Add new columns based on the information in the FileName and set factors
final_data_with_actuals <- final_data_with_actuals %>%
  mutate(
    # Extract the sample size (N) from the FileName with the correct values
    N = case_when(
      grepl("N_4000", FileName) ~ 4,  # Correct value for N_4000
      grepl("N_500", FileName) ~ 1,   
      grepl("N_1000", FileName) ~ 2,  
      grepl("N_2000", FileName) ~ 3,  
      TRUE ~ NA_integer_
    ),
    # Map the TPs from the FileName to the appropriate Population labels
    Population = case_when(
      grepl("TP_1.385", FileName) ~ ".800",   # Use ".800" instead of "0.800"
      grepl("TP_0.85", FileName) ~ ".700",    # Use ".700" instead of "0.700"
      grepl("TP_0.41", FileName) ~ ".600",    # Use ".600" instead of "0.600"
      grepl("TP_-0.41", FileName) ~ ".400",   # Use ".400" instead of "0.400"
      grepl("TP_-0.85", FileName) ~ ".300",   # Use ".300" instead of "0.300"
      grepl("TP_-1.385", FileName) ~ ".200",  # Use ".200" instead of "0.200"
      TRUE ~ NA_character_
    ),
Lambda = case_when(
  grepl("a_1$", FileName) ~ "1",        # Match 'a_1' at the end of the string
  grepl("a_1\\.5", FileName) ~ "1.5",   # Match 'a_1.5' (escaping the decimal point)
  grepl("a_2$", FileName) ~ "2",        # Match 'a_2' at the end of the string
  grepl("a_2\\.5", FileName) ~ "2.5",   # Match 'a_2.5' (escaping the decimal point)
  grepl("a_3$", FileName) ~ "3",        # Match 'a_3' at the end of the string
  grepl("a_0", FileName) ~ "0",         # Match 'a_0'
  TRUE ~ NA_character_
),


    # Create the Transitions variable based on Population values before Population is a factor
    Transitions = case_when(
      Population %in% c(".200", ".300", ".400") ~ 1,  # Assign 1 for Population .200, .300, .400
      Population %in% c(".600", ".700", ".800") ~ 2,  # Assign 2 for Population .600, .700, .800
      TRUE ~ NA_integer_
    )
  ) %>%
  # Convert columns to factors, ordering N_4000 first in the factor levels
  mutate(
    N = factor(N, levels = c(4, 1, 2, 3), labels = c("N = 4000", "N = 500", "N = 1000", "N = 2000")),
    Population = factor(Population, levels = c(".800", ".700", ".600", ".400", ".300", ".200")),
    Transitions = factor(Transitions, levels = c(1, 2), labels = c("Mover", "Stayer"))
  )

```

#### Step 6: Part 2 Calculate Violation Percentages per Condition

```{r}

#| label: "calculate-violation-percentages2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true


# Ensure no missing values in Any_Violation and create violation summary, including Population and Lambda
violation_summary <- final_data_with_actuals %>%
  mutate(Any_Violation = ifelse(is.na(Any_Violation), 0, Any_Violation)) %>%
  group_by(FileName, Population, N, Lambda) %>%  # Include Lambda in group_by
  summarize(
    Total_Rows = n(),  # Total rows per group
    Total_Violations = sum(Any_Violation, na.rm = TRUE),  # Total violations
    Percentage_Violations = (Total_Violations / Total_Rows) * 100  # Percentage of violations
  ) %>%
  ungroup() %>%
  select(-FileName)  # Remove FileName after ungrouping

# Ensure that all numeric columns are rounded and suppress scientific notation
options(scipen = 999)
violation_summary <- violation_summary %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)))

```

#### Step 6: Part 3 Summarize & Visualize Label Switching Percentage Results

```{r}
#| label: "summarize-violations2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Function to calculate the number of replications needed a priori, handling N and Population appropriately
calculate_needed_reps <- function(data) {
  data %>%
    # Convert N from character to numeric if it contains "N =" format
    mutate(
      N_numeric = as.numeric(gsub("N = ", "", N)),  # Remove "N = " prefix and convert to numeric
      Additional_Runs = (500 + Total_Violations) * (Percentage_Violations / 100),  # Correct calculation for additional runs
      Replications_Needed = ceiling(500 + Total_Violations + Additional_Runs + 20),  # Total replications needed + buffer of 20
      Replications_Needed = if_else(Replications_Needed < 500, 500, Replications_Needed)  # Ensure a minimum of 500 replications
    )
}

# Apply the updated function to calculate replications
violation_summary <- calculate_needed_reps(violation_summary)

# Ensure correct column names and assign TPs (logits) based on Population (for display and MplusAutomation)
violation_summary <- violation_summary %>%
  mutate(
    TPs = case_when(
      Population == ".800" ~ 1.385,
      Population == ".700" ~ 0.85,
      Population == ".600" ~ 0.41,
      Population == ".400" ~ -0.41,
      Population == ".300" ~ -0.85,
      Population == ".200" ~ -1.385,
      TRUE ~ NA_real_
    )
  ) %>%
  rename(
    `Transition Probability` = Population  # Rename Population to Transition Probability for display purposes
  )

# Now, create the table with formatted output
create_model_table <- function(data) {
  data %>%
    select(
      `Transition Probability`,  # Display transition probabilities
      TPs,  # Logit values for MplusAutomation
      `Sample Size` = N_numeric,  # Use the numeric version of N for clarity
      Lambda,  # Lambda column
      `Total Mplus Runs` = Total_Rows,  # Rename Total_Rows to Total Mplus Runs
      `Total Violations` = Total_Violations,  # Rename Total Violations to Total Violations
      `% of Violations` = Percentage_Violations,  # Rename Percentage Violations to % of Violations
      `Replications Needed` = Replications_Needed  # Add the column for replications needed
    ) %>%
    gt() %>%
    tab_header(
      title = "Monte Carlo Results:",
      subtitle = "Percentage of Cases with Label Switching and Replications Needed"
    ) %>%
    cols_align(
      align = "center",  # Center all columns
      columns = everything()
    ) %>%
    tab_options(
      data_row.padding = px(4)  # Set padding between rows
    ) %>%
    tab_style(
      style = cell_text(align = "center"),  # Center align the headers only
      locations = cells_column_labels(everything())  # Apply to headers only
    )
}

# Arrange the data by Transition Probability and Sample Size (numeric N)
violation_summary <- violation_summary %>%
  arrange(`Transition Probability`, N_numeric)

# Create and display the table
final_table <- create_model_table(violation_summary)

# Display the final table
final_table

```

#### 

# **Step 7: Delete Cases that Violate**

#### **Objective**

*Filter out cases with any violations, leaving only the clean data.*

```{r}
#| label: "delete-cases"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 1: Filter out cases with any violations (Any_Violation == 0)
cleaned_data <- final_data_with_actuals[final_data_with_actuals$Any_Violation == 0, ]
```

### **Step 8: Compute Monte Carlo (MC) Values**

#### **Objective**

*Calculate Monte Carlo values for `TRANS11`, including population values, averages, standard errors, Mean Squared Error (MSE), coverage, and power.*

```{r}
#| label: "compute-mc-values"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

cleaned_data <- cleaned_data %>%
  mutate(Population = as.numeric(as.character(Population)))

# Calculate the Monte Carlo values, including Population (transition probability), N, 
# number of replications, Lambda, and averages for TRANS11 and SE11
mc_values <- cleaned_data %>%
  group_by(FileName, Population, N, Transitions, Lambda) %>%  # Include Lambda in the grouping
  summarize(
    average = round(mean(TRANS11, na.rm = TRUE), 3),
    average_SE = round(mean(SE_11, na.rm = TRUE), 3),
    population_sd = round(sd(TRANS11, na.rm = TRUE), 3),
    
    # MSE calculation: mean squared error between TRANS11 and Population
    MSE = round(mean((TRANS11 - Population)^2, na.rm = TRUE), 3),
    
    # Coverage calculation: check if Population lies within the confidence interval
    Coverage = round(mean((Population >= (TRANS11 - 1.96 * SE_11)) & (Population <= (TRANS11 + 1.96 * SE_11)), na.rm = TRUE), 3),
    
    # Power calculation: proportion of cases where TRANS11 is significant
    Power = round(mean(TRANS11 / SE_11 > 1.96, na.rm = TRUE), 3),
    
    # Reps_Used counts the number of replications (rows) used for each FileName
    Reps_Used = n()
  )

# Round the values to 3 decimal points
mc_values <- mc_values %>%
  mutate(across(starts_with("Avg_"), ~ round(.x, 3)))

```

### **Step 9: Calculate Dichotomous Variables and Bias**

#### **Objective**

*Calculate dichotomous variables for Power and Coverage, compute Parameter and SE Bias, and prepare subsets for movers and stayers.*

```{r}
#| label: "calculate-bias-dichotomous-variables"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 1: Calculate dichotomous variable for Power (1 if Power >= 0.8, else 0)
mc_values <- mc_values %>%
  mutate(Power_Dic = ifelse(Power >= 0.8, 1, 0))

# Step 2: Calculate dichotomous variable for Coverage (0 if outside [0.91, 0.98], else 1)
mc_values <- mc_values %>%
  mutate(Coverage_Dic = ifelse(Coverage > 0.98 | Coverage < 0.91, 0, 1))

# Step 3: Remove any groupings before further calculations
mc_values <- mc_values %>%
  ungroup()

# Step 4: Ensure numeric columns are correctly formatted and **convert Population only for calculations**
mc_values <- mc_values %>%
  mutate(
    # Keep average as numeric but **do not convert Population for display purposes**
    average = as.numeric(average),
    population_numeric = as.numeric(Population),  # Create a temporary numeric version of Population
    average_se = as.numeric(average_SE),
    population_sd = as.numeric(population_sd)
  )

# Step 5: Calculate Parameter Bias and SE Bias, rounding the results to 2 decimal places
mc_values <- mc_values %>%
  mutate(
    # Use population_numeric for the calculations, but **retain Population in the original format**
    Parameter_Bias = (average - population_numeric) / population_numeric * 100,  # Bias for the parameter
    SE_Bias = (average_se - population_sd) / population_sd * 100  # Bias for the standard error
  ) %>%
  mutate(across(c(Parameter_Bias, SE_Bias), ~ round(.x, 2)))  # Round to 2 decimal places

# Drop the temporary numeric population column if no longer needed
mc_values <- mc_values %>%
  select(-population_numeric)

```

### **Step 10: Subset Data for Movers and Stayers**

#### **Objective**

*Subset the Monte Carlo values data for transitions with movers and stayers based on the population value.*

```{r}

#| label: "subset-data-for-bias-plots"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true


# Assuming Population is numeric in all_data
all_data <- mc_values

# Convert N to a factor with the correct labels for plotting
all_data <- all_data %>%
  mutate(N = factor(N,
                    levels = c("N = 500", "N = 1000", "N = 2000", "N = 4000"),  # These are the existing labels
                    labels = c(`1` = "N = 500", 
                               `2` = "N = 1000", 
                               `3` = "N = 2000", 
                               `4` = "N = 4000")))

# Define the labels for N using expression() for italics, which will be used in plotting
n_labels <- c(
  `1` = expression(italic('N') ~ "= 500"),
  `2` = expression(italic('N') ~ "= 1000"),
  `3` = expression(italic('N') ~ "= 2000"),
  `4` = expression(italic('N') ~ "= 4000")
)

# Assign the labels to the levels
all_data$N <- factor(all_data$N, labels = n_labels)
# Now you can use `n_labels` in the plotting code


# Ensure that Population_Label is correctly prepared
all_data$Population_Label <- factor(all_data$Population, 
    levels = c(0.2, 0.3, 0.4, 0.6, 0.7, 0.8),  # Numeric levels without leading zeros
    labels = c(
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .200"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .300"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .400"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .600"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .700"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .800")
    )
)


# Subset for Transitions movers (already correctly defined as "Mover")
subset_mover <- subset(all_data, Transitions == "Mover")
subset_mover <- subset_mover %>%
  mutate(Lambda = as.numeric(as.character(Lambda)))

# Subset for Transitions stayers (already correctly defined as "Stayer")
subset_stayer <- subset(all_data, Transitions == "Stayer")
subset_stayer <- subset_stayer %>%
  mutate(Lambda = as.numeric(as.character(Lambda)))

```

### **Step 11: Combined Plot for Mover and Stayer**

#### **Objective**

*Create streamlined plots for both mover and stayer subsets using a common theme and labels.*

```{r}
#| label: "plot-bias"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Define common themes and aesthetics
common_theme <- theme_minimal() +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.minor.y = element_blank(),
    axis.text.x = element_text(size = 8),
    axis.ticks = element_line(color = "black", linewidth = 0.2),
    legend.position = "bottom",
    legend.title = element_blank(),
    text = element_text(family = "Times New Roman"),
    axis.title.x = element_text(margin = margin(t = 10, b = 10)),
    legend.margin = margin(t = -10),
    plot.caption = element_text(hjust = 0, margin = margin(t = 10))
  )

common_labels <- labs(
  x = "Lambda Loadings on the RI",
  y = "Bias (%)",
  color = "",
  title = ""
)


create_plot <- function(data, title_suffix) {
  # Detect which legend items to show
  present_categories <- c("Parameter Bias", "Standard Error Bias")  # Base categories
  if (any(data$Coverage_Dic == 0)) present_categories <- c(present_categories, "Coverage Failure")
  if (any(data$Power_Dic == 0)) present_categories <- c(present_categories, "Power Failure")

  # Define colors and shapes for different categories
  colors <- c("Parameter Bias" = "#4169E1", "Standard Error Bias" = "#E14169", 
              "Coverage Failure" = "#4169E1", "Power Failure" = "black")
  shapes <- c("Parameter Bias" = 16, "Standard Error Bias" = 18, 
              "Coverage Failure" = 1, "Power Failure" = 4)

  # Filter colors and shapes based on detected categories
  filtered_colors <- colors[present_categories]
  filtered_shapes <- shapes[present_categories]

  ggplot(data = data, aes(x = Lambda, y = Parameter_Bias, color = "Parameter Bias", group = Population_Label)) +  
    geom_line(aes(group = Population_Label), linewidth = 0.3, linetype = "solid") +  
    geom_line(aes(y = SE_Bias, group = Population_Label, color = "Standard Error Bias"), linewidth = 0.3, linetype = "solid") +  
    geom_point(aes(y = Parameter_Bias), shape = 16, size = 1, fill = "#4169E1", alpha = 0.8) +  
    geom_point(aes(y = SE_Bias, color = "Standard Error Bias"), shape = 18, size = 1, fill = "#E14169", alpha = 0.8) +  # Corrected
    geom_point(data = subset(data, Coverage_Dic == 0), aes(y = Parameter_Bias, color = "Coverage Failure"), shape = 1, size = 2, fill = "#4169E1", alpha = 1) +  
    geom_point(data = subset(data, Power_Dic == 0), aes(y = Parameter_Bias, color = "Power Failure"), shape = 4, size = 2, fill = "black", alpha = 1) + 
    scale_color_manual(
      values = filtered_colors, 
      labels = present_categories, 
      breaks = present_categories,
      guide = guide_legend(
        override.aes = list(shape = filtered_shapes)
      )
    ) +  
    labs(
      x = "Lambda Loadings on the RI",
      y = "Bias (%)",
      color = "",
      title = paste("RILTA Generated, LTA Analyzed with", title_suffix, "Transition Probabilities")
    ) +
    coord_cartesian(ylim = c(-40, 40)) +  
    facet_grid(Population_Label ~ N, scales = "free_x", labeller = label_parsed) +  
    scale_x_continuous(breaks = seq(0, 3, by = 0.5), labels = scales::number_format(accuracy = 0.1)) +  
    scale_y_continuous(breaks = seq(-40, 40, by = 10)) +  
    theme_minimal() +
    theme(
      panel.grid.major.x = element_blank(),
      panel.grid.minor.x = element_blank(),
      panel.grid.minor.y = element_blank(),
      axis.text.x = element_text(size = 8),
      axis.ticks = element_line(color = "black", linewidth = 0.2),
      legend.position = "bottom",
      legend.title = element_blank(),
      text = element_text(family = "Times New Roman"),
      axis.title.x = element_text(margin = margin(t = 10, b = 10)),
      legend.margin = margin(t = -10),
      plot.caption = element_text(hjust = 0, margin = margin(t = 10))
    ) +
    geom_hline(yintercept = c(-10, 10), linetype = "dashed", color = "#4169E1", linewidth = 0.3) +  
    geom_hline(yintercept = c(-5, 5), linetype = "dashed", color = "#E14169", linewidth = 0.3)
}

```

#### Plot figure with Mover Transition Probabilities (.200, .300, .400)

```{r}
# Create and print plot for Mover
plot_mover <- create_plot(subset_mover, "Mover")
#| column: screen
#| fig-format: svg
print(plot_mover)
```

#### Plot figure with Mover Transition Probablities (.200, .300, .400)

```{r}
# Create and print plot for Stayer
plot_stayer <- create_plot(subset_stayer, "Stayer")
#| column: screen
#| fig-format: svg
print(plot_stayer)
```

### **Step 12: Prepare Data for Heatmaps**

#### **Objective**

*Prepare data for heatmap creation by ensuring correct formatting for population values, and subsetting the data based on class proportions and sample sizes.*

```{r}

#| label: "prepare-data-for-heatmaps"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 1: Define a function to extract numeric values from the 'N' column
extract_numeric_from_N <- function(N) {
  # Use regex to extract the numeric part (e.g., from 'italic("N") ~ "= 1000"')
  as.numeric(gsub("[^0-9]", "", N))
}

# Step 2: Apply this function to both movers and stayers
movers <- all_data %>%
  filter(Transitions == "Mover") %>%
  mutate(N_numeric = extract_numeric_from_N(N)) %>%  # Create a new column with numeric N
  arrange(N_numeric, Population)

stayers <- all_data %>%
  filter(Transitions == "Stayer") %>%
  mutate(N_numeric = extract_numeric_from_N(N)) %>%  # Create a new column with numeric N
  arrange(N_numeric, Population)

# Step 3: Assign N_Label only once for each N within Movers and Stayers
movers <- movers %>%
  group_by(N) %>%
  mutate(N_Label = ifelse(row_number() == 2, as.character(N), "")) %>%
  ungroup()

stayers <- stayers %>%
  group_by(N) %>%
  mutate(N_Label = ifelse(row_number() == 2, as.character(N), "")) %>%
  ungroup()

# Step 4: Combine movers and stayers back into one dataset, keeping them separate by transitions
all_data_sorted <- bind_rows(movers, stayers)

# Step 5: Ensure Lambda is included and columns are factored
all_data_sorted <- all_data_sorted %>% 
  mutate(N_Label = factor(N, labels = c("N = 500", "N = 1000", "N = 2000", "N = 4000"))) %>% 
  mutate(Population = factor(Population,
                             labels = c(`.2` = ".200", `.3` = ".300", `.4` = ".400",
                                        `.6` = ".600", `.7` = ".700", `.8` = ".800")))

# Step 6: Select necessary columns, including Lambda
test_map <- select(all_data_sorted, N_Label, Population, average, Coverage, Power, Parameter_Bias, SE_Bias, Lambda)

# Step 7: Ordering the table based on the "Lambda" column
test_map <- test_map %>%
  arrange(as.numeric(Lambda))

# Define the population values as characters
population_values <- c(".200", ".300", ".400", ".600", ".700", ".800")

# Function to subset the data for a specific population value
subset_data <- function(data, pop_value) {
  subset <- data %>%
    filter(Population == pop_value)
  return(subset)
}

# Apply the function to each population value
subset_list <- lapply(population_values, function(x) subset_data(test_map, as.character(x)))

# Access the subsets for each population value
subset_02 <- subset_list[[1]]  # Subset for population value .200
subset_03 <- subset_list[[2]]  # Subset for population value .300
subset_04 <- subset_list[[3]]  # Subset for population value .400
subset_06 <- subset_list[[4]]  # Subset for population value .600
subset_07 <- subset_list[[5]]  # Subset for population value .700
subset_08 <- subset_list[[6]]  # Subset for population value .800

```

### **Step 13: Heatmap Creation and Rendering**

#### **Objective**

*Create heatmaps using the `gt` package and render each table separately for different subsets of the data.*

```{r}

#| label: "create-heatmap-function"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

create_table <- function(subset, transition_probability) {
  
  # Create the gt object and set initial formatting
  gt_table <- subset %>%
    gt() %>%
    opt_table_font(stack = "geometric-humanist") %>% 
    tab_header(
      title = paste("RILTA Generated & RILTA Analyzed with Transition Probability of", transition_probability)
    ) %>%
    cols_label(
      N_Label = "Sample Size",
      average = "Estimated<br>Probability",
      Coverage = "Coverage",
      Power = "Power",
      Parameter_Bias = "Parameter<br>Bias",
      SE_Bias = "Standard Error<br>Bias",
      .fn = md
    ) %>%
    tab_spanner(
      label = "Bias",
      columns = c("Parameter_Bias", "SE_Bias")) %>%
    tab_row_group(
      label = "Lambda RI Loading of 3 (λ)",  # Label for the first subgroup
      rows = c(21:24)  # Rows corresponding to the first subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 2.5 (λ)",  # Label for the second subgroup
      rows = c(17:20)  # Rows corresponding to the second subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 2 (λ)",  # Label for the third subgroup
      rows = c(13:16)  # Rows corresponding to the third subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 1.5 (λ)",  # Label for the fourth subgroup
      rows = c(9:12)  # Rows corresponding to the fourth subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 1 (λ)",  # Label for the fifth subgroup
      rows = c(5:8)  # Rows corresponding to the fifth subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 0 (λ)",  # Label for the sixth subgroup
      rows = c(1:4)  # Rows corresponding to the sixth subgroup
    ) %>%
      tab_style(
    style = cell_text(
      font = "bold italic"  # Apply bold and italic styling
    ),
      locations = cells_row_groups()  # Apply style to row subheaders
    ) %>% 
    fmt_number(columns = c("Parameter_Bias", "SE_Bias"), decimals = 2) %>%  
    fmt_number(columns = 3, decimals = 3) %>%
    tab_options(
      table_body.hlines.color = "white",
      table.border.top.color = "black",
      table.border.bottom.color = "black",
      table_body.border.bottom.color = "black",
      heading.border.bottom.color = "black",
      column_labels.border.top.color = "black",
      column_labels.border.bottom.color = "black",
      row_group.border.bottom.color = "black" ,
      row_group.border.top.color = "black" 
    ) %>%
    cols_align(
      align = c("center"),
      columns = everything()
    )

  # Check for violations in Parameter_Bias column
  if (any(!(subset$Parameter_Bias >= -9.99 & subset$Parameter_Bias <= 9.99), na.rm = TRUE)) {
    gt_table <- gt_table %>%
      data_color(
        columns = "Parameter_Bias", 
        rows = !(Parameter_Bias >= -9.99 & Parameter_Bias <= 9.99),
        method = "numeric",
        palette = c("white", "#5ABCEB"),  # Brighter shade of blue
        domain = c(-30, 30),
        apply_to = "fill"
      ) %>%
      tab_footnote(
        footnote = md("Blue indicates violation of *Parameter Bias*."),
        locations = cells_column_labels(columns = Parameter_Bias)
      )
  }

  # Check for violations in SE_Bias column
  if (any(!(subset$SE_Bias >= -4.99 & subset$SE_Bias <= 4.99), na.rm = TRUE)) {
    gt_table <- gt_table %>%
      data_color(
        columns = "SE_Bias", 
        rows = !(SE_Bias >= -4.99 & SE_Bias <= 4.99),
        method = "numeric",
        palette = c("white", "#E71012"),  # Brighter shade of red
        domain = c(-70, 70),
        apply_to = "fill"
      ) %>%
      tab_footnote(
        footnote = md("Red indicates violation of *Standard Error Bias*."),
        locations = cells_column_labels(columns = SE_Bias)
      )
  }

  # Check for violations in Coverage column
  if (any(subset$Coverage < 0.93 | subset$Coverage > 0.979, na.rm = TRUE)) {
    gt_table <- gt_table %>%
      data_color(
        columns = "Coverage", 
        rows = Coverage < 0.93 | Coverage > 0.979,
        method = "numeric",
        palette = c("#193006", "white"),  # Same shade of blue
        domain = c(0, 1),
        apply_to = "fill"
      ) %>%
      tab_footnote(
        footnote = md("Green indicates failure to achieve adequate *Coverage*."),
        locations = cells_column_labels(columns = Coverage)
      )
  }

  # Check for violations in Power column
  if (any(subset$Power < 0.8, na.rm = TRUE)) {
    gt_table <- gt_table %>%
      data_color(
        columns = "Power", 
        rows = Power < 0.8,
        method = "numeric",
        palette = c("#300049", "white"),  # Same shade of red
        domain = c(0, 1),
        apply_to = "fill"
      ) %>%
      tab_footnote(
        footnote = md("Purple indicates failure to achieve adequate *Power*."),
        locations = cells_column_labels(columns = Power)
      )
  }
  
  return(gt_table)
}

```

#### Render tables for each transition probability condition TABLE FOR TRANSITION PROBABILITIES OF .200

```{r, warning = FALSE}
subset_02_table <- create_table(subset_02, ".200")
subset_02_table
subset_02_table |>  tab_options(table.width = pct(65)) |> gtsave(here("z2t_r_r_heatmaps", "2T_R_R_.200.png"))
```

#### TABLE FOR TRANSITION PROBABILITIES OF .300

```{r, warning = FALSE}
subset_03_table <- create_table(subset_03, ".300")
subset_03_table
subset_03_table |>  tab_options(table.width = pct(65)) |> gtsave(here("z2t_r_r_heatmaps","2T_R_R_.300.png"))
```

#### TABLE FOR TRANSITION PROBABILITIES OF .400

```{r, warning = FALSE}
subset_04_table <- create_table(subset_04, ".400")
subset_04_table
subset_04_table |>  tab_options(table.width = pct(65)) |> gtsave(here("z2t_r_r_heatmaps","2T_R_R_.400.png"))
```

#### TABLE FOR TRANSITION PROBABILITIES OF .600

```{r, warning = FALSE}
subset_06_table <- create_table(subset_06, ".600")
subset_06_table
subset_06_table |>  tab_options(table.width = pct(65)) |> gtsave(here("z2t_r_r_heatmaps","2T_R_R_.600.png"))
```

#### TABLE FOR TRANSITION PROBABILITIES OF .700

```{r, warning = FALSE}
subset_07_table <- create_table(subset_07, ".700")
subset_07_table
subset_07_table |>  tab_options(table.width = pct(65)) |> gtsave(here("z2t_r_r_heatmaps","2T_R_R_.700.png"))
```

#### TABLE FOR TRANSITION PROBABILITIES OF .800

```{r, warning = FALSE}
subset_08_table <- create_table(subset_08, ".800")
subset_08_table
subset_08_table |>  tab_options(table.width = pct(65)) |> gtsave(here("z2t_r_r_heatmaps","2T_R_R_.800.png"))
```
