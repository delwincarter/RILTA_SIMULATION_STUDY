---
title: "Study 1 (k = 2) RILTA Generated, RILTA Analyzed"
format:
  html:
    code-fold: true
editor: visual
author: "Delwin Carter"
page-layout: full
fig-format: svg
knitr:
  opts_chunk:
    out.width: "90%"
    fig.align: center
---

```{r, message=FALSE, warning=FALSE}

#| label: "load-libraries"
#| echo: true
#| message: false
#| warning: false


library(tidyverse)
library(MplusAutomation)
library(here)
library(gt)
library(janitor)
library(glue)
library(ggtext)
library(rlang)
library(knitr)
library(parallel)
library(tools)


```

```{r}

#EMERGENCY READ IN FILE BECAUSE I WROTE OVER THE 1st ITERATION FILES BUT SAVED THE CONDITIONS I NEED TO RE RUN

violation_summary_df <- read.csv(here("violation_summary_df.csv"), stringsAsFactors = FALSE)
```

# PART1:

## Data Conditions

![](images/RILTA_RILTA.png){width="354"}

Sample Size: N = 500, 1000, 2000, and 4000

Transition logit (probability): TPs = 1.385 (.8), .85 (.7), .41 (.6), -.41 (.4), -.85 (.3) , and -1.385 (.2)

![](images/clipboard-3344253592.png){width="450"}

RI Loadings: lambda = 0, 1, 1.5, 2, 2.5, and 3

![](images/clipboard-3119439446.png){width="351"}

```{r}

#| label: "simulation-conditions"
#| echo: true
#| message: false
#| warning: false


#Create grid of conditions for iteration
p1 <- expand.grid(N = c(500, 1000, 2000, 4000),
TPs = c(1.385, .85, .41, -.41, -.85, -1.385),
lambda = c(0, 1, 1.5, 2, 2.5, 3))
       
# Display the matrix using gt
p1 %>%
  gt() %>%
  tab_header(
    title = "Simulation Conditions Matrix",
    subtitle = "Combinations of Sample Sizes, Transition Probabilities, and Mixtures"
  ) %>%
  cols_align(
    align = "center",
    columns = everything() # Centers all columns
  )
```

```{r,message=FALSE, warning=FALSE, eval = FALSE}

#| label: "rilta-rilta-simulation"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

library(parallel)
# Step 1: Create the cluster for parallel processing
num_cores <- detectCores() - 1  # Detect the number of available cores (minus 1)
cl <- makeCluster(num_cores, type = "PSOCK")  # Create the PSOCK cluster

# Step 2: Define the function for the simulation

#Run all models
rilta_rilta_func <- function(N, TPs, lambda) {
  
  RILTA_RILTA <- mplusObject(
    TITLE = glue("Generate RILTA_RILTA_N = {N}_TP = {TPs}_TH_1"),

    MONTECARLO =
      glue("NAMES = u11-u15 u21-u25;
      GENERATE = u11-u15 u21-u25(1);
      CATEGORICAL = u11-u15 u21-u25;
      GENCLASSES = c1(2) c2(2);
      CLASSES = c1(2) c2(2);
      NOBSERVATIONS = {N};
      SEED = 07252005;
      NREPS = 500;
      !!SAVE = repM1*.dat;
      RESULTS = RILTA_RILTA_N_{N}_TP_{TPs}_TH_1_lambda_{lambda}.csv;"),

    ANALYSIS =
      "TYPE = MIXTURE;
      algorithm = integration;
      processors = 24;
      starts=50 10;
      logcriterion=0.00001;
      mconv=0.00001;",

    MODELPOPULATION = glue("	
        %OVERALL%

       [c1#1-c2#1*0];
      	c2#1 on c1#1*{TPs};
      	
       f by u11-u15*{lambda} (p1-p5)
            u21-u25*{lambda} (p1-p5);
        f@1;
        [f@0];
        
      MODEL POPULATION-c1:
        %c1#1%
     [u11$1*1 u12$1*1 u13$1*1 u14$1*1 u15$1*1] (p111-p115);

        %c1#2%
     [u11$1*-1 u12$1*-1 u13$1*-1 u14$1*-1 u15$1*-1] (p121-p125);

      MODEL POPULATION-c2:  
        %c2#1%
     [u21$1*1 u22$1*1 u23$1*1 u24$1*1 u25$1*1] (p111-p115);

        %c2#2%
     [u21$1*-1 u22$1*-1 u23$1*-1 u24$1*-1 u25$1*-1] (p121-p125);
       "),
     
    MODEL =
      glue("	
        %OVERALL%
          [c1#1-c2#1*0](par1-par2);
        	c2#1 on c1#1*{TPs} (par11);
        	
       f by u11-u15*{lambda} (p1-p5)
            u21-u25*{lambda} (p1-p5);
      	f@1;
        [f@0];

     MODEL c1:
        %c1#1%
     [u11$1*1 u12$1*1 u13$1*1 u14$1*1 u15$1*1] (p111-p115);

        %c1#2%
     [u11$1*-1 u12$1*-1 u13$1*-1 u14$1*-1 u15$1*-1] (p121-p125);

    MODEL c2: 	
        %c2#1%
     [u21$1*1 u22$1*1 u23$1*1 u24$1*1 u25$1*1] (p111-p115);

        %c2#2%
     [u21$1*-1 u22$1*-1 u23$1*-1 u24$1*-1 u25$1*-1] (p121-p125);
	      "),
      
    MODELCONSTRAINT =
      if (TPs == 1.385) {
        glue("
        New(
        trans11*.80 trans12*.20 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.65 prob22*.35);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
      } 
             else if (TPs == .85) {
        glue("
        New(
        trans11*.70 trans12*.30 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.60 prob22*.4);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
            else  if (TPs == .41) {
        glue("
        New(
        trans11*.60 trans12*.40 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.55 prob22*.45);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
             else if (TPs == -.41) {
        glue("
        New(
        trans11*.40 trans12*.60 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.45 prob22*.55);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
             else if (TPs == -.85) {
        glue("
        New(
        trans11*.30 trans12*.70 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.40 prob22*.60);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
        
        else if (TPs == -1.385) {
        glue("
         New(
        trans11*.20 trans12*.80 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.35 prob22*.65);

        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;")
      }
  )

  # Run Mplus model
  RILTA_RILTA_Model<- mplusModeler(RILTA_RILTA, 
                                   dataout = here("3. 2T RILTA GEN RILTA ANALYZED", glue("RILTA_RILTA_N_{N}_TP_{TPs}_TH_1_lambda_{lambda}.dat")),
                                   modelout = glue(here("3. 2T RILTA GEN RILTA ANALYZED", "RILTA_RILTA_N_{N}_TP_{TPs}_TH_1_lambda_{lambda}.inp")),
                                   check = TRUE, run = TRUE, hashfilename = FALSE)
return(RILTA_RILTA_Model)
}

# Step 3: Export necessary objects to the cluster
clusterExport(cl, c("rilta_rilta_func", "p1", "here", "glue", "mplusModeler", "mplusObject"))

# Ensure necessary libraries are loaded on each cluster node
clusterEvalQ(cl, {
  library(MplusAutomation)
  library(glue)
  library(here)
})

# Step 4: Run the simulation in parallel using the cluster
result_list <- parLapply(cl, 1:nrow(p1), function(i) {
  rilta_rilta_func(p1$N[i], p1$TPs[i], p1$lambda[i])
})

# Step 5: Stop the cluster after the simulation
stopCluster(cl)


```

# CHECK FOR LABEL SWITCHING

### Step 1: Combine All CSV Files into One Data Frame

**Objective:** 

*Load all CSV files and combine them into a single data frame.*

```{r}
#| label: "combine-csv-files-parallel"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 1: Set the correct CSV directory
csv_directory <- here('Simulations', 'STUDY_1', '2 Time Points', '4_2T_RILTA_GEN_RILTA_ANALYZED')

# Step 2: Source the child document
source(here('Child_Docs', 'data_scraping.R'))
```

### Step 2: Scrape Rows and Process Data

**Objective:** 

Extract data from the appropriate rows from each 9-row chunk and prepare the data for further processing.

```{r}
#| label: "scrape-rows-process-data-parallel"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 2: Process the data using the child script
source(here('Child_Docs', 'step2_2t_RILTA.R'))
```

### Step 3: Convert Logits to Probabilities and Add Actual (Population) Values

**Objective:** 

Convert the logits to probabilities and add the known actual values to each row.

```{r}
#| label: "convert-logits-to-probabilities"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 3: Process the data and return results
source(here('Child_Docs', 'step_3.R'))

# The objects `final_data_with_actuals` and `violators` should now be in the global environment
```

### **Step 4: Plot Random Sample of Violators for Visual Inspection**

#### **Objective**

*Generate plots of randomly sampled violators for visual inspection using parallel processing.*

```{r, eval=FALSE}
#| label: "plot-violators"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Set plot width and height
plot_width <- 8
plot_height <- 6

# Take a random sample of up to 250 violators (ensure not to exceed the total number of violators)
set.seed(123)  # For reproducibility
sample_size <- min(nrow(violators), 250)  # Handle cases where fewer than 250 violators exist
sampled_violators <- violators[sample(nrow(violators), sample_size), ]

# Define the function to create plots sequentially
plot_violator <- function(i) {
  row_data <- sampled_violators[i, ]
  
  # Extract the file name from the current row
  file_name <- row_data$FileName

  # Extract probability values for EC1 and EC2 (estimated probabilities) and AC1 and AC2 (actuals)
  estimated_probabilities <- c(
    as.numeric(row_data[c("Ec1u1", "Ec1u2", "Ec1u3", "Ec1u4", "Ec1u5")]),
    as.numeric(row_data[c("Ec2u1", "Ec2u2", "Ec2u3", "Ec2u4", "Ec2u5")])
  )
  
  actual_values <- c(
    as.numeric(row_data[c("Ac1u1", "Ac1u2", "Ac1u3", "Ac1u4", "Ac1u5")]),
    as.numeric(row_data[c("Ac2u1", "Ac2u2", "Ac2u3", "Ac2u4", "Ac2u5")])
  )
  
  # Create labels for the legend with actual values directly from the dataset
  labels <- c(
    paste0("EC1: (", round(row_data$Ec1u1, 3), ", ", round(row_data$Ec1u2, 3), ", ", round(row_data$Ec1u3, 3), ", ", round(row_data$Ec1u4, 3), ", ", round(row_data$Ec1u5, 3), ")"),
    paste0("EC2: (", round(row_data$Ec2u1, 3), ", ", round(row_data$Ec2u2, 3), ", ", round(row_data$Ec2u3, 3), ", ", round(row_data$Ec2u4, 3), ", ", round(row_data$Ec2u5, 3), ")"),
    paste0("AC1: (", round(row_data$Ac1u1, 3), ", ", round(row_data$Ac1u2, 3), ", ", round(row_data$Ac1u3, 3), ", ", round(row_data$Ac1u4, 3), ", ", round(row_data$Ac1u5, 3), ")"),
    paste0("AC2: (", round(row_data$Ac2u1, 3), ", ", round(row_data$Ac2u2, 3), ", ", round(row_data$Ac2u3, 3), ", ", round(row_data$Ac2u4, 3), ", ", round(row_data$Ac2u5, 3), ")")
  )

  # Step 6: Create a data frame for plotting
  plot_data <- data.frame(
    Items = rep(1:5, 4),
    Probabilities = c(estimated_probabilities, actual_values),
    Class = rep(labels, each = 5)
  )

  # Step 7: Create the plot with the file name in the title
  p <- ggplot(plot_data, aes(x = Items, y = Probabilities, color = Class, group = Class)) +
    geom_line(size = 1.2) +
    geom_point(size = 3) +
    labs(title = file_name, x = "Items", y = "Probabilities") +  # Only the file name in the title
    theme_minimal(base_size = 16) +
    theme(panel.background = element_rect(fill = "white"),
          plot.background = element_rect(fill = "white"),
          plot.title = element_text(size = 14, hjust = 0.5)) +  # Adjust title size and center
    scale_color_manual(values = c(
      "darkblue", "darkgreen",  # EC1 and EC2 (Estimated Probabilities)
      "lightblue", "lightgreen"  # AC1 and AC2 (Actual Values)
    ))

  ggsave(filename = file.path("z2t_rilta_rilta_violator_plots", paste0("violator_plot_", i, "_", file_name, ".png")),
         plot = p, width = plot_width, height = plot_height)
}

# Apply the function to generate plots sequentially (without parallelization)
invisible(lapply(1:sample_size, plot_violator))

```

### **Step 5: Summarize Violations**

#### **Objective**

*Calculate the percentage of violations for each file, handling missing values appropriately.*

```{r chunk-name, message=FALSE, warning=FALSE}

#| label: "summarize-errors"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

library(parallel)

extract_errors_from_file <- function(filepath, total_replications) {
  lines <- readLines(filepath)
  results <- vector("list", total_replications)
  error_keywords <- c("NON-POSITIVE DEFINITE", "SADDLE")

  # Initialize results for every replication
  for (rep in 1:total_replications) {
    results[[rep]] <- tibble(
      FileName = basename(filepath),
      Replication = rep,
      Message = "None",
      MessageType = "None"
    )
  }

  current_replication <- NULL
  for (line in lines) {
    if (str_detect(line, "REPLICATION")) {
      current_replication <- as.integer(str_extract(line, "\\d+"))
    }

    if (!is.null(current_replication) && current_replication <= total_replications &&
        any(sapply(error_keywords, grepl, line, ignore.case = TRUE))) {
      results[[current_replication]] <- tibble(
        FileName = basename(filepath),
        Replication = current_replication,
        Message = str_trim(line),
        MessageType = "Error"
      )
    }
  }

  return(bind_rows(results))
}

# Step 2: Extract Completed Replications
extract_completed_replications <- function(filepath) {
  lines <- readLines(filepath)
  completed_line <- lines[grepl("Completed", lines, ignore.case = TRUE)]
  completed <- as.integer(str_match(completed_line, "Completed\\s+(\\d+)")[, 2])
  if (length(completed) == 0) completed <- 0
  tibble(FileName = basename(filepath), CompletedReplications = completed)
}


# Step 3: Extract Requested Replications
extract_requested_replications <- function(filepath) {
  lines <- readLines(filepath)
  requested_line <- lines[grepl("Requested", lines, ignore.case = TRUE)]
  requested <- as.integer(str_match(requested_line, "Requested\\s+(\\d+)")[, 2])
  if (length(requested) == 0) requested <- 0
  tibble(FileName = basename(filepath), RequestedReplications = requested)
}

calculate_replication_summary <- function(error_summary, completed_replications, requested_replications) {
  summary <- error_summary %>%
    group_by(FileName) %>%
    summarise(
      ErrorReplications = n_distinct(Replication[MessageType == "Error"]),
      .groups = "drop"
    )

  full_summary <- requested_replications %>%
    left_join(completed_replications, by = "FileName") %>%
    left_join(summary, by = "FileName") %>%
    mutate(
      ErrorReplications = coalesce(ErrorReplications, 0),
      GoodReplications = CompletedReplications - ErrorReplications,
      ErrorRate = if_else(CompletedReplications > 0, (ErrorReplications / CompletedReplications) * 100, 0)
    ) %>%
    select(FileName, RequestedReplications, CompletedReplications, ErrorReplications, GoodReplications, ErrorRate)

  full_summary
}

# Step 4: Parallelized Processing (Windows/Mac/Linux Compatible)
output_folder <- here('Simulations', 'STUDY_1', '2 Time Points', '4_2T_RILTA_GEN_RILTA_ANALYZED')  # Adjust to your folder path
file_list <- list.files(output_folder, pattern = "\\.out$", full.names = TRUE)

# Step 5: Detect OS and Set Up Cluster
cluster_type <- ifelse(.Platform$OS.type == "windows", "PSOCK", "FORK")
num_cores <- detectCores() - 1  # Use all but one core
cl <- makeCluster(num_cores, type = cluster_type)

# Export necessary libraries and functions to the cluster
invisible(clusterExport(cl, c("extract_errors_from_file", "extract_completed_replications", "extract_requested_replications")))
invisible(clusterEvalQ(cl, library(tidyverse)))

# Step 6: Parallel Processing
# Calculate completed replications first
completed_rep_list <- parLapply(cl, file_list, extract_completed_replications)

# Extract errors while passing the total number of completed replications to the function
error_summary <- bind_rows(mapply(function(filepath, completed_data) {
  extract_errors_from_file(filepath, completed_data$CompletedReplications)
}, file_list, completed_rep_list, SIMPLIFY = FALSE))

completed_replications <- bind_rows(parLapply(cl, file_list, extract_completed_replications))
requested_replications <- bind_rows(parLapply(cl, file_list, extract_requested_replications))

# Stop the cluster
stopCluster(cl)

# Step 7: Calculate Replication Summary
replication_summary <- calculate_replication_summary(error_summary, completed_replications, requested_replications)

# Step 8: Create and Display the Table with Error Rate
replication_summary_table <- replication_summary %>%
  gt() %>%
  tab_header(
    title = "Replication Summary",
    subtitle = paste0("Folder: ", output_folder)
  ) %>%
  fmt_number(columns = c(CompletedReplications, RequestedReplications, ErrorReplications, GoodReplications, ErrorRate), decimals = 2) %>%
  cols_label(
    FileName = "File Name",
    CompletedReplications = "Completed Replications",
    RequestedReplications = "Requested Replications",
    ErrorReplications = "Replications with Errors",
    GoodReplications = "Good Replications",
    ErrorRate = "Error Rate (%)"
  ) %>%
  tab_options(
    table.font.size = "small",
    heading.title.font.size = "medium",
    heading.subtitle.font.size = "small"
  )

# Display the table
replication_summary_table

completed_replications <- completed_replications %>%
  mutate(FileName = str_replace(FileName, "\\.out$", ""),
         FileName = tolower(FileName),
         FileName = str_trim(FileName))

error_summary <- error_summary %>%
  mutate(FileName = str_replace(FileName, "\\.out$", ""),
         FileName = tolower(FileName),
         FileName = str_trim(FileName))

final_data_with_actuals <- final_data_with_actuals %>%
  mutate(FileName = tolower(FileName),
         FileName = str_trim(FileName))

replication_summary <- replication_summary %>%
  mutate(FileName = str_replace(FileName, "\\.out$", ""),
         FileName = tolower(FileName),
         FileName = str_trim(FileName))

cat("Rows in final_data_with_actuals:", nrow(final_data_with_actuals), "\n")
# Output the final number of rows to confirm data handling
cat("Number of rows in error_summary: ", nrow(error_summary), "\n")
cat("Number of rows in replication_summary: ", nrow(replication_summary), "\n")

```

#### Step 5: Part 1 Merge errors with main file

```{r}
#| label: "merge-errors"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Ensure consistent column formats and remove .out and .csv extensions
error_summary <- error_summary %>%
  mutate(
    FileName = tolower(str_trim(gsub("\\.out$|\\.csv$", "", FileName))), # Remove extensions and standardize FileName
    Replication = as.character(Replication) # Convert Replication to character
  )

final_data_with_actuals <- final_data_with_actuals %>%
  mutate(
    FileName = tolower(str_trim(gsub("\\.out$|\\.csv$", "", FileName))), # Remove extensions and standardize FileName
    Replication = as.character(Replication) # Convert Replication to character
  )

# Add a new column to flag errors
error_summary <- error_summary %>%
  mutate(ErrorFlag = if_else(Message == "None", 0, 1))

# Merge ErrorFlag into final_data_with_actuals
final_data_with_actuals <- final_data_with_actuals %>%
  left_join(error_summary %>% select(FileName, Replication, ErrorFlag),
            by = c("FileName" = "FileName", "Replication" = "Replication"))
```

#### Step 5: Part 2 Create Column Names from the Filename

```{r}
#| label: "create-column-names-from-filename"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true


# Add new columns based on the information in the FileName
final_data_with_actuals <- final_data_with_actuals %>%
  mutate(
    N = case_when(
      grepl("n_4000", FileName) ~ 4000,
      grepl("n_500", FileName) ~ 500,
      grepl("n_1000", FileName) ~ 1000,
      grepl("n_2000", FileName) ~ 2000,
      TRUE ~ NA_integer_
    ),
    Population = case_when(
      grepl("tp_1.385", FileName) ~ ".800",
      grepl("tp_0.85", FileName) ~ ".700",
      grepl("tp_0.41", FileName) ~ ".600",
      grepl("tp_-0.41", FileName) ~ ".400",
      grepl("tp_-0.85", FileName) ~ ".300",
      grepl("tp_-1.385", FileName) ~ ".200",
      TRUE ~ NA_character_
    ),
    Lambda_values = case_when(
      grepl("a_1$", FileName) ~ "1",
      grepl("a_1\\.5", FileName) ~ "1.5",
      grepl("a_2$", FileName) ~ "2",
      grepl("a_2\\.5", FileName) ~ "2.5",
      grepl("a_3$", FileName) ~ "3",
      grepl("a_0", FileName) ~ "0",
      TRUE ~ NA_character_
    ),
    Transitions = case_when(
      Population %in% c(".200", ".300", ".400") ~ 1,
      Population %in% c(".600", ".700", ".800") ~ 2,
      TRUE ~ NA_integer_
    )
  ) %>%
  mutate(
    N = factor(N, levels = c(4000, 500, 1000, 2000), labels = c("N = 4000", "N = 500", "N = 1000", "N = 2000")),
    Population = factor(Population, levels = c(".800", ".700", ".600", ".400", ".300", ".200")),
    Transitions = factor(Transitions, levels = c(1, 2), labels = c("Mover", "Stayer"))
  )

```

#### Step 5: Part 3 Calculate Violation Percentages per Condition

```{r}

#| label: "calculate-violations"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 6 Part 2: Summarize Violations and Adjust for Errors

# 1. Summarize violations per condition
violation_summary <- final_data_with_actuals %>%
  mutate(
    Any_Violation = ifelse(is.na(Any_Violation), 0, Any_Violation),
    ErrorFlag = ifelse(is.na(ErrorFlag), 0, ErrorFlag)  # Ensure no missing values for ErrorFlag
  ) %>%
  group_by(FileName, Population, N, Lambda_values) %>%
  summarize(
    Total_Rows = n(),                                # Total runs
    Total_Violations = sum(Any_Violation, na.rm = TRUE), # Total violations
    Total_Errors = sum(ErrorFlag, na.rm = TRUE),         # Total errors from ErrorFlag
    Percentage_Violations = (Total_Violations / Total_Rows) * 100, # % violations
    .groups = "drop"
  ) %>%
  # 2. Calculate Replications Needed for label switching
  mutate(
    N_numeric = as.numeric(gsub("N = ", "", as.character(N))), 
    Additional_Runs = (500 + Total_Violations) * (Percentage_Violations / 100), 
    Replications_Needed = ceiling(500 + Total_Violations + Additional_Runs + 20),
    Replications_Needed = if_else(Replications_Needed < 500, 500, Replications_Needed),
    ErrorRate = Total_Errors / Total_Rows,  # Calculate ErrorRate directly
    Adjusted_Replications_Needed = ceiling(Replications_Needed / (1 - ErrorRate)),
    Adjusted_Replications_Needed = if_else(Adjusted_Replications_Needed < 500, 500, Adjusted_Replications_Needed)
  ) %>%
  select(
    FileName, Population, N, N_numeric,Lambda_values, Total_Rows, Total_Violations, Total_Errors, ErrorRate, Percentage_Violations, Replications_Needed,
    Adjusted_Replications_Needed
  )

```

#### Step 5: Part 3 Summarize & Visualize Label Switching Percentage Results

```{r}
#| label: "summarize-violations"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true


violation_summary <- violation_summary %>%
 mutate(
    TPs = case_when(
      Population == ".800" ~ 1.385,
      Population == ".700" ~ 0.85,
      Population == ".600" ~ 0.41,
      Population == ".400" ~ -0.41,
      Population == ".300" ~ -0.85,
      Population == ".200" ~ -1.385,
      TRUE ~ NA_real_
    ),
    # Clean `N_numeric` by stripping out "N = " and converting to numeric
    N_numeric = as.numeric(gsub("N = ", "", as.character(N)))
  )

# Summarize and visualize the final table
final_table <- violation_summary %>%
  select(
    `Transition Probability` = Population,               # Transition probabilities
    TPs,                                    # Logit values
    Lambda_values,                                 # Lambda values
    N_numeric,              # Cleaned sample size
    `Total Mplus Runs` = Total_Rows,        # Total runs
    Total_Violations,                       # Total violations
    `% of Violations` = Percentage_Violations, # Violation percentage
    ErrorRate,                              # Error rate
    Replications_Needed,                    # Replications needed
    `Adjusted Replications Needed` = Adjusted_Replications_Needed # Adjusted replications needed
  ) %>%
  gt() %>%
  tab_header(
    title = "Monte Carlo Results:",
    subtitle = "Percentage of Cases with Label Switching and Replications Needed"
  ) %>%
  cols_align(
    align = "center",  # Center all columns
    columns = everything()
  ) %>%
  fmt_number(
    columns = c(`N_numeric`, `Total Mplus Runs`, Total_Violations, `% of Violations`,
                ErrorRate, Replications_Needed, `Adjusted Replications Needed`),
    decimals = 2  # Format numbers with two decimal places
  ) %>%
  tab_options(
    data_row.padding = px(4)  # Set padding between rows
  ) %>%
  tab_style(
    style = cell_text(align = "center"),  # Center align the headers only
    locations = cells_column_labels(everything())  # Apply to headers only
  )

# Display the table
final_table



```

# PART 2: SIMULATION

#### Objective

Rerun Analysis *with the NUmber of replications needed to deal with the Error rates*

```{r}
#| label: "rilta-rilta2-simulation"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true



  # Define the Mplus object with the dynamic replications
rilta_rilta_func <- function(TPs, Lambda, N_numeric,  Adjusted_Replications_Needed) {
  
  
  RILTA_RILTA <- mplusObject(
    TITLE = glue("Generate RILTA_RILTA_N_{N_numeric}_TP_{TPs}_1_Lambda_{Lambda}"),

    MONTECARLO =
      glue("NAMES = u11-u15 u21-u25;
      GENERATE = u11-u15 u21-u25(1);
      CATEGORICAL = u11-u15 u21-u25;
      GENCLASSES = c1(2) c2(2);
      CLASSES = c1(2) c2(2);
      NOBSERVATIONS = {N_numeric};
      SEED = 07252005;
      NREPS = {Adjusted_Replications_Needed};  ! Dynamic number of Replications_Needed
      !!SAVE = repM1*.dat;
      RESULTS = RILTA_RILTA_N_{N_numeric}_TP_{TPs}_TH_1_Lambda_{Lambda}.csv;"),

    ANALYSIS =
      "TYPE = MIXTURE;
      algorithm = integration;
      processors = 24;
      starts=50 10;
      logcriterion=0.00001;
      mconv=0.00001;",

    MODELPOPULATION = glue("	
        %OVERALL%

       [c1#1-c2#1*0];
      	c2#1 on c1#1*{TPs};
      	
       f by u11-u15*{Lambda} (p1-p5)
            u21-u25*{Lambda} (p1-p5);
        f@1;
        [f@0];
        
      MODEL POPULATION-c1:
        %c1#1%
     [u11$1*1 u12$1*1 u13$1*1 u14$1*1 u15$1*1] (p111-p115);

        %c1#2%
     [u11$1*-1 u12$1*-1 u13$1*-1 u14$1*-1 u15$1*-1] (p121-p125);

      MODEL POPULATION-c2:  
        %c2#1%
     [u21$1*1 u22$1*1 u23$1*1 u24$1*1 u25$1*1] (p111-p115);

        %c2#2%
     [u21$1*-1 u22$1*-1 u23$1*-1 u24$1*-1 u25$1*-1] (p121-p125);
       "),
     
    MODEL =
      glue("	
        %OVERALL%
          [c1#1-c2#1*0](par1-par2);
        	c2#1 on c1#1*{TPs} (par11);
        	
       f by u11-u15*{Lambda} (p1-p5)
            u21-u25*{Lambda} (p1-p5);
      	f@1;
        [f@0];

     MODEL c1:
        %c1#1%
     [u11$1*1 u12$1*1 u13$1*1 u14$1*1 u15$1*1] (p111-p115);

        %c1#2%
     [u11$1*-1 u12$1*-1 u13$1*-1 u14$1*-1 u15$1*-1] (p121-p125);

    MODEL c2: 	
        %c2#1%
     [u21$1*1 u22$1*1 u23$1*1 u24$1*1 u25$1*1] (p111-p115);

        %c2#2%
     [u21$1*-1 u22$1*-1 u23$1*-1 u24$1*-1 u25$1*-1] (p121-p125);
	      "),
      
    MODELCONSTRAINT =
      if (TPs == 1.385) {
        glue("
        New(
        trans11*.80 trans12*.20 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.65 prob22*.35);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
      } 
             else if (TPs == 0.85) {
        glue("
        New(
        trans11*.70 trans12*.30 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.60 prob22*.4);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
            else  if (TPs == 0.41) {
        glue("
        New(
        trans11*.60 trans12*.40 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.55 prob22*.45);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
             else if (TPs == -0.410) {
        glue("
        New(
        trans11*.40 trans12*.60 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.45 prob22*.55);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
             else if (TPs == -0.85) {
        glue("
        New(
        trans11*.30 trans12*.70 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.40 prob22*.60);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
        
        else if (TPs == -1.385) {
        glue("
         New(
        trans11*.20 trans12*.80 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.35 prob22*.65);

        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;")
      }
  )

  # Run Mplus model
  RILTA_RILTA_Model<- mplusModeler(RILTA_RILTA, 
                                   dataout = here('Simulations', 'STUDY_1', '2 Time Points', '4_2T_RILTA_GEN_RILTA_ANALYZED_REP', glue("RILTA_RILTA_N_{N_numeric}_TP_{TPs}_TH_1_Lambda_{Lambda}.dat")),
                                   modelout = glue(here('Simulations', 'STUDY_1', '2 Time Points', '4_2T_RILTA_GEN_RILTA_ANALYZED_REP', "RILTA_RILTA_N_{N_numeric}_TP_{TPs}_TH_1_Lambda_{Lambda}.inp")),
                                   check = TRUE, run = TRUE, hashfilename = FALSE)
return(RILTA_RILTA_Model)
}

# Start the cluster
num_cores <- detectCores() - 1

# Step 2: Select the cluster type based on the system (PSOCK for Windows, FORK for macOS/Linux)
cluster_type <- ifelse(.Platform$OS.type == "windows", "PSOCK", "FORK")


cl <- makeCluster(num_cores, type = cluster_type)

# Export necessary objects to the cluster
invisible(clusterExport(cl, c("rilta_rilta_func", "violation_summary_df", "here", "glue", "mplusModeler", "mplusObject")))

# Ensure required libraries are loaded on each node
invisible(clusterEvalQ(cl, {
  library(MplusAutomation)
  library(glue)
  library(here)
}))

result_list <- parLapply(cl, 1:nrow(violation_summary_df), function(i) {
  rilta_rilta_func(
    violation_summary_df$TPs[i],  
    violation_summary_df$Lambda[i], 
    violation_summary_df$N_numeric[i], 
    violation_summary_df$Adjusted_Replications_Needed[i]
  )
})

# Stop the cluster after the simulation
stopCluster(cl)
```

# CHECK FOR LABEL SWITCHING

### Step 6: Combine All CSV Files into One Data Frame From the new data

**Objective:** 

*Load all CSV files and combine them into a single data frame.*

```{r}
#| label: "combine-csv-files-parallel2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 1: Set the correct CSV directory
csv_directory <- here('Simulations', 'STUDY_1', '2 Time Points', '4_2T_RILTA_GEN_RILTA_ANALYZED_REP')

# Step 2: Source the child document
source(here('Child_Docs', 'data_scraping.R'))
```

### Step 7: Scrape Rows and Process Data

**Objective:** 

Extract data from the appropriate rows from each 9-row chunk and prepare the data for further processing.

```{r}
#| label: "scrape-rows-process-data-parallel2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 2: Process the data using the child script
source(here('Child_Docs', 'step2_2t_LTA.R'))
```

### Step 8: Convert Logits to Probabilities and Add Actual (Population) Values

**Objective:** 

Convert the logits to probabilities and add the known actual values to each row.

```{r}
#| label: "convert-logits-to-probabilities2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 3: Process the data and return results
source(here('Child_Docs', 'step_3.R'))

# The objects `final_data_with_actuals` and `violators` should now be in the global environment
```

### **Step 9: Summarize Violations**

#### **Objective**

*Calculate the percentage of violations for each file, handling missing values appropriately.*

```{r chunk-name, message=FALSE, warning=FALSE}
#| label: "summarize-errors2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

library(parallel)

extract_errors_from_file <- function(filepath, total_replications) {
  lines <- readLines(filepath)
  results <- vector("list", total_replications)
  error_keywords <- c("NON-POSITIVE DEFINITE", "SADDLE")

  # Initialize results for every replication
  for (rep in 1:total_replications) {
    results[[rep]] <- tibble(
      FileName = basename(filepath),
      Replication = rep,
      Message = "None",
      MessageType = "None"
    )
  }

  current_replication <- NULL
  for (line in lines) {
    if (str_detect(line, "REPLICATION")) {
      current_replication <- as.integer(str_extract(line, "\\d+"))
    }

    if (!is.null(current_replication) && current_replication <= total_replications &&
        any(sapply(error_keywords, grepl, line, ignore.case = TRUE))) {
      results[[current_replication]] <- tibble(
        FileName = basename(filepath),
        Replication = current_replication,
        Message = str_trim(line),
        MessageType = "Error"
      )
    }
  }

  return(bind_rows(results))
}

# Step 2: Extract Completed Replications
extract_completed_replications <- function(filepath) {
  lines <- readLines(filepath)
  completed_line <- lines[grepl("Completed", lines, ignore.case = TRUE)]
  completed <- as.integer(str_match(completed_line, "Completed\\s+(\\d+)")[, 2])
  if (length(completed) == 0) completed <- 0
  tibble(FileName = basename(filepath), CompletedReplications = completed)
}

# Step 3: Extract Requested Replications
extract_requested_replications <- function(filepath) {
  lines <- readLines(filepath)
  requested_line <- lines[grepl("Requested", lines, ignore.case = TRUE)]
  requested <- as.integer(str_match(requested_line, "Requested\\s+(\\d+)")[, 2])
  if (length(requested) == 0) requested <- 0
  tibble(FileName = basename(filepath), RequestedReplications = requested)
}

calculate_replication_summary <- function(error_summary, completed_replications, requested_replications) {
  summary <- error_summary %>%
    group_by(FileName) %>%
    summarise(
      ErrorReplications = n_distinct(Replication[MessageType == "Error"]),
      .groups = "drop"
    )

  full_summary <- requested_replications %>%
    left_join(completed_replications, by = "FileName") %>%
    left_join(summary, by = "FileName") %>%
    mutate(
      ErrorReplications = coalesce(ErrorReplications, 0),
      GoodReplications = CompletedReplications - ErrorReplications,
      ErrorRate = if_else(CompletedReplications > 0, (ErrorReplications / CompletedReplications) * 100, 0)
    ) %>%
    select(FileName, RequestedReplications, CompletedReplications, ErrorReplications, GoodReplications, ErrorRate)

  full_summary
}

# Step 4: Parallelized Processing (Windows/Mac/Linux Compatible)
output_folder <- here('Simulations', 'STUDY_1', '2 Time Points', '4_2T_RILTA_GEN_RILTA_ANALYZED_REP')  # Adjust to your folder path
file_list <- list.files(output_folder, pattern = "\\.out$", full.names = TRUE)

# Step 5: Detect OS and Set Up Cluster
cluster_type <- ifelse(.Platform$OS.type == "windows", "PSOCK", "FORK")
num_cores <- detectCores() - 1  # Use all but one core
cl <- makeCluster(num_cores, type = cluster_type)

# Export necessary libraries and functions to the cluster
invisible(clusterExport(cl, c("extract_errors_from_file", "extract_completed_replications", "extract_requested_replications")))
invisible(clusterEvalQ(cl, library(tidyverse)))

# Step 6: Parallel Processing
# Calculate completed replications first
completed_rep_list <- parLapply(cl, file_list, extract_completed_replications)

# Extract errors while passing the total number of completed replications to the function
error_summary <- bind_rows(mapply(function(filepath, completed_data) {
  extract_errors_from_file(filepath, completed_data$CompletedReplications)
}, file_list, completed_rep_list, SIMPLIFY = FALSE))

completed_replications <- bind_rows(parLapply(cl, file_list, extract_completed_replications))
requested_replications <- bind_rows(parLapply(cl, file_list, extract_requested_replications))

# Stop the cluster
stopCluster(cl)

# Step 7: Calculate Replication Summary
replication_summary <- calculate_replication_summary(error_summary, completed_replications, requested_replications)

# Step 8: Create and Display the Table with Error Rate
replication_summary_table <- replication_summary %>%
  gt() %>%
  tab_header(
    title = "Replication Summary",
    subtitle = paste0("Folder: ", output_folder)
  ) %>%
  fmt_number(columns = c(CompletedReplications, RequestedReplications, ErrorReplications, GoodReplications, ErrorRate), decimals = 2) %>%
  cols_label(
    FileName = "File Name",
    CompletedReplications = "Completed Replications",
    RequestedReplications = "Requested Replications",
    ErrorReplications = "Replications with Errors",
    GoodReplications = "Good Replications",
    ErrorRate = "Error Rate (%)"
  ) %>%
  tab_options(
    table.font.size = "small",
    heading.title.font.size = "medium",
    heading.subtitle.font.size = "small"
  )

# Display the table
replication_summary_table

completed_replications <- completed_replications %>%
  mutate(FileName = str_replace(FileName, "\\.out$", ""),
         FileName = tolower(FileName),
         FileName = str_trim(FileName))

error_summary <- error_summary %>%
  mutate(FileName = str_replace(FileName, "\\.out$", ""),
         FileName = tolower(FileName),
         FileName = str_trim(FileName))

final_data_with_actuals <- final_data_with_actuals %>%
  mutate(FileName = tolower(FileName),
         FileName = str_trim(FileName))

replication_summary <- replication_summary %>%
  mutate(FileName = str_replace(FileName, "\\.out$", ""),
         FileName = tolower(FileName),
         FileName = str_trim(FileName))

cat("Rows in final_data_with_actuals:", nrow(final_data_with_actuals), "\n")
# Output the final number of rows to confirm data handling
cat("Number of rows in error_summary: ", nrow(error_summary), "\n")
cat("Number of rows in replication_summary: ", nrow(replication_summary), "\n")

```

#### STEP 9: Part 1 Merge ERRORS with main data

```{r}
#| label: "merge-errors2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Ensure consistent column formats and remove .out and .csv extensions
error_summary <- error_summary %>%
  mutate(
    FileName = tolower(str_trim(gsub("\\.out$|\\.csv$", "", FileName))), # Remove extensions and standardize FileName
    Replication = as.character(Replication) # Convert Replication to character
  )

final_data_with_actuals <- final_data_with_actuals %>%
  mutate(
    FileName = tolower(str_trim(gsub("\\.out$|\\.csv$", "", FileName))), # Remove extensions and standardize FileName
    Replication = as.character(Replication) # Convert Replication to character
  )

# Add a new column to flag errors
error_summary <- error_summary %>%
  mutate(ErrorFlag = if_else(Message == "None", 0, 1))

# Merge ErrorFlag into final_data_with_actuals
final_data_with_actuals <- final_data_with_actuals %>%
  left_join(error_summary %>% select(FileName, Replication, ErrorFlag),
            by = c("FileName" = "FileName", "Replication" = "Replication"))
```

#### Step 9: Part 2 Create Column Names from the Filename

```{r}

#| label: "create-column-names-from-filename2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true
# Add new columns based on the information in the FileName
final_data_with_actuals <- final_data_with_actuals %>%
  mutate(
    N = case_when(
      grepl("n_4000", FileName) ~ 4000,
      grepl("n_500", FileName) ~ 500,
      grepl("n_1000", FileName) ~ 1000,
      grepl("n_2000", FileName) ~ 2000,
      TRUE ~ NA_integer_
    ),
    Population = case_when(
      grepl("tp_1.385", FileName) ~ ".800",
      grepl("tp_0.85", FileName) ~ ".700",
      grepl("tp_0.41", FileName) ~ ".600",
      grepl("tp_-0.41", FileName) ~ ".400",
      grepl("tp_-0.85", FileName) ~ ".300",
      grepl("tp_-1.385", FileName) ~ ".200",
      TRUE ~ NA_character_
    ),
    Lambda_values = case_when(
      grepl("a_1$", FileName) ~ "1",
      grepl("a_1\\.5", FileName) ~ "1.5",
      grepl("a_2$", FileName) ~ "2",
      grepl("a_2\\.5", FileName) ~ "2.5",
      grepl("a_3$", FileName) ~ "3",
      grepl("a_0", FileName) ~ "0",
      TRUE ~ NA_character_
    ),
    Transitions = case_when(
      Population %in% c(".200", ".300", ".400") ~ 1,
      Population %in% c(".600", ".700", ".800") ~ 2,
      TRUE ~ NA_integer_
    )
  ) %>%
  mutate(
    N = factor(N, levels = c(4000, 500, 1000, 2000), labels = c("N = 4000", "N = 500", "N = 1000", "N = 2000")),
    Population = factor(Population, levels = c(".800", ".700", ".600", ".400", ".300", ".200")),
    Transitions = factor(Transitions, levels = c(1, 2), labels = c("Mover", "Stayer"))
  )


```

#### Step 9: Part 3 Calculate Violation Percentages per Condition after resimulation

```{r}

#| label: "calculate-violations2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

violation_summary <- final_data_with_actuals %>%
  mutate(
    Any_Violation = ifelse(is.na(Any_Violation), 0, Any_Violation),
    ErrorFlag = ifelse(is.na(ErrorFlag), 0, ErrorFlag)  # No missing values for ErrorFlag
  ) %>%
  group_by(FileName, Population, N, Lambda_values) %>%
  summarize(
    Total_Rows = n(),                                # Total runs
    Total_Violations = sum(Any_Violation, na.rm = TRUE), # Sum of violations
    Total_Errors = sum(ErrorFlag, na.rm = TRUE),     # Sum of errors
    Percentage_Violations = (Total_Violations / Total_Rows) * 100, # Percentage of violations
    .groups = "drop"
  ) %>%
  left_join(replication_summary %>% select(FileName, CompletedReplications), by = "FileName") %>%
  mutate(
    CompletedReplications = ifelse(is.na(CompletedReplications), 500, CompletedReplications),
    N_numeric = as.numeric(gsub("N = ", "", N)),
    GoodReplications = CompletedReplications - Total_Violations - Total_Errors,
    GoodReplications = ifelse(GoodReplications < 0, 0, GoodReplications),
    Reanalysis_Needed = if_else(GoodReplications >= 500, "No", "Yes"),  # Flag for reanalysis needed
    ErrorRate = Total_Errors / Total_Rows,          # Calculate ErrorRate
    Replications_Needed = ceiling(CompletedReplications + Total_Violations + (CompletedReplications * Percentage_Violations / 100) + 20),
    Adjusted_Replications_Needed = ceiling(Replications_Needed / (1 - ErrorRate))
  ) %>%
  select(
    FileName, Population, N, N_numeric,
    Lambda_values, Total_Rows, Total_Violations, Total_Errors,
    ErrorRate, Percentage_Violations, GoodReplications,
    Replications_Needed, Adjusted_Replications_Needed,
    Reanalysis_Needed
  )


```

#### Step 9: Part 4 Summarize & Visualize Label Switching Percentage Results

```{r}
#| label: "summarize-violations"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Filter the data for rows with non-zero error rates and clean N_numeric
# Modify the TPs assignment to force formatting
violation_summary <- violation_summary %>%
  mutate(
    TPs = case_when(
      Population == ".800" ~ formatC(1.385, format = "f", digits = 3),
      Population == ".700" ~ formatC(0.85, format = "f", digits = 2),
      Population == ".600" ~ formatC(0.41, format = "f", digits = 2),
      Population == ".400" ~ formatC(-0.41, format = "f", digits = 2),
      Population == ".300" ~ formatC(-0.85, format = "f", digits = 2),
      Population == ".200" ~ formatC(-1.385, format = "f", digits = 3),
      TRUE ~ NA_character_
  ),
    N_numeric = as.numeric(gsub("N = ", "", as.character(N)))  # Clean N_numeric by stripping out "N = "
  )

# Summarize and visualize the final table
final_table <- violation_summary %>%
  select(
    `Transition Probability` = Population,  # Rename column for clarity
    TPs,  # Logit values
    Lambda = Lambda_values,  # Lambda values
    N_numeric,  # Cleaned sample size
    `Total Mplus Runs` = Total_Rows,  # Total runs
    Total_Violations,  # Total violations
    `% of Violations` = Percentage_Violations,  # Violation percentage
    Total_Errors, 
    ErrorRate,  # Error rate
    Replications_Needed,  # Replications needed
    `Adjusted Replications Needed` = Adjusted_Replications_Needed,  # Adjusted replications needed
    Reanalysis_Needed
  ) %>%
  gt() %>%
  tab_header(
    title = "Monte Carlo Results:",
    subtitle = "Percentage of Cases with Label Switching and Replications Needed"
  ) %>%
  cols_align(
    align = "center",  # Center all columns
    columns = everything()
  ) %>%
  fmt_number(
    columns = c(`% of Violations`, ErrorRate),
    decimals = 2  # Ensure these percentages are formatted to 2 decimal places
  ) %>%
  tab_options(
    data_row.padding = px(4)  # Set padding between rows
  ) %>%
  tab_style(
    style = cell_text(align = "center"),  # Center align the headers only
    locations = cells_column_labels(everything())  # Apply to headers only
  )

# Display the table
final_table



```

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAAaCAYAAADFTB7LAAAAcElEQVR4Xu3OwQmAQAxE0bClWYCW5N06tM6V2YPg5CjoF/JhLoHAi6iqn9eOefUbqrYvHY0cQDLyAlKRNyARmYA0ZMLRkAlGQyaU72tkAtlim7r/vJqDUDjlKBROOQyFU2icQuMUGqfQuBEaV1XPOwEx96nYACK8+wAAAABJRU5ErkJggg== "Run Current Chunk")

#### Step 9: Part 5 Create Follow up iterator table

```{r}
# Assuming 'violation_summary' has been previously defined and transformed

# Filter for rows where Reanalysis_Needed is 'Yes' and select specific columns
# Save this filtered data as a DataFrame
violation_summary_df <- violation_summary %>%
  filter(Reanalysis_Needed == "Yes") %>%
  select(
    TPs,                     # Logit values
    Lambda = Lambda_values,  # Lambda values
    N_numeric,               # Cleaned sample size
    Adjusted_Replications_Needed  # Adjusted replications needed
  )

# Use the saved DataFrame to create the gt table
violation_summary_table <- gt(data = violation_summary_df) %>%
  tab_header(
    title = "Monte Carlo Results for Reanalysis:",
    subtitle = "Selected cases requiring further analysis"
  ) %>%
  cols_align(
    align = "center",  # Center all columns
    columns = everything()
  ) %>%
  tab_options(
    data_row.padding = px(4)  # Set padding between rows
  ) %>%
  tab_style(
    style = cell_text(align = "center"),  # Center align the headers only
    locations = cells_column_labels(everything())  # Apply to headers only
  )

# Display the table
violation_summary_table

```

### Step 10: Delete Cases that Violate

#### **Objective**

*Filter out cases with any violations, leaving only the clean data.*

```{r}
#| label: "delete-cases"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Filter out cases with no violations and no errors
filtered_data_with_no_violations <- final_data_with_actuals[
  final_data_with_actuals$Any_Violation == 0 & final_data_with_actuals$ErrorFlag == 0, ]
```

#### STEP 10 Part 1

Randomly select 500 replications per condition

```{r}
# Set seed for reproducibility
set.seed(07252005)

# Group data by FileName and sample 500 rows per condition (if possible)
cleaned_data <- filtered_data_with_no_violations %>%
  group_by(FileName) %>%
  slice_sample(n = 500, replace = FALSE) %>%  # Randomly sample 500 rows (without replacement)
  ungroup()

# Verify the number of rows per condition
condition_counts <- cleaned_data %>%
  group_by(FileName) %>%
  summarize(Count = n(), .groups = "drop")

# Print conditions with more or less than 500 rows (sanity check)
sanity_check <- condition_counts %>%
  filter(Count != 500)

if (nrow(sanity_check) > 0) {
  warning("Some conditions do not have exactly 500 rows. Please verify the data and ensure extra replications are sufficient.")
  print(sanity_check)
} else {
  message("All conditions have exactly 500 rows.")
}



```

### **Step 11: Compute Monte Carlo (MC) Values**

#### **Objective**

*Calculate Monte Carlo values for `TRANS11`, including population values, averages, standard errors, Mean Squared Error (MSE), coverage, and power.*

```{r}
#| label: "compute-mc-values"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

cleaned_data <- cleaned_data %>%
  mutate(Population = as.numeric(as.character(Population)))

# Calculate the Monte Carlo values, including Population (transition probability), N, 
# number of replications, Lambda, and averages for TRANS11 and SE11
mc_values <- cleaned_data %>%
  group_by(FileName, Population, N, Transitions, Lambda) %>%  # Include Lambda in the grouping
  summarize(
    average = round(mean(TRANS11, na.rm = TRUE), 3),
    average_SE = round(mean(SE_11, na.rm = TRUE), 3),
    population_sd = round(sd(TRANS11, na.rm = TRUE), 3),
    
    # MSE calculation: mean squared error between TRANS11 and Population
    MSE = round(mean((TRANS11 - Population)^2, na.rm = TRUE), 3),
    
    # Coverage calculation: check if Population lies within the confidence interval
    Coverage = round(mean((Population >= (TRANS11 - 1.96 * SE_11)) & (Population <= (TRANS11 + 1.96 * SE_11)), na.rm = TRUE), 3),
    
    # Power calculation: proportion of cases where TRANS11 is significant
    Power = round(mean(TRANS11 / SE_11 > 1.96, na.rm = TRUE), 3),
    
    # Reps_Used counts the number of replications (rows) used for each FileName
    Reps_Used = n()
  )

# Round the values to 3 decimal points
mc_values <- mc_values %>%
  mutate(across(starts_with("Avg_"), ~ round(.x, 3)))

```

### **Step 17: Calculate Dichotomous Variables and Bias**

#### **Objective**

*Calculate dichotomous variables for Power and Coverage, compute Parameter and SE Bias, and prepare subsets for movers and stayers.*

```{r}
#| label: "calculate-bias-dichotomous-variables"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 1: Calculate dichotomous variable for Power (1 if Power >= 0.8, else 0)
mc_values <- mc_values %>%
  mutate(Power_Dic = ifelse(Power >= 0.8, 1, 0))

# Step 2: Calculate dichotomous variable for Coverage (0 if outside [0.91, 0.98], else 1)
mc_values <- mc_values %>%
  mutate(Coverage_Dic = ifelse(Coverage > 0.98 | Coverage < 0.91, 0, 1))

# Step 3: Remove any groupings before further calculations
mc_values <- mc_values %>%
  ungroup()

# Step 4: Ensure numeric columns are correctly formatted and **convert Population only for calculations**
mc_values <- mc_values %>%
  mutate(
    # Keep average as numeric but **do not convert Population for display purposes**
    average = as.numeric(average),
    population_numeric = as.numeric(Population),  # Create a temporary numeric version of Population
    average_se = as.numeric(average_SE),
    population_sd = as.numeric(population_sd)
  )

# Step 5: Calculate Parameter Bias and SE Bias, rounding the results to 2 decimal places
mc_values <- mc_values %>%
  mutate(
    # Use population_numeric for the calculations, but **retain Population in the original format**
    Parameter_Bias = (average - population_numeric) / population_numeric * 100,  # Bias for the parameter
    SE_Bias = (average_se - population_sd) / population_sd * 100  # Bias for the standard error
  ) %>%
  mutate(across(c(Parameter_Bias, SE_Bias), ~ round(.x, 2)))  # Round to 2 decimal places

# Drop the temporary numeric population column if no longer needed
mc_values <- mc_values %>%
  select(-population_numeric)

```

### **Step 12: Subset Data for Movers and Stayers**

#### **Objective**

*Subset the Monte Carlo values data for transitions with movers and stayers based on the population value.*

```{r}

#| label: "subset-data-for-bias-plots"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true


# Assuming Population is numeric in all_data
all_data <- mc_values

# Convert N to a factor with the correct labels for plotting
all_data <- all_data %>%
  mutate(N = factor(N,
                    levels = c("N = 500", "N = 1000", "N = 2000", "N = 4000"),  # These are the existing labels
                    labels = c(`1` = "N = 500", 
                               `2` = "N = 1000", 
                               `3` = "N = 2000", 
                               `4` = "N = 4000")))

# Define the labels for N using expression() for italics, which will be used in plotting
n_labels <- c(
  `1` = expression(italic('N') ~ "= 500"),
  `2` = expression(italic('N') ~ "= 1000"),
  `3` = expression(italic('N') ~ "= 2000"),
  `4` = expression(italic('N') ~ "= 4000")
)

# Assign the labels to the levels
all_data$N <- factor(all_data$N, labels = n_labels)
# Now you can use `n_labels` in the plotting code


# Ensure that Population_Label is correctly prepared
all_data$Population_Label <- factor(all_data$Population, 
    levels = c(0.2, 0.3, 0.4, 0.6, 0.7, 0.8),  # Numeric levels without leading zeros
    labels = c(
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .200"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .300"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .400"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .600"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .700"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .800")
    )
)


# Subset for Transitions movers (already correctly defined as "Mover")
subset_mover <- subset(all_data, Transitions == "Mover")
subset_mover <- subset_mover %>%
  mutate(Lambda = as.numeric(as.character(Lambda)))

# Subset for Transitions stayers (already correctly defined as "Stayer")
subset_stayer <- subset(all_data, Transitions == "Stayer")
subset_stayer <- subset_stayer %>%
  mutate(Lambda = as.numeric(as.character(Lambda)))

```

### **Step 13: Combined Plot for Mover and Stayer**

#### **Objective**

*Create streamlined plots for both mover and stayer subsets using a common theme and labels.*

```{r}
#| label: "plot-bias"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Define common themes and aesthetics
common_theme <- theme_minimal() +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.minor.y = element_blank(),
    axis.text.x = element_text(size = 8),
    axis.ticks = element_line(color = "black", linewidth = 0.2),
    legend.position = "bottom",
    legend.title = element_blank(),
    text = element_text(family = "Times New Roman"),
    axis.title.x = element_text(margin = margin(t = 10, b = 10)),
    legend.margin = margin(t = -10),
    plot.caption = element_text(hjust = 0, margin = margin(t = 10))
  )

common_labels <- labs(
  x = "Lambda Loadings on the RI",
  y = "Bias (%)",
  color = "",
  title = ""
)


create_plot <- function(data, title_suffix) {
  # Detect which legend items to show
  present_categories <- c("Parameter Bias", "Standard Error Bias")  # Base categories
  if (any(data$Coverage_Dic == 0)) present_categories <- c(present_categories, "Coverage Failure")
  if (any(data$Power_Dic == 0)) present_categories <- c(present_categories, "Power Failure")

  # Define colors and shapes for different categories
  colors <- c("Parameter Bias" = "#113386", "Standard Error Bias" = "#C830CC", 
              "Coverage Failure" = "#113386", "Power Failure" = "black")
  shapes <- c("Parameter Bias" = 16, "Standard Error Bias" = 18, 
              "Coverage Failure" = 1, "Power Failure" = 4)

  # Filter colors and shapes based on detected categories
  filtered_colors <- colors[present_categories]
  filtered_shapes <- shapes[present_categories]

  ggplot(data = data, aes(x = Lambda, y = Parameter_Bias, color = "Parameter Bias", group = Population_Label)) +  
    geom_line(aes(group = Population_Label), linewidth = 0.3, linetype = "solid") +  
    geom_line(aes(y = SE_Bias, group = Population_Label, color = "Standard Error Bias"), linewidth = 0.3, linetype = "solid") +  
    geom_point(aes(y = Parameter_Bias), shape = 16, size = 1, fill = "#113386", alpha = 0.8) +  
    geom_point(aes(y = SE_Bias, color = "Standard Error Bias"), shape = 18, size = 1, fill = "#C830CC", alpha = 0.8) +  # Corrected
    geom_point(data = subset(data, Coverage_Dic == 0), aes(y = Parameter_Bias, color = "Coverage Failure"), shape = 1, size = 2, fill = "#113386", alpha = 1) +  
    geom_point(data = subset(data, Power_Dic == 0), aes(y = Parameter_Bias, color = "Power Failure"), shape = 4, size = 2, fill = "black", alpha = 1) + 
    scale_color_manual(
      values = filtered_colors, 
      labels = present_categories, 
      breaks = present_categories,
      guide = guide_legend(
        override.aes = list(shape = filtered_shapes)
      )
    ) +  
    labs(
      x = "Lambda Loadings on the RI",
      y = "Bias (%)",
      color = "",
      title = paste("RILTA Generated, LTA Analyzed with", title_suffix, "Transition Probabilities")
    ) +
    coord_cartesian(ylim = c(-40, 40)) +  
    facet_grid(Population_Label ~ N, scales = "free_x", labeller = label_parsed) +  
    scale_x_continuous(breaks = seq(0, 3, by = 0.5), labels = scales::number_format(accuracy = 0.1)) +  
    scale_y_continuous(breaks = seq(-40, 40, by = 10)) +  
    theme_minimal() +
    theme(
      panel.grid.major.x = element_blank(),
      panel.grid.minor.x = element_blank(),
      panel.grid.minor.y = element_blank(),
      axis.text.x = element_text(size = 8),
      axis.ticks = element_line(color = "black", linewidth = 0.2),
      legend.position = "bottom",
      legend.title = element_blank(),
      text = element_text(family = "Times New Roman"),
      axis.title.x = element_text(margin = margin(t = 10, b = 10)),
      legend.margin = margin(t = -10),
      plot.caption = element_text(hjust = 0, margin = margin(t = 10))
    ) +
    geom_hline(yintercept = c(-10, 10), linetype = "dashed", color = "#113386", linewidth = 0.3) +  
    geom_hline(yintercept = c(-5, 5), linetype = "dashed", color = "#C830CC", linewidth = 0.3)
}

```

#### Plot figure with Mover Transition Probabilities (.200, .300, .400)

```{r}
# Create and print plot for Mover
plot_mover <- create_plot(subset_mover, "Mover")
#| column: screen
#| fig-format: svg
print(plot_mover)

# Remove title for the saved version
plot_mover_no_title <- plot_mover + labs(title = NULL)

# Save Mover plot without title as .svg
ggsave(here("2 Time Points", "zFIGURES", "x2t_rilta_rilta_plots", "plot_mover.svg"), plot = plot_mover_no_title, width = 6, height = 3, dpi = 300, device = "svg")
```

#### Plot figure with Mover Transition Probablities (.200, .300, .400)

```{r}
# Create and print plot for Stayer
plot_stayer <- create_plot(subset_stayer, "Stayer")
#| column: screen
#| fig-format: svg
print(plot_stayer)

# Remove title for the saved version
plot_stayer_no_title <- plot_stayer + labs(title = NULL)

# Save Stayer plot without title as .svg
ggsave(here("2 Time Points", "zFIGURES", "x2t_rilta_rilta_plots", "plot_stayer.svg"), plot = plot_stayer_no_title, width = 6, height = 3, dpi = 300, device = "svg")
```

### **Step 14: Prepare Data for Heatmaps**

#### **Objective**

*Prepare data for heatmap creation by ensuring correct formatting for population values, and subsetting the data based on class proportions and sample sizes.*

```{r}

#| label: "prepare-data-for-heatmaps"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 1: Define a function to extract numeric values from the 'N' column
extract_numeric_from_N <- function(N) {
  # Use regex to extract the numeric part (e.g., from 'italic("N") ~ "= 1000"')
  as.numeric(gsub("[^0-9]", "", N))
}

# Step 2: Apply this function to both movers and stayers
movers <- all_data %>%
  filter(Transitions == "Mover") %>%
  mutate(N_numeric = extract_numeric_from_N(N)) %>%  # Create a new column with numeric N
  arrange(N_numeric, Population)

stayers <- all_data %>%
  filter(Transitions == "Stayer") %>%
  mutate(N_numeric = extract_numeric_from_N(N)) %>%  # Create a new column with numeric N
  arrange(N_numeric, Population)

# Step 3: Assign N_Label only once for each N within Movers and Stayers
movers <- movers %>%
  group_by(N) %>%
  mutate(N_Label = ifelse(row_number() == 2, as.character(N), "")) %>%
  ungroup()

stayers <- stayers %>%
  group_by(N) %>%
  mutate(N_Label = ifelse(row_number() == 2, as.character(N), "")) %>%
  ungroup()

# Step 4: Combine movers and stayers back into one dataset, keeping them separate by transitions
all_data_sorted <- bind_rows(movers, stayers)

# Step 5: Ensure Lambda is included and columns are factored
all_data_sorted <- all_data_sorted %>% 
  mutate(N_Label = factor(N, labels = c("N = 500", "N = 1000", "N = 2000", "N = 4000"))) %>% 
  mutate(Population = factor(Population,
                             labels = c(`.2` = ".200", `.3` = ".300", `.4` = ".400",
                                        `.6` = ".600", `.7` = ".700", `.8` = ".800")))

# Step 6: Select necessary columns, including Lambda
test_map <- select(all_data_sorted, N_Label, Population, average, Coverage, Power, Parameter_Bias, SE_Bias, Lambda)

# Step 7: Ordering the table based on the "Lambda" column
test_map <- test_map %>%
  arrange(as.numeric(Lambda))

# Define the population values as characters
population_values <- c(".200", ".300", ".400", ".600", ".700", ".800")

# Function to subset the data for a specific population value
subset_data <- function(data, pop_value) {
  subset <- data %>%
    filter(Population == pop_value)
  return(subset)
}

# Apply the function to each population value
subset_list <- lapply(population_values, function(x) subset_data(test_map, as.character(x)))

# Access the subsets for each population value
subset_02 <- subset_list[[1]]  # Subset for population value .200
subset_03 <- subset_list[[2]]  # Subset for population value .300
subset_04 <- subset_list[[3]]  # Subset for population value .400
subset_06 <- subset_list[[4]]  # Subset for population value .600
subset_07 <- subset_list[[5]]  # Subset for population value .700
subset_08 <- subset_list[[6]]  # Subset for population value .800

```

### **Step 15: Heatmap Creation and Rendering**

#### **Objective**

*Create heatmaps using the `gt` package and render each table separately for different subsets of the data.*

```{r}

#| label: "create-heatmap-function"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

create_table <- function(subset, transition_probability) {
  
  # Create the gt object and set initial formatting
  gt_table <- subset %>%
    gt() %>%
    opt_table_font(stack = "geometric-humanist") %>% 
    tab_header(
      title = paste("RILTA Generated & RILTA Analyzed with Transition Probability of", transition_probability)
    ) %>%
    cols_label(
      N_Label = "Sample Size",
      average = "Estimated<br>Probability",
      Coverage = "Coverage",
      Power = "Power",
      Parameter_Bias = "Parameter<br>Bias",
      SE_Bias = "Standard Error<br>Bias",
      .fn = md
    ) %>%
    tab_spanner(
      label = "Bias",
      columns = c("Parameter_Bias", "SE_Bias")) %>%
    tab_row_group(
      label = "Lambda RI Loading of 3 (λ)",  # Label for the first subgroup
      rows = c(21:24)  # Rows corresponding to the first subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 2.5 (λ)",  # Label for the second subgroup
      rows = c(17:20)  # Rows corresponding to the second subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 2 (λ)",  # Label for the third subgroup
      rows = c(13:16)  # Rows corresponding to the third subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 1.5 (λ)",  # Label for the fourth subgroup
      rows = c(9:12)  # Rows corresponding to the fourth subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 1 (λ)",  # Label for the fifth subgroup
      rows = c(5:8)  # Rows corresponding to the fifth subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 0 (λ)",  # Label for the sixth subgroup
      rows = c(1:4)  # Rows corresponding to the sixth subgroup
    ) %>%
      tab_style(
    style = cell_text(
      font = "bold italic"  # Apply bold and italic styling
    ),
      locations = cells_row_groups()  # Apply style to row subheaders
    ) %>% 
    fmt_number(columns = c("Parameter_Bias", "SE_Bias"), decimals = 2) %>%  
    fmt_number(columns = 3, decimals = 3) %>%
    tab_options(
      table_body.hlines.color = "white",
      table.border.top.color = "black",
      table.border.bottom.color = "black",
      table_body.border.bottom.color = "black",
      heading.border.bottom.color = "black",
      column_labels.border.top.color = "black",
      column_labels.border.bottom.color = "black",
      row_group.border.bottom.color = "black" ,
      row_group.border.top.color = "black" 
    ) %>%
    cols_align(
      align = c("center"),
      columns = everything()
    )

  # Apply color highlighting for violations in Parameter Bias
  if (any(!(subset$Parameter_Bias >= -9.99 & subset$Parameter_Bias <= 9.99), na.rm = TRUE)) {
    gt_table <- gt_table %>%
      data_color(
        columns = "Parameter_Bias",
        rows = .data$Parameter_Bias < -9.99 | .data$Parameter_Bias > 9.99,  # Apply color only if outside the threshold
        method = "numeric",
        palette = c("#113386", "#DAE3FA", "#113386"),  # Darker blue for larger deviations
        domain = c(-40, 40)  # Adjust the domain to reflect the range of values
      ) %>%
      tab_footnote(
        footnote = md("Darker blue indicates larger deviations from zero *Parameter Bias* beyond the ±9.99 threshold."),
        locations = cells_column_labels(columns = "Parameter_Bias")
      )
  }

  # Apply color highlighting for violations in SE Bias
  if (any(!(subset$SE_Bias >= -4.99 & subset$SE_Bias <= 4.99), na.rm = TRUE)) {
    gt_table <- gt_table %>%
      data_color(
        columns = "SE_Bias",
        rows = .data$SE_Bias < -4.99 | .data$SE_Bias > 4.99,  # Apply color only if outside the threshold
        method = "numeric",
        palette = c("#B4186E", "#F9D5E9", "#B4186E"),  # Darker red for larger deviations
        domain = c(-80, 80)  # Adjust the domain for the SE_Bias range
      ) %>%
      tab_footnote(
        footnote = md("Darker red indicates larger deviations from zero *Standard Error Bias* beyond the ±4.99 threshold."),
        locations = cells_column_labels(columns = "SE_Bias")
      )
  }

  if (any(subset$Coverage < 0.93 | subset$Coverage > 0.979, na.rm = TRUE)) {
    gt_table <- gt_table %>%
      data_color(
        columns = "Coverage",
        rows = subset$Coverage < 0.93 | subset$Coverage > 0.979,
        method = "numeric",
        palette = c("#93C6B1", "white"),  # Green for coverage issues
        domain = c(0, 1)
      ) %>%
      tab_footnote(
        footnote = md("Green indicates failure to achieve adequate *Coverage*."),
        locations = cells_column_labels(columns = "Coverage")
      )
  }

  if (any(subset$Power < 0.8, na.rm = TRUE)) {
    gt_table <- gt_table %>%
      data_color(
        columns = "Power",
        rows = subset$Power < 0.8,
        method = "numeric",
        palette = c("#502CD1", "white"),  # Purple for power issues
        domain = c(0, 1)
      ) %>%
      tab_footnote(
        footnote = md("Purple indicates failure to achieve adequate *Power*."),
        locations = cells_column_labels(columns = "Power")
      )
  }
  
  return(gt_table)
}

```

#### Render tables for each transition probability condition TABLE FOR TRANSITION PROBABILITIES OF .200

```{r, warning = FALSE}
subset_02_table <- create_table(subset_02, ".200")
subset_02_table
subset_02_table |>  tab_options(table.width = pct(65)) |> gtsave(here("2 Time Points", "zHEATMAPS", "z2t_heatmaps", "z2t_r_r_heatmaps", "2T_R_R_.200.png"))
```

#### TABLE FOR TRANSITION PROBABILITIES OF .300

```{r, warning = FALSE}
subset_03_table <- create_table(subset_03, ".300")
subset_03_table
subset_03_table |>  tab_options(table.width = pct(65)) |> gtsave(here("2 Time Points", "zHEATMAPS", "z2t_heatmaps", "z2t_r_r_heatmaps","2T_R_R_.300.png"))
```

#### TABLE FOR TRANSITION PROBABILITIES OF .400

```{r, warning = FALSE}
subset_04_table <- create_table(subset_04, ".400")
subset_04_table
subset_04_table |>  tab_options(table.width = pct(65)) |> gtsave(here("2 Time Points", "zHEATMAPS", "z2t_heatmaps", "z2t_r_r_heatmaps","2T_R_R_.400.png"))
```

#### TABLE FOR TRANSITION PROBABILITIES OF .600

```{r, warning = FALSE}
subset_06_table <- create_table(subset_06, ".600")
subset_06_table
subset_06_table |>  tab_options(table.width = pct(65)) |> gtsave(here("2 Time Points", "zHEATMAPS", "z2t_heatmaps", "z2t_r_r_heatmaps","2T_R_R_.600.png"))
```

#### TABLE FOR TRANSITION PROBABILITIES OF .700

```{r, warning = FALSE}
subset_07_table <- create_table(subset_07, ".700")
subset_07_table
subset_07_table |>  tab_options(table.width = pct(65)) |> gtsave(here("2 Time Points", "zHEATMAPS", "z2t_heatmaps", "z2t_r_r_heatmaps","2T_R_R_.700.png"))
```

#### TABLE FOR TRANSITION PROBABILITIES OF .800

```{r, warning = FALSE}
subset_08_table <- create_table(subset_08, ".800")
subset_08_table
subset_08_table |>  tab_options(table.width = pct(65)) |> gtsave(here("2 Time Points", "zHEATMAPS", "z2t_heatmaps", "z2t_r_r_heatmaps","2T_R_R_.800.png"))
```
