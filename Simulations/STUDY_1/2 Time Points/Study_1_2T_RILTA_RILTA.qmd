---
title: "Understanding the Performance of Random Intercepts Latent Transition Analysis (RI-LTA): A Monte Carlo Simulation Study"
subtitle: "*Study 1: RILTA Generated, RILTA Analyzed with Two Timepoints*"
format:
  html:
    toc: true
    toc-depth: 3
editor: visual
theme: flatly
author: "Delwin Carter"
date: "`r format(Sys.time(), '%B %d, %Y')`"
page-layout: full
fig-format: svg
Mainfont: "Avenir.ttc"
knitr:
  opts_chunk:
    out.width: "90%"
    fig.align: center
---

------------------------------------------------------------------------

::: {layout-ncol="2"}
![](images/LVG%20FINAL.png){width="300"}

![](images/UCSB_Gauchos_logo_PNG2.png){width="300"}
:::

------------------------------------------------------------------------

Load Packages

```{r, message=FALSE, warning=FALSE}
#| label: "load-libraries"
#| echo: true
#| message: false
#| warning: false


library(tidyverse)
library(MplusAutomation)
library(here)
library(gt)
library(janitor)
library(glue)
library(ggtext)
library(rlang)
library(knitr)
library(parallel)
library(tools)
```

------------------------------------------------------------------------

# RILTA Generated, RILTA Analyzed

![](images/RILTA_RILTA.png){width="354"}

------------------------------------------------------------------------

# Part 1: Conduct Simulation

> In this section, I am conducting a simulation where I am generating data as Latent Transition Analysis and analyzing it as Random Intercepts Latent Transition Analysis to fully explore the model's performance. The simulation consists of 144 conditions, combining four sample sizes (N = 500, 1000, 2000, 4000) with six transition probabilities linked to logits (1.385, .85, .41, -.41, -.85, -1.385), corresponding to probabilities of .8, .7, .6, .4, .3, and .2., and lambda loadings on the random intercept of 0, 1, 1.5, 2, 2.5, and 3. These conditions are iterated over programmatically using MplusAutomation to set up and execute the models. To speed up the process, I employ parallel processing, which distributes computations across multiple CPU cores, enabling efficient completion of the simulations across all scenarios.

##### Conditions:

Sample Size: N = 500, 1000, 2000, and 4000

Transition logit (probability): TPs = 1.385 (.8), .85 (.7), .41 (.6), -.41 (.4), -.85 (.3) , and -1.385 (.2)

![](images/clipboard-3344253592.png){width="450"}

RI Loadings: lambda = 0, 1, 1.5, 2, 2.5, and 3

![](images/clipboard-3119439446.png){width="351"}

------------------------------------------------------------------------

Setting up the Simulation

```{r}
#| label: "simulation-conditions"
#| echo: true
#| message: false
#| warning: false


#Create grid of conditions for iteration
p1 <- expand.grid(N = c(500, 1000, 2000, 4000),
TPs = c(1.385, .85, .41, -.41, -.85, -1.385),
lambda = c(0, 1, 1.5, 2, 2.5, 3))
       
# Display the matrix using gt
p1 %>%
  gt() %>%
  tab_header(
    title = "Simulation Conditions Matrix",
    subtitle = "Combinations of Sample Sizes, Transition Probabilities, and Mixtures"
  ) %>%
  cols_align(
    align = "center",
    columns = everything() # Centers all columns
  )
```

------------------------------------------------------------------------

## Run Initial Simulation

TEMP ITERATOR MATRIX

```{r}
library(dplyr)
library(stringr)

# List filenames from the directory
input_dir <- file.path("TEMP R R")
input_files <- list.files(input_dir, pattern = "\\.inp$", full.names = FALSE)

# Extract relevant information directly from filenames
p1 <- data.frame(FileName = input_files) %>%
  mutate(
    # Extract sample size (N)
    N = case_when(
      grepl("N_4000", FileName) ~ 4000,
      grepl("N_500", FileName) ~ 500,
      grepl("N_1000", FileName) ~ 1000,
      grepl("N_2000", FileName) ~ 2000,
      TRUE ~ NA_integer_
    ),
    # Extract Population based on `TP` values
    TPs = case_when(
      grepl("TP_1.385", FileName) ~ "1.385",
      grepl("TP_0.85", FileName) ~ ".85",
      grepl("TP_0.41", FileName) ~ ".41",
      grepl("TP_-0.41", FileName) ~ "-.41",
      grepl("TP_-0.85", FileName) ~ "-.85",
      grepl("TP_-1.385", FileName) ~ "-1.385",
      TRUE ~ NA_character_
    ),
    # Extract Lambda (only "1" or "0")
    Lambda = case_when(
      grepl("lambda_1", FileName) ~ "1",
      grepl("lambda_0", FileName) ~ "0",
      TRUE ~ NA_character_
    ),
  ) %>%
  mutate(
    Lambda = factor(Lambda, levels = c("0", "1")),
  )


```

```{r,eval = FALSE}
#| label: "rilta-rilta-simulation"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

library(parallel)
# Step 1: Create the cluster for parallel processing
num_cores <- detectCores() - 1  # Detect the number of available cores (minus 1)
cl <- makeCluster(num_cores, type = "PSOCK")  # Create the PSOCK cluster

# Step 2: Define the function for the simulation

#Run all models
rilta_rilta_func <- function(N, TPs, Lambda) {
  
  RILTA_RILTA <- mplusObject(
    TITLE = glue("Generate RILTA_RILTA_N = {N}_TP = {TPs}_TH_1"),

    MONTECARLO =
      glue("NAMES = u11-u15 u21-u25;
      GENERATE = u11-u15 u21-u25(1);
      CATEGORICAL = u11-u15 u21-u25;
      GENCLASSES = c1(2) c2(2);
      CLASSES = c1(2) c2(2);
      NOBSERVATIONS = {N};
      SEED = 07252005;
      NREPS = 500;
      !!SAVE = repM1*.dat;
      RESULTS = RILTA_RILTA_N_{N}_TP_{TPs}_TH_1_lambda_{Lambda}.csv;"),

    ANALYSIS =
      "TYPE = MIXTURE;
      algorithm = integration;
      processors = 24;
      starts=50 10;
      logcriterion=0.00001;
      mconv=0.00001;",

    MODELPOPULATION = glue("	
        %OVERALL%

       [c1#1-c2#1*0];
      	c2#1 on c1#1*{TPs};
      	
       f by u11-u15*{Lambda} (p1-p5)
            u21-u25*{Lambda} (p1-p5);
        f@1;
        [f@0];
        
      MODEL POPULATION-c1:
        %c1#1%
     [u11$1*1 u12$1*1 u13$1*1 u14$1*1 u15$1*1] (p111-p115);

        %c1#2%
     [u11$1*-1 u12$1*-1 u13$1*-1 u14$1*-1 u15$1*-1] (p121-p125);

      MODEL POPULATION-c2:  
        %c2#1%
     [u21$1*1 u22$1*1 u23$1*1 u24$1*1 u25$1*1] (p111-p115);

        %c2#2%
     [u21$1*-1 u22$1*-1 u23$1*-1 u24$1*-1 u25$1*-1] (p121-p125);
       "),
     
    MODEL =
      glue("	
        %OVERALL%
          [c1#1-c2#1*0](par1-par2);
        	c2#1 on c1#1*{TPs} (par11);
        	
       f by u11-u15*{Lambda} (p1-p5)
            u21-u25*{Lambda} (p1-p5);
      	f@1;
        [f@0];

     MODEL c1:
        %c1#1%
     [u11$1*1 u12$1*1 u13$1*1 u14$1*1 u15$1*1] (p111-p115);

        %c1#2%
     [u11$1*-1 u12$1*-1 u13$1*-1 u14$1*-1 u15$1*-1] (p121-p125);

    MODEL c2: 	
        %c2#1%
     [u21$1*1 u22$1*1 u23$1*1 u24$1*1 u25$1*1] (p111-p115);

        %c2#2%
     [u21$1*-1 u22$1*-1 u23$1*-1 u24$1*-1 u25$1*-1] (p121-p125);
	      "),
      
    MODELCONSTRAINT =
      if (TPs == 1.385) {
        glue("
        New(
        trans11*.80 trans12*.20 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.65 prob22*.35);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
      } 
             else if (TPs == .85) {
        glue("
        New(
        trans11*.70 trans12*.30 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.60 prob22*.4);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
            else  if (TPs == .41) {
        glue("
        New(
        trans11*.60 trans12*.40 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.55 prob22*.45);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
             else if (TPs == -.41) {
        glue("
        New(
        trans11*.40 trans12*.60 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.45 prob22*.55);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
             else if (TPs == -.85) {
        glue("
        New(
        trans11*.30 trans12*.70 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.40 prob22*.60);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
        
        else if (TPs == -1.385) {
        glue("
         New(
        trans11*.20 trans12*.80 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.35 prob22*.65);

        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;")
      }
  )

  # Run Mplus model
  RILTA_RILTA_Model<- mplusModeler(RILTA_RILTA, 
                                   dataout = here("Simulations", "STUDY_1", "2 Time Points", "TEMP R R", glue("RILTA_RILTA_N_{N}_TP_{TPs}_TH_1_lambda_{Lambda}.dat")),
                                   modelout = glue(here("Simulations", "STUDY_1", "2 Time Points", "TEMP R R",  "RILTA_RILTA_N_{N}_TP_{TPs}_TH_1_lambda_{Lambda}.inp")),
                                   check = TRUE, run = TRUE, hashfilename = FALSE)
return(RILTA_RILTA_Model)
}

# Step 3: Export necessary objects to the cluster
clusterExport(cl, c("rilta_rilta_func", "p1", "here", "glue", "mplusModeler", "mplusObject"))

# Ensure necessary libraries are loaded on each cluster node
clusterEvalQ(cl, {
  library(MplusAutomation)
  library(glue)
  library(here)
})

# Step 4: Run the simulation in parallel using the cluster
result_list <- parLapply(cl, 1:nrow(p1), function(i) {
  rilta_rilta_func(p1$N[i], p1$TPs[i], p1$Lambda[i])
})

# Step 5: Stop the cluster after the simulation
stopCluster(cl)

library(here)

# Write the CSV file using the `here` package
write.csv(p1, file = here("Simulations", "STUDY_1", "2 Time Points", "TEMP R R", "p1_extracted_data.csv"), row.names = FALSE)

p1 <- read.csv(here("Simulations", "STUDY_1", "2 Time Points", "TEMP R R", "p1_extracted_data.csv"))

```

------------------------------------------------------------------------

## [Check for Label Switching and Errors]{.underline}

> In this section: .csv files are first merged into a single data frame, from which specific parameters are extracted. Logit values are then converted to probabilities, and known class values are incorporated into the data frame. A subset of cases involving label switching is selected randomly and plotted for visual review. Output files are scanned for errors, which are subsequently merged back into the original data file. Additional columns derived from the file name are added, and the percentage of violations is calculated. Both errors and label switching violations are visually represented, and the total number of corrected replications is reported.

*First, Load all CSV files and combine them into a single data frame.*

```{r}
#| label: "combine-csv-files-parallel"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false

# Step 1: Set the correct CSV directory
csv_directory <- here('Simulations', 'STUDY_1', '2 Time Points', '4_2T_RILTA_GEN_RILTA_ANALYZED')

# Step 2: Source the child document
source(here('Child_Docs', 'data_scraping.R'))

# Will return combined_data dataframe
```

*Extract data from the appropriate rows from each 9-row chunk and prepare the data for further processing.*

```{r}
#| label: "scrape-rows-process-data-parallel"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false

# Step 2: Process the data using the child script
source(here('Child_Docs', 'step2_2t_RILTA.R'))

# Will populate final_combined_data dataframe in global environment
```

*Convert the logits to probabilities and add the known actual values to each row.*

```{r}
#| label: "convert-logits-to-probabilities"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false

# Step 3: Process the data and return results
source(here('Child_Docs', 'step_3.R'))

# The objects `final_data_with_actuals` and `violators` should now be in the global environment
```

*Generate plots of randomly sampled violators for visual inspection using parallel processing.*

```{r, eval=FALSE}
#| label: "plot-violators"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Set plot width and height
plot_width <- 8
plot_height <- 6

# Take a random sample of up to 250 violators (ensure not to exceed the total number of violators)
set.seed(123)  # For reproducibility
sample_size <- min(nrow(violators), 250)  # Handle cases where fewer than 250 violators exist
sampled_violators <- violators[sample(nrow(violators), sample_size), ]

# Define the function to create plots sequentially
plot_violator <- function(i) {
  row_data <- sampled_violators[i, ]
  
  # Extract the file name from the current row
  file_name <- row_data$FileName

  # Extract probability values for EC1 and EC2 (estimated probabilities) and AC1 and AC2 (actuals)
  estimated_probabilities <- c(
    as.numeric(row_data[c("Ec1u1", "Ec1u2", "Ec1u3", "Ec1u4", "Ec1u5")]),
    as.numeric(row_data[c("Ec2u1", "Ec2u2", "Ec2u3", "Ec2u4", "Ec2u5")])
  )
  
  actual_values <- c(
    as.numeric(row_data[c("Ac1u1", "Ac1u2", "Ac1u3", "Ac1u4", "Ac1u5")]),
    as.numeric(row_data[c("Ac2u1", "Ac2u2", "Ac2u3", "Ac2u4", "Ac2u5")])
  )
  
  # Create labels for the legend with actual values directly from the dataset
  labels <- c(
    paste0("EC1: (", round(row_data$Ec1u1, 3), ", ", round(row_data$Ec1u2, 3), ", ", round(row_data$Ec1u3, 3), ", ", round(row_data$Ec1u4, 3), ", ", round(row_data$Ec1u5, 3), ")"),
    paste0("EC2: (", round(row_data$Ec2u1, 3), ", ", round(row_data$Ec2u2, 3), ", ", round(row_data$Ec2u3, 3), ", ", round(row_data$Ec2u4, 3), ", ", round(row_data$Ec2u5, 3), ")"),
    paste0("AC1: (", round(row_data$Ac1u1, 3), ", ", round(row_data$Ac1u2, 3), ", ", round(row_data$Ac1u3, 3), ", ", round(row_data$Ac1u4, 3), ", ", round(row_data$Ac1u5, 3), ")"),
    paste0("AC2: (", round(row_data$Ac2u1, 3), ", ", round(row_data$Ac2u2, 3), ", ", round(row_data$Ac2u3, 3), ", ", round(row_data$Ac2u4, 3), ", ", round(row_data$Ac2u5, 3), ")")
  )

  # Step 6: Create a data frame for plotting
  plot_data <- data.frame(
    Items = rep(1:5, 4),
    Probabilities = c(estimated_probabilities, actual_values),
    Class = rep(labels, each = 5)
  )

  # Step 7: Create the plot with the file name in the title
  p <- ggplot(plot_data, aes(x = Items, y = Probabilities, color = Class, group = Class)) +
    geom_line(size = 1.2) +
    geom_point(size = 3) +
    labs(title = file_name, x = "Items", y = "Probabilities") +  # Only the file name in the title
    theme_minimal(base_size = 16) +
    theme(panel.background = element_rect(fill = "white"),
          plot.background = element_rect(fill = "white"),
          plot.title = element_text(size = 14, hjust = 0.5)) +  # Adjust title size and center
    scale_color_manual(values = c(
      "darkblue", "darkgreen",  # EC1 and EC2 (Estimated Probabilities)
      "lightblue", "lightgreen"  # AC1 and AC2 (Actual Values)
    ))

  ggsave(filename = file.path("z2t_rilta_rilta_violator_plots", paste0("violator_plot_", i, "_", file_name, ".png")),
         plot = p, width = plot_width, height = plot_height)
}

# Apply the function to generate plots sequentially (without parallelization)
invisible(lapply(1:sample_size, plot_violator))

```

*Scrape output files for errors*

```{r}
#| label: "process-out-files-parallel"
#| echo: true
#| message: false
#| warning: false

# Step 1: Set the correct output directory for .out files
output_folder <- here('Simulations', 'STUDY_1', '2 Time Points', '1_2T_LTA_GEN_LTA_ANALYZED')

# Step 2: Source the child document that processes .out files
source(here('Child_Docs', 'out_scraping.R'))


```

*Create Visual Summary of Errors per Condition*

```{r}
#| label: "create-table-errors"
#| echo: true
#| message: false
#| warning: false

replication_summary_table <- replication_summary %>%
  gt() %>%
  tab_header(
    title = "Replication Summary",
    subtitle = paste0("Folder: ", output_folder)
  ) %>%
  fmt_number(
    columns = c("ErrorRate"),  # Use the actual column name, ensure it matches your dataframe
    decimals = 2
  ) %>%
  cols_label(
    FileName = "File Name",
    CompletedReplications = "Completed Replications",
    RequestedReplications = "Requested Replications",
    ErrorReplications = "Replications with Errors",
    GoodReplications = "Good Replications",
    ErrorRate = "Error Rate (%)"
  ) %>%
  cols_align(
    align = "center",
    columns = everything()  # This aligns all columns' text to center
  ) %>%
  tab_options(
    table.font.size = "small",
    heading.title.font.size = "medium",
    heading.subtitle.font.size = "small",
    table.width = pct(80)  # Adjust this percentage to reduce the width of the table
  )

# Display the table
replication_summary_table
#Compare rows in both .csv and .out data frames
cat("Rows in final_data_with_actuals:", nrow(final_data_with_actuals), "\n")
# Output the final number of rows to confirm data handling
cat("Number of rows in error_summary: ", nrow(error_summary), "\n")
cat("Number of rows in replication_summary: ", nrow(replication_summary), "\n")

```

*Merge Errors with Main Data*

```{r}
#| label: "merge-errors"
#| echo: true
#| message: false
#| warning: false

# Step 2: Process the data using the child script
source(here('Child_Docs', 'merge_errors.R'))
```

*Create Column Names from the File Name*

```{r}
#| label: "create-column-names-from-filename"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false

# Add new columns based on the information in the FileName
final_data_with_actuals <- final_data_with_actuals %>%
  mutate(
    N = case_when(
      grepl("n_4000", FileName) ~ 4000,
      grepl("n_500", FileName) ~ 500,
      grepl("n_1000", FileName) ~ 1000,
      grepl("n_2000", FileName) ~ 2000,
      TRUE ~ NA_integer_
    ),
    Population = case_when(
      grepl("tp_1.385", FileName) ~ ".800",
      grepl("tp_0.85", FileName) ~ ".700",
      grepl("tp_0.41", FileName) ~ ".600",
      grepl("tp_-0.41", FileName) ~ ".400",
      grepl("tp_-0.85", FileName) ~ ".300",
      grepl("tp_-1.385", FileName) ~ ".200",
      TRUE ~ NA_character_
    ),
    Lambda = case_when(
      grepl("a_1$", FileName) ~ "1",
      grepl("a_1\\.5", FileName) ~ "1.5",
      grepl("a_2$", FileName) ~ "2",
      grepl("a_2\\.5", FileName) ~ "2.5",
      grepl("a_3$", FileName) ~ "3",
      grepl("a_0", FileName) ~ "0",
      TRUE ~ NA_character_
    ),
    Transitions = case_when(
      Population %in% c(".200", ".300", ".400") ~ 1,
      Population %in% c(".600", ".700", ".800") ~ 2,
      TRUE ~ NA_integer_
    )
  ) %>%
  mutate(
    N = factor(N, levels = c(4000, 500, 1000, 2000), labels = c("N = 4000", "N = 500", "N = 1000", "N = 2000")),
    Population = factor(Population, levels = c(".800", ".700", ".600", ".400", ".300", ".200")),
    Transitions = factor(Transitions, levels = c(1, 2), labels = c("Mover", "Stayer"))
  )
```

*Calculate Violation Percentages per Condition*

```{r}
#| label: "calculate-violations"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false

# 1. Summarize violations per condition
violation_summary <- final_data_with_actuals %>%
  mutate(
    Any_Violation = ifelse(is.na(Any_Violation), 0, Any_Violation),
    ErrorFlag = ifelse(is.na(ErrorFlag), 0, ErrorFlag)  # Ensure no missing values for ErrorFlag
  ) %>%
  group_by(FileName, Population, N, Lambda) %>%
  summarize(
    Total_Rows = n(),                                # Total runs
    Total_Violations = sum(Any_Violation, na.rm = TRUE), # Total violations
    Total_Errors = sum(ErrorFlag, na.rm = TRUE),         # Total errors from ErrorFlag
    Percentage_Violations = (Total_Violations / Total_Rows) * 100, # % violations
    .groups = "drop"
  ) %>%
  # 2. Calculate Replications Needed for label switching
  mutate(
    N_numeric = as.numeric(gsub("N = ", "", as.character(N))), 
    Additional_Runs = (500 + Total_Violations) * (Percentage_Violations / 100), 
    Replications_Needed = ceiling(500 + Total_Violations + Additional_Runs + 20),
    Replications_Needed = if_else(Replications_Needed < 500, 500, Replications_Needed),
    ErrorRate = Total_Errors / Total_Rows,  # Calculate ErrorRate directly
    Adjusted_Replications_Needed = ceiling(Replications_Needed / (1 - ErrorRate)),
    Adjusted_Replications_Needed = if_else(Adjusted_Replications_Needed < 500, 500, Adjusted_Replications_Needed)
  ) %>%
  select(
    FileName, Population, N, N_numeric,Lambda, Total_Rows, Total_Violations, Total_Errors, ErrorRate, Percentage_Violations, Replications_Needed,
    Adjusted_Replications_Needed
  )
```

*Summarize & Visualize Label Switching Percentage Results*

```{r}
#| label: "summarize-violations"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false

violation_summary <- violation_summary %>%
 mutate(
    TPs = case_when(
      Population == ".800" ~ 1.385,
      Population == ".700" ~ 0.85,
      Population == ".600" ~ 0.41,
      Population == ".400" ~ -0.41,
      Population == ".300" ~ -0.85,
      Population == ".200" ~ -1.385,
      TRUE ~ NA_real_
    ),
    # Clean `N_numeric` by stripping out "N = " and converting to numeric
    N_numeric = as.numeric(gsub("N = ", "", as.character(N)))
  )

# Summarize and visualize the final table
final_table <- violation_summary %>%
  select(
    `Transition Probability` = Population,               # Transition probabilities
    TPs,                                    # Logit values
    Lambda,                                 # Lambda values
    N_numeric,              # Cleaned sample size
    `Total Mplus Runs` = Total_Rows,        # Total runs
    Total_Violations,                       # Total violations
    `% of Violations` = Percentage_Violations, # Violation percentage
    ErrorRate,                              # Error rate
    Replications_Needed,                    # Replications needed
    `Adjusted Replications Needed` = Adjusted_Replications_Needed # Adjusted replications needed
  ) %>%
  gt() %>%
  tab_header(
    title = "Monte Carlo Results:",
    subtitle = "Percentage of Cases with Label Switching and Replications Needed"
  ) %>%
  cols_align(
    align = "center",  # Center all columns
    columns = everything()
  ) %>%
  fmt_number(
    columns = c(`N_numeric`, `Total Mplus Runs`, Total_Violations, `% of Violations`,
                ErrorRate, Replications_Needed, `Adjusted Replications Needed`),
    decimals = 2  # Format numbers with two decimal places
  ) %>%
  tab_options(
    data_row.padding = px(4)  # Set padding between rows
  ) %>%
  tab_style(
    style = cell_text(align = "center"),  # Center align the headers only
    locations = cells_column_labels(everything())  # Apply to headers only
  )

# Display the table
final_table
```

------------------------------------------------------------------------

# Part 2: Re Run Simulation

------------------------------------------------------------------------

#### Re-Run Simulation with Dynamic Replication Conditions

```{r, eval=FALSE}
#| label: "rilta-rilta2-simulation"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

  # Define the Mplus object with the dynamic replications
rilta_rilta_func <- function(TPs, Lambda, N_numeric,  Adjusted_Replications_Needed) {
  
  RILTA_RILTA <- mplusObject(
    TITLE = glue("Generate RILTA_RILTA_N_{N_numeric}_TP_{TPs}_1_Lambda_{Lambda}"),

    MONTECARLO =
      glue("NAMES = u11-u15 u21-u25;
      GENERATE = u11-u15 u21-u25(1);
      CATEGORICAL = u11-u15 u21-u25;
      GENCLASSES = c1(2) c2(2);
      CLASSES = c1(2) c2(2);
      NOBSERVATIONS = {N_numeric};
      SEED = 07252005;
      NREPS = {Adjusted_Replications_Needed};  ! Dynamic number of Replications_Needed
      !!SAVE = repM1*.dat;
      RESULTS = RILTA_RILTA_N_{N_numeric}_TP_{TPs}_TH_1_Lambda_{Lambda}.csv;"),

    ANALYSIS =
      "TYPE = MIXTURE;
      algorithm = integration;
      processors = 24;
      starts=50 10;
      logcriterion=0.00001;
      mconv=0.00001;",

    MODELPOPULATION = glue("	
        %OVERALL%

       [c1#1-c2#1*0];
      	c2#1 on c1#1*{TPs};
      	
       f by u11-u15*{Lambda} (p1-p5)
            u21-u25*{Lambda} (p1-p5);
        f@1;
        [f@0];
        
      MODEL POPULATION-c1:
        %c1#1%
     [u11$1*1 u12$1*1 u13$1*1 u14$1*1 u15$1*1] (p111-p115);

        %c1#2%
     [u11$1*-1 u12$1*-1 u13$1*-1 u14$1*-1 u15$1*-1] (p121-p125);

      MODEL POPULATION-c2:  
        %c2#1%
     [u21$1*1 u22$1*1 u23$1*1 u24$1*1 u25$1*1] (p111-p115);

        %c2#2%
     [u21$1*-1 u22$1*-1 u23$1*-1 u24$1*-1 u25$1*-1] (p121-p125);
       "),
     
    MODEL =
      glue("	
        %OVERALL%
          [c1#1-c2#1*0](par1-par2);
        	c2#1 on c1#1*{TPs} (par11);
        	
       f by u11-u15*{Lambda} (p1-p5)
            u21-u25*{Lambda} (p1-p5);
      	f@1;
        [f@0];

     MODEL c1:
        %c1#1%
     [u11$1*1 u12$1*1 u13$1*1 u14$1*1 u15$1*1] (p111-p115);

        %c1#2%
     [u11$1*-1 u12$1*-1 u13$1*-1 u14$1*-1 u15$1*-1] (p121-p125);

    MODEL c2: 	
        %c2#1%
     [u21$1*1 u22$1*1 u23$1*1 u24$1*1 u25$1*1] (p111-p115);

        %c2#2%
     [u21$1*-1 u22$1*-1 u23$1*-1 u24$1*-1 u25$1*-1] (p121-p125);
	      "),
      
    MODELCONSTRAINT =
      if (TPs == 1.385) {
        glue("
        New(
        trans11*.80 trans12*.20 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.65 prob22*.35);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
      } 
             else if (TPs == 0.85) {
        glue("
        New(
        trans11*.70 trans12*.30 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.60 prob22*.4);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
            else  if (TPs == 0.41) {
        glue("
        New(
        trans11*.60 trans12*.40 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.55 prob22*.45);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
             else if (TPs == -0.410) {
        glue("
        New(
        trans11*.40 trans12*.60 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.45 prob22*.55);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
             else if (TPs == -0.85) {
        glue("
        New(
        trans11*.30 trans12*.70 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.40 prob22*.60);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
        
        else if (TPs == -1.385) {
        glue("
         New(
        trans11*.20 trans12*.80 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.35 prob22*.65);

        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;")
      }
  )

  # Run Mplus model
  RILTA_RILTA_Model<- mplusModeler(RILTA_RILTA, 
                                   dataout = here('Simulations', 'STUDY_1', '2 Time Points', '4_2T_RILTA_GEN_RILTA_ANALYZED_REP', glue("RILTA_RILTA_N_{N_numeric}_TP_{TPs}_TH_1_Lambda_{Lambda}.dat")),
                                   modelout = glue(here('Simulations', 'STUDY_1', '2 Time Points', '4_2T_RILTA_GEN_RILTA_ANALYZED_REP', "RILTA_RILTA_N_{N_numeric}_TP_{TPs}_TH_1_Lambda_{Lambda}.inp")),
                                   check = TRUE, run = TRUE, hashfilename = FALSE)
return(RILTA_RILTA_Model)
}

# Start the cluster
num_cores <- detectCores() - 1

# Step 2: Select the cluster type based on the system (PSOCK for Windows, FORK for macOS/Linux)
cluster_type <- ifelse(.Platform$OS.type == "windows", "PSOCK", "FORK")

cl <- makeCluster(num_cores, type = cluster_type)

# Export necessary objects to the cluster
invisible(clusterExport(cl, c("rilta_rilta_func", "violation_summary_df", "here", "glue", "mplusModeler", "mplusObject")))

# Ensure required libraries are loaded on each node
invisible(clusterEvalQ(cl, {
  library(MplusAutomation)
  library(glue)
  library(here)
}))

result_list <- parLapply(cl, 1:nrow(violation_summary_df), function(i) {
  rilta_rilta_func(
    violation_summary_df$TPs[i],  
    violation_summary_df$Lambda[i], 
    violation_summary_df$N_numeric[i], 
    violation_summary_df$Adjusted_Replications_Needed[i]
  )
})

# Stop the cluster after the simulation
stopCluster(cl)
```

------------------------------------------------------------------------

## Check for Label Switching and Errors

> In this section: we re conduct the steps for aggregating the label switching and errors to guarantee that we will have at minimum 500 replications per condition.

------------------------------------------------------------------------

*Load all CSV files and combine them into a single data frame.*

```{r}
#| label: "combine-csv-files-parallel2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false

# Step 1: Set the correct CSV directory
csv_directory <- here('Simulations', 'STUDY_1', '2 Time Points', '4_2T_RILTA_GEN_RILTA_ANALYZED_REP')

# Step 2: Source the child document
source(here('Child_Docs', 'data_scraping.R'))
```

*Extract data from the appropriate rows from each 9-row chunk and prepare the data for further processing.*

```{r}
#| label: "scrape-rows-process-data-parallel2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false

# Step 2: Process the data using the child script
source(here('Child_Docs', 'step2_2t_LTA.R'))
```

*Convert the logits to probabilities and add the known actual values to each row.*

```{r}
#| label: "convert-logits-to-probabilities2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false

# Step 3: Process the data and return results
source(here('Child_Docs', 'step_3.R'))

# The objects `final_data_with_actuals` and `violators` should now be in the global environment
```

*Scrape Output Files for Errors*

```{r}
#| label: "process-out-files-parallel2"
#| echo: true
#| message: false
#| warning: false

# Step 1: Set the correct output directory for .out files
output_folder <- here('Simulations', 'STUDY_1', '2 Time Points', '4_2T_RILTA_GEN_RILTA_ANALYZED_REP')

# Step 2: Source the child document that processes .out files
source(here('Child_Docs', 'out_scraping.R'))
```

***Create Visual Summary of Errors per Condition***

```{r}
#| label: "create-table-errors2"
#| echo: true
#| message: false
#| warning: false

replication_summary_table <- replication_summary %>%
  gt() %>%
  tab_header(
    title = "Replication Summary",
    subtitle = paste0("Folder: ", output_folder)
  ) %>%
  fmt_number(
    columns = c("ErrorRate"),  # Use the actual column name, ensure it matches your dataframe
    decimals = 2
  ) %>%
  cols_label(
    FileName = "File Name",
    CompletedReplications = "Completed Replications",
    RequestedReplications = "Requested Replications",
    ErrorReplications = "Replications with Errors",
    GoodReplications = "Good Replications",
    ErrorRate = "Error Rate (%)"
  ) %>%
  cols_align(
    align = "center",
    columns = everything()  # This aligns all columns' text to center
  ) %>%
  tab_options(
    table.font.size = "small",
    heading.title.font.size = "medium",
    heading.subtitle.font.size = "small",
    table.width = pct(80)  # Adjust this percentage to reduce the width of the table
  )

# Display the table
replication_summary_table

#Compare rows in both .csv and .out data frames
cat("Rows in final_data_with_actuals:", nrow(final_data_with_actuals), "\n")
# Output the final number of rows to confirm data handling
cat("Number of rows in error_summary: ", nrow(error_summary), "\n")
cat("Number of rows in replication_summary: ", nrow(replication_summary), "\n")
```

*Merge Errors with Main Data*

```{r}
#| label: "merge-errors2"
#| echo: true
#| message: false
#| warning: false

# Step 2: Process the data using the child script
source(here('Child_Docs', 'merge_errors.R'))
```

*Create Column Names from the File Name*

```{r}
#| label: "create-column-names-from-filename2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false

# Add new columns based on the information in the FileName
final_data_with_actuals <- final_data_with_actuals %>%
  mutate(
    N = case_when(
      grepl("n_4000", FileName) ~ 4000,
      grepl("n_500", FileName) ~ 500,
      grepl("n_1000", FileName) ~ 1000,
      grepl("n_2000", FileName) ~ 2000,
      TRUE ~ NA_integer_
    ),
    Population = case_when(
      grepl("tp_1.385", FileName) ~ ".800",
      grepl("tp_0.85", FileName) ~ ".700",
      grepl("tp_0.41", FileName) ~ ".600",
      grepl("tp_-0.41", FileName) ~ ".400",
      grepl("tp_-0.85", FileName) ~ ".300",
      grepl("tp_-1.385", FileName) ~ ".200",
      TRUE ~ NA_character_
    ),
    Lambda = case_when(
      grepl("a_1$", FileName) ~ "1",
      grepl("a_1\\.5", FileName) ~ "1.5",
      grepl("a_2$", FileName) ~ "2",
      grepl("a_2\\.5", FileName) ~ "2.5",
      grepl("a_3$", FileName) ~ "3",
      grepl("a_0", FileName) ~ "0",
      TRUE ~ NA_character_
    ),
    Transitions = case_when(
      Population %in% c(".200", ".300", ".400") ~ 1,
      Population %in% c(".600", ".700", ".800") ~ 2,
      TRUE ~ NA_integer_
    )
  ) %>%
  mutate(
    N = factor(N, levels = c(4000, 500, 1000, 2000), labels = c("N = 4000", "N = 500", "N = 1000", "N = 2000")),
    Population = factor(Population, levels = c(".800", ".700", ".600", ".400", ".300", ".200")),
    Transitions = factor(Transitions, levels = c(1, 2), labels = c("Mover", "Stayer"))
  )
```

*Calculate Violation Percentages per Condition after resimulation*

```{r}
#| label: "calculate-violations2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false

violation_summary <- final_data_with_actuals %>%
  mutate(
    Any_Violation = ifelse(is.na(Any_Violation), 0, Any_Violation),
    ErrorFlag = ifelse(is.na(ErrorFlag), 0, ErrorFlag)  # No missing values for ErrorFlag
  ) %>%
  group_by(FileName, Population, N, Lambda) %>%
  summarize(
    Total_Rows = n(),                                # Total runs
    Total_Violations = sum(Any_Violation, na.rm = TRUE), # Sum of violations
    Total_Errors = sum(ErrorFlag, na.rm = TRUE),     # Sum of errors
    Percentage_Violations = (Total_Violations / Total_Rows) * 100, # Percentage of violations
    .groups = "drop"
  ) %>%
  left_join(replication_summary %>% select(FileName, CompletedReplications), by = "FileName") %>%
  mutate(
    CompletedReplications = ifelse(is.na(CompletedReplications), 500, CompletedReplications),
    N_numeric = as.numeric(gsub("N = ", "", N)),
    GoodReplications = CompletedReplications - Total_Violations - Total_Errors,
    GoodReplications = ifelse(GoodReplications < 0, 0, GoodReplications),
    Reanalysis_Needed = if_else(GoodReplications >= 500, "No", "Yes"),  # Flag for reanalysis needed
    ErrorRate = Total_Errors / Total_Rows,          # Calculate ErrorRate
    Replications_Needed = ceiling(CompletedReplications + Total_Violations + (CompletedReplications * Percentage_Violations / 100) + 20),
    Adjusted_Replications_Needed = ceiling(Replications_Needed / (1 - ErrorRate))
  ) %>%
  select(
    FileName, Population, N, N_numeric,
    Lambda, Total_Rows, Total_Violations, Total_Errors,
    ErrorRate, Percentage_Violations, GoodReplications,
    Replications_Needed, Adjusted_Replications_Needed,
    Reanalysis_Needed
  )
```

*Summarize & Visualize Label Switching Percentage Results*

```{r}
#| label: "summarize-violations2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false

# Filter the data for rows with non-zero error rates and clean N_numeric
# Modify the TPs assignment to force formatting
violation_summary <- violation_summary %>%
  mutate(
    TPs = case_when(
      Population == ".800" ~ formatC(1.385, format = "f", digits = 3),
      Population == ".700" ~ formatC(0.85, format = "f", digits = 2),
      Population == ".600" ~ formatC(0.41, format = "f", digits = 2),
      Population == ".400" ~ formatC(-0.41, format = "f", digits = 2),
      Population == ".300" ~ formatC(-0.85, format = "f", digits = 2),
      Population == ".200" ~ formatC(-1.385, format = "f", digits = 3),
      TRUE ~ NA_character_
  ),
    N_numeric = as.numeric(gsub("N = ", "", as.character(N)))  # Clean N_numeric by stripping out "N = "
  )

# Summarize and visualize the final table
final_table <- violation_summary %>%
  select(
    `Transition Probability` = Population,  # Rename column for clarity
    TPs,  # Logit values
    Lambda = Lambda,  # Lambda values
    N_numeric,  # Cleaned sample size
    `Total Mplus Runs` = Total_Rows,  # Total runs
    Total_Violations,  # Total violations
    `% of Violations` = Percentage_Violations,  # Violation percentage
    Total_Errors, 
    ErrorRate,  # Error rate
    Replications_Needed,  # Replications needed
    `Adjusted Replications Needed` = Adjusted_Replications_Needed,  # Adjusted replications needed
    Reanalysis_Needed
  ) %>%
  gt() %>%
  tab_header(
    title = "Monte Carlo Results:",
    subtitle = "Percentage of Cases with Label Switching and Replications Needed"
  ) %>%
  cols_align(
    align = "center",  # Center all columns
    columns = everything()
  ) %>%
  fmt_number(
    columns = c(`% of Violations`, ErrorRate),
    decimals = 2  # Ensure these percentages are formatted to 2 decimal places
  ) %>%
  tab_options(
    data_row.padding = px(4)  # Set padding between rows
  ) %>%
  tab_style(
    style = cell_text(align = "center"),  # Center align the headers only
    locations = cells_column_labels(everything())  # Apply to headers only
  )

# Display the table
final_table
```

*Create Follow up iterator table*

```{r}
#| label: "visualize-violations-and-errors-2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false

# Assuming 'violation_summary' has been previously defined and transformed

# Filter for rows where Reanalysis_Needed is 'Yes' and select specific columns
# Save this filtered data as a DataFrame
violation_summary_df <- violation_summary %>%
  filter(Reanalysis_Needed == "Yes") %>%
  select(
    TPs,                     # Logit values
    Lambda,  # Lambda values
    N_numeric,               # Cleaned sample size
    Adjusted_Replications_Needed  # Adjusted replications needed
  )

# Use the saved DataFrame to create the gt table
violation_summary_table <- gt(data = violation_summary_df) %>%
  tab_header(
    title = "Monte Carlo Results for Reanalysis:",
    subtitle = "Selected cases requiring further analysis"
  ) %>%
  cols_align(
    align = "center",  # Center all columns
    columns = everything()
  ) %>%
  tab_options(
    data_row.padding = px(4)  # Set padding between rows
  ) %>%
  tab_style(
    style = cell_text(align = "center"),  # Center align the headers only
    locations = cells_column_labels(everything())  # Apply to headers only
  )

# Display the table
violation_summary_table
```

*Filter out cases with any violations, leaving only the clean data.*

```{r}
#| label: "delete-cases"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false

# Filter out cases with no violations and no errors
filtered_data_with_no_violations <- final_data_with_actuals[
  final_data_with_actuals$Any_Violation == 0 & final_data_with_actuals$ErrorFlag == 0, ]
```

*Randomly select 500 replications per condition*

```{r}
#| label: "select-500-random-replication"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false

# Set seed for reproducibility
set.seed(07252005)

# Check initial counts to ensure each FileName group has at least 500 rows
initial_counts <- filtered_data_with_no_violations %>%
  group_by(FileName) %>%
  summarize(Initial_Count = n(), .groups = "drop")

# Forcefully display the condition and message
if (all(initial_counts$Initial_Count >= 500)) {
  # Message to indicate successful operation
  cat("All FileName groups had at least 500 rows. Sampling performed.\n")

  # Proceed with sampling
  sampled_data <- filtered_data_with_no_violations %>%
    group_by(FileName) %>%
    sample_n(size = 500, replace = FALSE) %>%
    ungroup()
} else {
  # Warning if not all groups meet the requirement
  cat("Some FileName groups do not have at least 500 rows. Sampling not performed.\n")
}
```

------------------------------------------------------------------------

### **Prepare Final Data for Visualization**

*Calculate Monte Carlo values for `TRANS11`, including population values, averages, standard errors, Mean Squared Error (MSE), coverage, and power.*

```{r}
#| label: "compute-mc-values"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false

# Compute Monte Carlo Values
source(here('Child_Docs', 'compute_mc_lambda.R'))
```

*Calculate dichotomous variables for Power and Coverage, compute Parameter and SE Bias, and prepare subsets for movers and stayers.*

```{r}
#| label: "calculate-bias-dichotomous-variables"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false

# Step 1: Calculate dichotomous variable for Power (1 if Power >= 0.8, else 0)
mc_values <- mc_values %>%
  mutate(Power_Dic = ifelse(Power >= 0.8, 1, 0))

# Step 2: Calculate dichotomous variable for Coverage (0 if outside [0.91, 0.98], else 1)
mc_values <- mc_values %>%
  mutate(Coverage_Dic = ifelse(Coverage > 0.98 | Coverage < 0.91, 0, 1))

# Step 3: Remove any groupings before further calculations
mc_values <- mc_values %>%
  ungroup()

# Step 4: Ensure numeric columns are correctly formatted and **convert Population only for calculations**
mc_values <- mc_values %>%
  mutate(
    # Keep average as numeric but **do not convert Population for display purposes**
    average = as.numeric(average),
    population_numeric = as.numeric(Population),  # Create a temporary numeric version of Population
    average_se = as.numeric(average_SE),
    population_sd = as.numeric(population_sd)
  )

# Step 5: Calculate Parameter Bias and SE Bias, rounding the results to 2 decimal places
mc_values <- mc_values %>%
  mutate(
    # Use population_numeric for the calculations, but **retain Population in the original format**
    Parameter_Bias = (average - population_numeric) / population_numeric * 100,  # Bias for the parameter
    SE_Bias = (average_se - population_sd) / population_sd * 100  # Bias for the standard error
  ) %>%
  mutate(across(c(Parameter_Bias, SE_Bias), ~ round(.x, 2)))  # Round to 2 decimal places

# Drop the temporary numeric population column if no longer needed
mc_values <- mc_values %>%
  select(-population_numeric)

```

*Subset the Monte Carlo values data for transitions with movers and stayers based on the population value.*

```{r}
#| label: "subset-data-for-bias-plots"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false


# Assuming Population is numeric in all_data
all_data <- mc_values

# Convert N to a factor with the correct labels for plotting
all_data <- all_data %>%
  mutate(N = factor(N,
                    levels = c("N = 500", "N = 1000", "N = 2000", "N = 4000"),  # These are the existing labels
                    labels = c(`1` = "N = 500", 
                               `2` = "N = 1000", 
                               `3` = "N = 2000", 
                               `4` = "N = 4000")))

# Define the labels for N using expression() for italics, which will be used in plotting
n_labels <- c(
  `1` = expression(italic('N') ~ "= 500"),
  `2` = expression(italic('N') ~ "= 1000"),
  `3` = expression(italic('N') ~ "= 2000"),
  `4` = expression(italic('N') ~ "= 4000")
)

# Assign the labels to the levels
all_data$N <- factor(all_data$N, labels = n_labels)
# Now you can use `n_labels` in the plotting code


# Ensure that Population_Label is correctly prepared
all_data$Population_Label <- factor(all_data$Population, 
    levels = c(0.2, 0.3, 0.4, 0.6, 0.7, 0.8),  # Numeric levels without leading zeros
    labels = c(
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .200"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .300"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .400"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .600"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .700"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .800")
    )
)


# Subset for Transitions movers (already correctly defined as "Mover")
subset_mover <- subset(all_data, Transitions == "Mover")
subset_mover <- subset_mover %>%
  mutate(Lambda = as.numeric(as.character(Lambda)))

# Subset for Transitions stayers (already correctly defined as "Stayer")
subset_stayer <- subset(all_data, Transitions == "Stayer")
subset_stayer <- subset_stayer %>%
  mutate(Lambda = as.numeric(as.character(Lambda)))

```

*Create function to generate plots*

```{r}
#| label: "plot-bias"
#| echo: true
#| message: false
#| warning: false
#| code-fold: false

# Define common themes and aesthetics
common_theme <- theme_minimal() +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.minor.y = element_blank(),
    axis.text.x = element_text(size = 8),
    axis.ticks = element_line(color = "black", linewidth = 0.2),
    legend.position = "bottom",
    legend.title = element_blank(),
    text = element_text(family = "Times New Roman"),
    axis.title.x = element_text(margin = margin(t = 10, b = 10)),
    legend.margin = margin(t = -10),
    plot.caption = element_text(hjust = 0, margin = margin(t = 10))
  )

common_labels <- labs(
  x = "Lambda Loadings on the RI",
  y = "Bias (%)",
  color = "",
  title = ""
)


create_plot <- function(data, title_suffix) {
  # Detect which legend items to show
  present_categories <- c("Parameter Bias", "Standard Error Bias")  # Base categories
  if (any(data$Coverage_Dic == 0)) present_categories <- c(present_categories, "Coverage Failure")
  if (any(data$Power_Dic == 0)) present_categories <- c(present_categories, "Power Failure")

  # Define colors and shapes for different categories
  colors <- c("Parameter Bias" = "#113386", "Standard Error Bias" = "#C830CC", 
              "Coverage Failure" = "#113386", "Power Failure" = "black")
  shapes <- c("Parameter Bias" = 16, "Standard Error Bias" = 18, 
              "Coverage Failure" = 1, "Power Failure" = 4)

  # Filter colors and shapes based on detected categories
  filtered_colors <- colors[present_categories]
  filtered_shapes <- shapes[present_categories]

  ggplot(data = data, aes(x = Lambda, y = Parameter_Bias, color = "Parameter Bias", group = Population_Label)) +  
    geom_line(aes(group = Population_Label), linewidth = 0.3, linetype = "solid") +  
    geom_line(aes(y = SE_Bias, group = Population_Label, color = "Standard Error Bias"), linewidth = 0.3, linetype = "solid") +  
    geom_point(aes(y = Parameter_Bias), shape = 16, size = 1, fill = "#113386", alpha = 0.8) +  
    geom_point(aes(y = SE_Bias, color = "Standard Error Bias"), shape = 18, size = 1, fill = "#C830CC", alpha = 0.8) +  # Corrected
    geom_point(data = subset(data, Coverage_Dic == 0), aes(y = Parameter_Bias, color = "Coverage Failure"), shape = 1, size = 2, fill = "#113386", alpha = 1) +  
    geom_point(data = subset(data, Power_Dic == 0), aes(y = Parameter_Bias, color = "Power Failure"), shape = 4, size = 2, fill = "black", alpha = 1) + 
    scale_color_manual(
      values = filtered_colors, 
      labels = present_categories, 
      breaks = present_categories,
      guide = guide_legend(
        override.aes = list(shape = filtered_shapes)
      )
    ) +  
    labs(
      x = "Lambda Loadings on the RI",
      y = "Bias (%)",
      color = "",
      title = paste("RILTA Generated, LTA Analyzed with", title_suffix, "Transition Probabilities")
    ) +
    coord_cartesian(ylim = c(-40, 40)) +  
    facet_grid(Population_Label ~ N, scales = "free_x", labeller = label_parsed) +  
    scale_x_continuous(breaks = seq(0, 3, by = 0.5), labels = scales::number_format(accuracy = 0.1)) +  
    scale_y_continuous(breaks = seq(-40, 40, by = 10)) +  
    theme_minimal() +
    theme(
      panel.grid.major.x = element_blank(),
      panel.grid.minor.x = element_blank(),
      panel.grid.minor.y = element_blank(),
      axis.text.x = element_text(size = 8),
      axis.ticks = element_line(color = "black", linewidth = 0.2),
      legend.position = "bottom",
      legend.title = element_blank(),
      text = element_text(family = "Times New Roman"),
      axis.title.x = element_text(margin = margin(t = 10, b = 10)),
      legend.margin = margin(t = -10),
      plot.caption = element_text(hjust = 0, margin = margin(t = 10))
    ) +
    geom_hline(yintercept = c(-10, 10), linetype = "dashed", color = "#113386", linewidth = 0.3) +  
    geom_hline(yintercept = c(-5, 5), linetype = "dashed", color = "#C830CC", linewidth = 0.3)
}

```

------------------------------------------------------------------------

### Render Bias Figures

```{r}
# Create and print plot for Mover
plot_mover <- create_plot(subset_mover, "Mover")
#| column: screen
#| fig-format: svg
print(plot_mover)

# Remove title for the saved version
plot_mover_no_title <- plot_mover + labs(title = NULL)

# Save Mover plot without title as .svg
ggsave(here("2 Time Points", "zFIGURES", "x2t_rilta_rilta_plots", "plot_mover.svg"), plot = plot_mover_no_title, width = 6, height = 3, dpi = 300, device = "svg")
```

```{r}
# Create and print plot for Stayer
plot_stayer <- create_plot(subset_stayer, "Stayer")
#| column: screen
#| fig-format: svg
print(plot_stayer)

# Remove title for the saved version
plot_stayer_no_title <- plot_stayer + labs(title = NULL)

# Save Stayer plot without title as .svg
ggsave(here("2 Time Points", "zFIGURES", "x2t_rilta_rilta_plots", "plot_stayer.svg"), plot = plot_stayer_no_title, width = 6, height = 3, dpi = 300, device = "svg")
```

------------------------------------------------------------------------

*Prepare data for heat map creation by ensuring correct formatting for population values, and subsetting the data based on class proportions and sample sizes.*

```{r}

#| label: "prepare-data-for-heatmaps"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 1: Define a function to extract numeric values from the 'N' column
extract_numeric_from_N <- function(N) {
  # Use regex to extract the numeric part (e.g., from 'italic("N") ~ "= 1000"')
  as.numeric(gsub("[^0-9]", "", N))
}

# Step 2: Apply this function to both movers and stayers
movers <- all_data %>%
  filter(Transitions == "Mover") %>%
  mutate(N_numeric = extract_numeric_from_N(N)) %>%  # Create a new column with numeric N
  arrange(N_numeric, Population)

stayers <- all_data %>%
  filter(Transitions == "Stayer") %>%
  mutate(N_numeric = extract_numeric_from_N(N)) %>%  # Create a new column with numeric N
  arrange(N_numeric, Population)

# Step 3: Assign N_Label only once for each N within Movers and Stayers
movers <- movers %>%
  group_by(N) %>%
  mutate(N_Label = ifelse(row_number() == 2, as.character(N), "")) %>%
  ungroup()

stayers <- stayers %>%
  group_by(N) %>%
  mutate(N_Label = ifelse(row_number() == 2, as.character(N), "")) %>%
  ungroup()

# Step 4: Combine movers and stayers back into one dataset, keeping them separate by transitions
all_data_sorted <- bind_rows(movers, stayers)

# Step 5: Ensure Lambda is included and columns are factored
all_data_sorted <- all_data_sorted %>% 
  mutate(N_Label = factor(N, labels = c("N = 500", "N = 1000", "N = 2000", "N = 4000"))) %>% 
  mutate(Population = factor(Population,
                             labels = c(`.2` = ".200", `.3` = ".300", `.4` = ".400",
                                        `.6` = ".600", `.7` = ".700", `.8` = ".800")))

# Step 6: Select necessary columns, including Lambda
test_map <- select(all_data_sorted, N_Label, Population, average, Coverage, Power, Parameter_Bias, SE_Bias, Lambda)

# Step 7: Ordering the table based on the "Lambda" column
test_map <- test_map %>%
  arrange(as.numeric(Lambda))

# Define the population values as characters
population_values <- c(".200", ".300", ".400", ".600", ".700", ".800")

# Function to subset the data for a specific population value
subset_data <- function(data, pop_value) {
  subset <- data %>%
    filter(Population == pop_value)
  return(subset)
}

# Apply the function to each population value
subset_list <- lapply(population_values, function(x) subset_data(test_map, as.character(x)))

# Access the subsets for each population value
subset_02 <- subset_list[[1]]  # Subset for population value .200
subset_03 <- subset_list[[2]]  # Subset for population value .300
subset_04 <- subset_list[[3]]  # Subset for population value .400
subset_06 <- subset_list[[4]]  # Subset for population value .600
subset_07 <- subset_list[[5]]  # Subset for population value .700
subset_08 <- subset_list[[6]]  # Subset for population value .800
```

*Create heat map function to prepare for heat map creation*

```{r}

#| label: "create-heatmap-function"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

create_table <- function(subset, transition_probability) {
  
  # Create the gt object and set initial formatting
  gt_table <- subset %>%
    gt() %>%
    opt_table_font(stack = "geometric-humanist") %>% 
    tab_header(
      title = paste("RILTA Generated & RILTA Analyzed with Transition Probability of", transition_probability)
    ) %>%
    cols_label(
      N_Label = "Sample Size",
      average = "Estimated<br>Probability",
      Coverage = "Coverage",
      Power = "Power",
      Parameter_Bias = "Parameter<br>Bias",
      SE_Bias = "Standard Error<br>Bias",
      .fn = md
    ) %>%
    tab_spanner(
      label = "Bias",
      columns = c("Parameter_Bias", "SE_Bias")) %>%
    tab_row_group(
      label = "Lambda RI Loading of 3 (λ)",  # Label for the first subgroup
      rows = c(21:24)  # Rows corresponding to the first subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 2.5 (λ)",  # Label for the second subgroup
      rows = c(17:20)  # Rows corresponding to the second subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 2 (λ)",  # Label for the third subgroup
      rows = c(13:16)  # Rows corresponding to the third subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 1.5 (λ)",  # Label for the fourth subgroup
      rows = c(9:12)  # Rows corresponding to the fourth subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 1 (λ)",  # Label for the fifth subgroup
      rows = c(5:8)  # Rows corresponding to the fifth subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 0 (λ)",  # Label for the sixth subgroup
      rows = c(1:4)  # Rows corresponding to the sixth subgroup
    ) %>%
      tab_style(
    style = cell_text(
      font = "bold italic"  # Apply bold and italic styling
    ),
      locations = cells_row_groups()  # Apply style to row subheaders
    ) %>% 
    fmt_number(columns = c("Parameter_Bias", "SE_Bias"), decimals = 2) %>%  
    fmt_number(columns = 3, decimals = 3) %>%
    tab_options(
      table_body.hlines.color = "white",
      table.border.top.color = "black",
      table.border.bottom.color = "black",
      table_body.border.bottom.color = "black",
      heading.border.bottom.color = "black",
      column_labels.border.top.color = "black",
      column_labels.border.bottom.color = "black",
      row_group.border.bottom.color = "black" ,
      row_group.border.top.color = "black" 
    ) %>%
    cols_align(
      align = c("center"),
      columns = everything()
    )

  # Apply color highlighting for violations in Parameter Bias
  if (any(!(subset$Parameter_Bias >= -9.99 & subset$Parameter_Bias <= 9.99), na.rm = TRUE)) {
    gt_table <- gt_table %>%
      data_color(
        columns = "Parameter_Bias",
        rows = .data$Parameter_Bias < -9.99 | .data$Parameter_Bias > 9.99,  # Apply color only if outside the threshold
        method = "numeric",
        palette = c("#113386", "#DAE3FA", "#113386"),  # Darker blue for larger deviations
        domain = c(-40, 40)  # Adjust the domain to reflect the range of values
      ) %>%
      tab_footnote(
        footnote = md("Darker blue indicates larger deviations from zero *Parameter Bias* beyond the ±9.99 threshold."),
        locations = cells_column_labels(columns = "Parameter_Bias")
      )
  }

  # Apply color highlighting for violations in SE Bias
  if (any(!(subset$SE_Bias >= -4.99 & subset$SE_Bias <= 4.99), na.rm = TRUE)) {
    gt_table <- gt_table %>%
      data_color(
        columns = "SE_Bias",
        rows = .data$SE_Bias < -4.99 | .data$SE_Bias > 4.99,  # Apply color only if outside the threshold
        method = "numeric",
        palette = c("#B4186E", "#F9D5E9", "#B4186E"),  # Darker red for larger deviations
        domain = c(-80, 80)  # Adjust the domain for the SE_Bias range
      ) %>%
      tab_footnote(
        footnote = md("Darker red indicates larger deviations from zero *Standard Error Bias* beyond the ±4.99 threshold."),
        locations = cells_column_labels(columns = "SE_Bias")
      )
  }

  if (any(subset$Coverage < 0.93 | subset$Coverage > 0.979, na.rm = TRUE)) {
    gt_table <- gt_table %>%
      data_color(
        columns = "Coverage",
        rows = subset$Coverage < 0.93 | subset$Coverage > 0.979,
        method = "numeric",
        palette = c("#93C6B1", "white"),  # Green for coverage issues
        domain = c(0, 1)
      ) %>%
      tab_footnote(
        footnote = md("Green indicates failure to achieve adequate *Coverage*."),
        locations = cells_column_labels(columns = "Coverage")
      )
  }

  if (any(subset$Power < 0.8, na.rm = TRUE)) {
    gt_table <- gt_table %>%
      data_color(
        columns = "Power",
        rows = subset$Power < 0.8,
        method = "numeric",
        palette = c("#502CD1", "white"),  # Purple for power issues
        domain = c(0, 1)
      ) %>%
      tab_footnote(
        footnote = md("Purple indicates failure to achieve adequate *Power*."),
        locations = cells_column_labels(columns = "Power")
      )
  }
  
  return(gt_table)
}

```

### Render Heat Maps

*TABLE FOR TRANSITION PROBABILITIES OF .200*

```{r, warning = FALSE}
subset_02_table <- create_table(subset_02, ".200")
subset_02_table
subset_02_table |>  tab_options(table.width = pct(65)) |> gtsave(here("2 Time Points", "zHEATMAPS", "z2t_heatmaps", "z2t_r_r_heatmaps", "2T_R_R_.200.png"))
```

*TABLE FOR TRANSITION PROBABILITIES OF .300*

```{r, warning = FALSE}
subset_03_table <- create_table(subset_03, ".300")
subset_03_table
subset_03_table |>  tab_options(table.width = pct(65)) |> gtsave(here("2 Time Points", "zHEATMAPS", "z2t_heatmaps", "z2t_r_r_heatmaps","2T_R_R_.300.png"))
```

*TABLE FOR TRANSITION PROBABILITIES OF .400*

```{r, warning = FALSE}
subset_04_table <- create_table(subset_04, ".400")
subset_04_table
subset_04_table |>  tab_options(table.width = pct(65)) |> gtsave(here("2 Time Points", "zHEATMAPS", "z2t_heatmaps", "z2t_r_r_heatmaps","2T_R_R_.400.png"))
```

*TABLE FOR TRANSITION PROBABILITIES OF .600*

```{r, warning = FALSE}
subset_06_table <- create_table(subset_06, ".600")
subset_06_table
subset_06_table |>  tab_options(table.width = pct(65)) |> gtsave(here("2 Time Points", "zHEATMAPS", "z2t_heatmaps", "z2t_r_r_heatmaps","2T_R_R_.600.png"))
```

*TABLE FOR TRANSITION PROBABILITIES OF .700*

```{r, warning = FALSE}
subset_07_table <- create_table(subset_07, ".700")
subset_07_table
subset_07_table |>  tab_options(table.width = pct(65)) |> gtsave(here("2 Time Points", "zHEATMAPS", "z2t_heatmaps", "z2t_r_r_heatmaps","2T_R_R_.700.png"))
```

*TABLE FOR TRANSITION PROBABILITIES OF .800*

```{r, warning = FALSE}
subset_08_table <- create_table(subset_08, ".800")
subset_08_table
subset_08_table |>  tab_options(table.width = pct(65)) |> gtsave(here("2 Time Points", "zHEATMAPS", "z2t_heatmaps", "z2t_r_r_heatmaps","2T_R_R_.800.png"))
```
