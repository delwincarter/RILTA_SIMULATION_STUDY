---
title: "Study 1 (k = 2) RILTA Generated, RILTA Analyzed; Three Timepoints"
format:
  html:
    code-fold: true
editor: visual
author: "Delwin Carter"
page-layout: full
fig-format: svg
knitr:
  opts_chunk:
    out.width: "90%"
    fig.align: center
---

```{r, message=FALSE, warning=FALSE}

#| label: "load-libraries"
#| echo: true
#| message: false
#| warning: false


library(tidyverse)
library(glue)
library(MplusAutomation)
library(here)
library(gt)
library(janitor)
library(parallel)
library(tools)
library(stringr)
```

# Study 1 (k = 2): RILTA Generated, RILTA Analyzed

![](images/clipboard-3927155885.png){width="350"}

# Model 1:

### Conditions:

Sample Size: N = 500, 1000, 2000, and 4000

Transition logit (probability): TPs = 1.385 (.8), .85 (.7), .41 (.6), -.41 (.4), -.85 (.3) , and -1.385 (.2)

![](images/clipboard-4066126347.png){width="591"}

RI Loadings: lambda = 0, 1, 1.5, 2, 2.5, and 3

![](images/clipboard-3119439446.png){width="351"}

```{r}
#| label: "simulation-conditions"
#| echo: true
#| message: false
#| warning: false


p1 <- expand.grid(N = c(500, 1000, 2000, 4000),
TPs = c(1.385, .85, .41, -.41, -.85, -1.385),
lambda = c(0, 1, 1.5, 2, 2.5, 3))
       
# Display the matrix using gt
p1 %>%
  gt() %>%
  tab_header(
    title = "Simulation Conditions Matrix",
    subtitle = "Combinations of Sample Sizes, Transition Probabilities, and Mixtures"
  ) %>%
  cols_align(
    align = "center",
    columns = everything() # Centers all columns
  )
```

```{r,message=FALSE, warning=FALSE, eval = FALSE}

#| label: "rilta-rilta-simulation"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

library(parallel)
# Step 1: Create the cluster for parallel processing
num_cores <- detectCores() - 1  # Detect the number of available cores (minus 1)
cl <- makeCluster(num_cores, type = "PSOCK")  # Create the PSOCK cluster

rilta_rilta_func <- function(N, TPs, lambda) {
  
  RILTA_RILTA <- mplusObject(
    TITLE = glue("Generate RILTA_RILTA_N_{N}_TP = {TPs}_TH_1_lambda_{lambda}"),

    MONTECARLO =
      glue("NAMES = u11-u15 u21-u25 u31-u35;
      GENERATE = u11-u15 u21-u25 u31-u35(1);
      CATEGORICAL = u11-u15 u21-u25 u31-u35;
      GENCLASSES = c1(2) c2(2) c3(2);
      CLASSES = c1(2) c2(2) c3(2);
      NOBSERVATIONS = {N};
      SEED = 07252005;
      NREPS = 500;
      !!SAVE = repM1*.dat;
      RESULTS = RILTA_RILTA_N_{N}_TP_{TPs}_TH_1_lambda_{lambda}.csv;"),

    ANALYSIS =
      "TYPE = MIXTURE;
      algorithm = integration;
      processors = 24;
      starts = 50 10;
      logcriterion=0.00001;
      mconv=0.00001;",

    MODELPOPULATION = glue("	
        %OVERALL%

        [c1#1-c3#1*0];
        c2#1 on c1#1*{TPs};
        c3#1 on c2#1*0;
      	
          f by u11-u15*{lambda} (p1-p5)
               u21-u25*{lambda} (p1-p5)
               u31-u35*{lambda} (p1-p5);

        f@1;
        [f@0];
        
      MODEL POPULATION-c1:
        %c1#1%
     [u11$1*1 u12$1*1 u13$1*1 u14$1*1 u15$1*1] (p111-p115);

        %c1#2%
     [u11$1*-1 u12$1*-1 u13$1*-1 u14$1*-1 u15$1*-1] (p121-p125);

      MODEL POPULATION-c2:  
        %c2#1%
     [u21$1*1 u22$1*1 u23$1*1 u24$1*1 u25$1*1] (p111-p115);

        %c2#2%
     [u21$1*-1 u22$1*-1 u23$1*-1 u24$1*-1 u25$1*-1] (p121-p125);
     
      MODEL POPULATION-c3:  
        %c3#1%
     [u31$1*1 u32$1*1 u33$1*1 u34$1*1 u35$1*1] (p111-p115);

        %c3#2%
     [u31$1*-1 u32$1*-1 u33$1*-1 u34$1*-1 u35$1*-1] (p121-p125);
       "),
     

    MODEL =
      glue("	
        %OVERALL%
          [c1#1-c3#1*0](par1-par3);
        	c2#1 on c1#1*{TPs} (par11);
        	c3#1 on c2#1*0;
        	
          f by u11-u15*{lambda} (p1-p5)
               u21-u25*{lambda} (p1-p5)
               u31-u35*{lambda} (p1-p5);
               
      	f@1;
        [f@0];

     MODEL c1:
        %c1#1%
     [u11$1*1 u12$1*1 u13$1*1 u14$1*1 u15$1*1] (p111-p115);

        %c1#2%
     [u11$1*-1 u12$1*-1 u13$1*-1 u14$1*-1 u15$1*-1] (p121-p125);

    MODEL c2: 	
        %c2#1%
     [u21$1*1 u22$1*1 u23$1*1 u24$1*1 u25$1*1] (p111-p115);

        %c2#2%
     [u21$1*-1 u22$1*-1 u23$1*-1 u24$1*-1 u25$1*-1] (p121-p125);
     
    MODEL c3:  
        %c3#1%
     [u31$1*1 u32$1*1 u33$1*1 u34$1*1 u35$1*1] (p111-p115);

        %c3#2%
     [u31$1*-1 u32$1*-1 u33$1*-1 u34$1*-1 u35$1*-1] (p121-p125);
	      "),
      

    MODELCONSTRAINT =
      if (TPs == 1.385) {
        glue("
        New(
        trans11*.80 trans12*.20 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.65 prob22*.35);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
      } 
             else if (TPs == .85) {
        glue("
        New(
        trans11*.70 trans12*.30 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.60 prob22*.4);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
            else  if (TPs == .41) {
        glue("
        New(
        trans11*.60 trans12*.40 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.55 prob22*.45);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
             else if (TPs == -.41) {
        glue("
        New(
        trans11*.40 trans12*.60 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.45 prob22*.55);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
             else if (TPs == -.85) {
        glue("
        New(
        trans11*.30 trans12*.70 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.40 prob22*.60);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
        
        else if (TPs == -1.385) {
        glue("
         New(
        trans11*.20 trans12*.80 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.35 prob22*.65);

        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;")
      }
  )

  # Run Mplus model
  RILTA_RILTA_Model<- mplusModeler(RILTA_RILTA, 
                                   dataout = here('Simulations', 'STUDY_1', '3 Time Points', '7_3T_RILTA_GEN_LTA_ANALYZED', glue("RILTA_RILTA_N_{N}_TP_{TPs}_TH_1_lambda_{lambda}.dat")),
                                   modelout = glue(here('Simulations', 'STUDY_1', '3 Time Points', '7_3T_RILTA_GEN_LTA_ANALYZED', "RILTA_RILTA_N_{N}_TP_{TPs}_TH_1_lambda_{lambda}.inp")),
                                   check = TRUE, run = TRUE, hashfilename = FALSE)
return(RILTA_RILTA_Model)
}

# Step 3: Export necessary objects to the cluster
clusterExport(cl, c("rilta_rilta_func", "p1", "here", "glue", "mplusModeler", "mplusObject"))

# Ensure necessary libraries are loaded on each cluster node
clusterEvalQ(cl, {
  library(MplusAutomation)
  library(glue)
  library(here)
})

# Step 4: Run the simulation in parallel using the cluster
result_list <- parLapply(cl, 1:nrow(p1), function(i) {
  rilta_rilta_func(p1$N[i], p1$TPs[i],  p1$lambda[i])
})

# Step 5: Stop the cluster after the simulation
stopCluster(cl)



```

# CHECK FOR LABEL SWITCHING

### Step 1: Combine All CSV Files into One Data Frame

**Objective:** 

*Load all CSV files and combine them into a single data frame.*

```{r}
#| label: "combine-csv-files-parallel"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 1: Set the correct CSV directory
csv_directory <- here('Simulations', 'STUDY_1', '3 Time Points', '8_3T_RILTA_GEN_RILTA_ANALYZED')

# Step 2: Source the child document
source(here('Child_Docs', 'data_scraping.R'))
```

### Step 2: Scrape Rows and Process Data

**Objective:** 

Extract data from the appropriate rows from each 9-row chunk and prepare the data for further processing.

```{r}
#| label: "scrape-rows-process-data-parallel"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 2: Process the data using the child script
source(here('Child_Docs', 'step2_3t_RILTA.R'))
```

### Step 3: Convert Logits to Probabilities and Add Actual (Population) Values

**Objective:** 

Convert the logits to probabilities and add the known actual values to each row.

```{r}
#| label: "convert-logits-to-probabilities"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 3 and 4: Process the data and return results
source(here('Child_Docs', 'step_3.R'))

# The objects `final_data_with_actuals` and `violators` should now be in the global environment
```

### **Step 4: Plot Random Sample of Violators for Visual Inspection**

#### **Objective**

*Generate plots of randomly sampled violators for visual inspection using parallel processing.*

```{r, eval=FALSE}
#| label: "plot-violators"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Set plot width and height
plot_width <- 8
plot_height <- 6

# Take a random sample of up to 250 violators (ensure not to exceed the total number of violators)
set.seed(123)  # For reproducibility
sample_size <- min(nrow(violators), 250)  # Handle cases where fewer than 250 violators exist
sampled_violators <- violators[sample(nrow(violators), sample_size), ]

# Define the function to create plots sequentially
plot_violator <- function(i) {
  row_data <- sampled_violators[i, ]
  
  # Extract the file name from the current row
  file_name <- row_data$FileName

  # Extract probability values for EC1 and EC2 (estimated probabilities) and AC1 and AC2 (actuals)
  estimated_probabilities <- c(
    as.numeric(row_data[c("Ec1u1", "Ec1u2", "Ec1u3", "Ec1u4", "Ec1u5")]),
    as.numeric(row_data[c("Ec2u1", "Ec2u2", "Ec2u3", "Ec2u4", "Ec2u5")])
  )
  
  actual_values <- c(
    as.numeric(row_data[c("Ac1u1", "Ac1u2", "Ac1u3", "Ac1u4", "Ac1u5")]),
    as.numeric(row_data[c("Ac2u1", "Ac2u2", "Ac2u3", "Ac2u4", "Ac2u5")])
  )
  
  # Create labels for the legend with actual values directly from the dataset
  labels <- c(
    paste0("EC1: (", round(row_data$Ec1u1, 3), ", ", round(row_data$Ec1u2, 3), ", ", round(row_data$Ec1u3, 3), ", ", round(row_data$Ec1u4, 3), ", ", round(row_data$Ec1u5, 3), ")"),
    paste0("EC2: (", round(row_data$Ec2u1, 3), ", ", round(row_data$Ec2u2, 3), ", ", round(row_data$Ec2u3, 3), ", ", round(row_data$Ec2u4, 3), ", ", round(row_data$Ec2u5, 3), ")"),
    paste0("AC1: (", round(row_data$Ac1u1, 3), ", ", round(row_data$Ac1u2, 3), ", ", round(row_data$Ac1u3, 3), ", ", round(row_data$Ac1u4, 3), ", ", round(row_data$Ac1u5, 3), ")"),
    paste0("AC2: (", round(row_data$Ac2u1, 3), ", ", round(row_data$Ac2u2, 3), ", ", round(row_data$Ac2u3, 3), ", ", round(row_data$Ac2u4, 3), ", ", round(row_data$Ac2u5, 3), ")")
  )

  # Step 6: Create a data frame for plotting
  plot_data <- data.frame(
    Items = rep(1:5, 4),
    Probabilities = c(estimated_probabilities, actual_values),
    Class = rep(labels, each = 5)
  )

  # Step 7: Create the plot with the file name in the title
  p <- ggplot(plot_data, aes(x = Items, y = Probabilities, color = Class, group = Class)) +
    geom_line(size = 1.2) +
    geom_point(size = 3) +
    labs(title = file_name, x = "Items", y = "Probabilities") +  # Only the file name in the title
    theme_minimal(base_size = 16) +
    theme(panel.background = element_rect(fill = "white"),
          plot.background = element_rect(fill = "white"),
          plot.title = element_text(size = 14, hjust = 0.5)) +  # Adjust title size and center
    scale_color_manual(values = c(
      "darkblue", "darkgreen",  # EC1 and EC2 (Estimated Probabilities)
      "lightblue", "lightgreen"  # AC1 and AC2 (Actual Values)
    ))

  ggsave(filename = file.path("3 Time Points", "zVIOLATOR_PLOTS","z3t_rilta_rilta_violator_plots", paste0("violator_plot_", i, "_", file_name, ".png")),
         plot = p, width = plot_width, height = plot_height)
}

# Apply the function to generate plots sequentially (without parallelization)
invisible(lapply(1:sample_size, plot_violator))

```

### **Step 5: Summarize Violations**

#### **Objective**

*Calculate the percentage of violations for each file, handling missing values appropriately.*

```{r}

#| label: "summarize-errors-and-violations"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true


library(parallel)

extract_errors_from_file <- function(filepath, total_replications) {
  lines <- readLines(filepath)
  results <- vector("list", total_replications)
  error_keywords <- c("NON-POSITIVE DEFINITE", "C2#2 ON C1#1 \\(equality/label\\)", 
                      "C2#2 ON C1#2 \\(equality/label\\)", 
                      "C2#1 ON C1#2 \\(equality/label\\)", 
                      "C2#1 ON C1#1 \\(equality/label\\)")

  # Initialize results for every replication
  for (rep in 1:total_replications) {
    results[[rep]] <- tibble(
      FileName = basename(filepath),
      Replication = rep,
      Message = "None",
      MessageType = "None"
    )
  }

  current_replication <- NULL
  for (line in lines) {
    if (str_detect(line, "REPLICATION")) {
      current_replication <- as.integer(str_extract(line, "\\d+"))
    }

    if (!is.null(current_replication) && current_replication <= total_replications &&
        any(sapply(error_keywords, grepl, line, ignore.case = TRUE))) {
      results[[current_replication]] <- tibble(
        FileName = basename(filepath),
        Replication = current_replication,
        Message = str_trim(line),
        MessageType = "Error"
      )
    }
  }

  return(bind_rows(results))
}




# Step 2: Extract Completed Replications
extract_completed_replications <- function(filepath) {
  lines <- readLines(filepath)
  completed_line <- lines[grepl("Completed", lines, ignore.case = TRUE)]
  completed <- as.integer(str_match(completed_line, "Completed\\s+(\\d+)")[, 2])
  if (length(completed) == 0) completed <- 0
  tibble(FileName = basename(filepath), CompletedReplications = completed)
}


# Step 3: Extract Requested Replications
extract_requested_replications <- function(filepath) {
  lines <- readLines(filepath)
  requested_line <- lines[grepl("Requested", lines, ignore.case = TRUE)]
  requested <- as.integer(str_match(requested_line, "Requested\\s+(\\d+)")[, 2])
  if (length(requested) == 0) requested <- 0
  tibble(FileName = basename(filepath), RequestedReplications = requested)
}

calculate_replication_summary <- function(error_summary, completed_replications, requested_replications) {
  summary <- error_summary %>%
    group_by(FileName) %>%
    summarise(
      ErrorReplications = n_distinct(Replication[MessageType == "Error"]),
      .groups = "drop"
    )

  full_summary <- requested_replications %>%
    left_join(completed_replications, by = "FileName") %>%
    left_join(summary, by = "FileName") %>%
    mutate(
      ErrorReplications = coalesce(ErrorReplications, 0),
      GoodReplications = CompletedReplications - ErrorReplications,
      ErrorRate = if_else(CompletedReplications > 0, (ErrorReplications / CompletedReplications) * 100, 0)
    ) %>%
    select(FileName, RequestedReplications, CompletedReplications, ErrorReplications, GoodReplications, ErrorRate)

  full_summary
}

# Step 4: Parallelized Processing (Windows/Mac/Linux Compatible)
output_folder <- here('Simulations', 'STUDY_1', '3 Time Points', "8_3T_RILTA_GEN_RILTA_ANALYZED")  # Adjust to your folder path
file_list <- list.files(output_folder, pattern = "\\.out$", full.names = TRUE)

# Step 5: Detect OS and Set Up Cluster
cluster_type <- ifelse(.Platform$OS.type == "windows", "PSOCK", "FORK")
num_cores <- detectCores() - 1  # Use all but one core
cl <- makeCluster(num_cores, type = cluster_type)

# Export necessary libraries and functions to the cluster
invisible(clusterExport(cl, c("extract_errors_from_file", "extract_completed_replications", "extract_requested_replications")))
invisible(clusterEvalQ(cl, library(tidyverse)))

# Step 6: Parallel Processing
# Calculate completed replications first
completed_rep_list <- parLapply(cl, file_list, extract_completed_replications)

# Extract errors while passing the total number of completed replications to the function
error_summary <- bind_rows(mapply(function(filepath, completed_data) {
  extract_errors_from_file(filepath, completed_data$CompletedReplications)
}, file_list, completed_rep_list, SIMPLIFY = FALSE))

completed_replications <- bind_rows(parLapply(cl, file_list, extract_completed_replications))
requested_replications <- bind_rows(parLapply(cl, file_list, extract_requested_replications))

# Stop the cluster
stopCluster(cl)

# Step 7: Calculate Replication Summary
replication_summary <- calculate_replication_summary(error_summary, completed_replications, requested_replications)

# Step 8: Create and Display the Table with Error Rate
replication_summary_table <- replication_summary %>%
  gt() %>%
  tab_header(
    title = "Replication Summary",
    subtitle = paste0("Folder: ", output_folder)
  ) %>%
  fmt_number(columns = c(CompletedReplications, RequestedReplications, ErrorReplications, GoodReplications, ErrorRate), decimals = 2) %>%
  cols_label(
    FileName = "File Name",
    CompletedReplications = "Completed Replications",
    RequestedReplications = "Requested Replications",
    ErrorReplications = "Replications with Errors",
    GoodReplications = "Good Replications",
    ErrorRate = "Error Rate (%)"
  ) %>%
  tab_options(
    table.font.size = "small",
    heading.title.font.size = "medium",
    heading.subtitle.font.size = "small"
  )

# Display the table
replication_summary_table

completed_replications <- completed_replications %>%
  mutate(FileName = str_replace(FileName, "\\.out$", ""),
         FileName = tolower(FileName),
         FileName = str_trim(FileName))

error_summary <- error_summary %>%
  mutate(FileName = str_replace(FileName, "\\.out$", ""),
         FileName = tolower(FileName),
         FileName = str_trim(FileName))
final_data_with_actuals <- final_data_with_actuals %>%
  mutate(FileName = tolower(FileName),
         FileName = str_trim(FileName))

cat("Rows in final_data_with_actuals:", nrow(final_data_with_actuals), "\n")
# Output the final number of rows to confirm data handling
cat("Number of rows in error_summary: ", nrow(error_summary), "\n")
cat("Number of rows in replication_summary: ", nrow(replication_summary), "\n")

```

#### Step 5: Part 1 Create Column Names from the Filename

```{r}

#| label: "create-column-names-from-filename"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Add new columns based on the information in the FileName and set factors
final_data_with_actuals <- final_data_with_actuals %>%
  mutate(
    # Extract the sample size (N) from the FileName with the correct values
    N = case_when(
      grepl("N_4000", FileName) ~ 4,  # Correct value for N_4000
      grepl("N_500", FileName) ~ 1,   
      grepl("N_1000", FileName) ~ 2,  
      grepl("N_2000", FileName) ~ 3,  
      TRUE ~ NA_integer_
    ),
    # Map the TPs from the FileName to the appropriate Population labels
    Population = case_when(
      grepl("TP_1.385", FileName) ~ ".800",   # Use ".800" instead of "0.800"
      grepl("TP_0.85", FileName) ~ ".700",    # Use ".700" instead of "0.700"
      grepl("TP_0.41", FileName) ~ ".600",    # Use ".600" instead of "0.600"
      grepl("TP_-0.41", FileName) ~ ".400",   # Use ".400" instead of "0.400"
      grepl("TP_-0.85", FileName) ~ ".300",   # Use ".300" instead of "0.300"
      grepl("TP_-1.385", FileName) ~ ".200",  # Use ".200" instead of "0.200"
      TRUE ~ NA_character_
    ),
Lambda = case_when(
  grepl("a_1$", FileName) ~ "1",        # Match 'a_1' at the end of the string
  grepl("a_1\\.5", FileName) ~ "1.5",   # Match 'a_1.5' (escaping the decimal point)
  grepl("a_2$", FileName) ~ "2",        # Match 'a_2' at the end of the string
  grepl("a_2\\.5", FileName) ~ "2.5",   # Match 'a_2.5' (escaping the decimal point)
  grepl("a_3$", FileName) ~ "3",        # Match 'a_3' at the end of the string
  grepl("a_0", FileName) ~ "0",         # Match 'a_0'
  TRUE ~ NA_character_
),


    # Create the Transitions variable based on Population values before Population is a factor
    Transitions = case_when(
      Population %in% c(".200", ".300", ".400") ~ 1,  # Assign 1 for Population .200, .300, .400
      Population %in% c(".600", ".700", ".800") ~ 2,  # Assign 2 for Population .600, .700, .800
      TRUE ~ NA_integer_
    )
  ) %>%
  # Convert columns to factors, ordering N_4000 first in the factor levels
  mutate(
    N = factor(N, levels = c(4, 1, 2, 3), labels = c("N = 4000", "N = 500", "N = 1000", "N = 2000")),
    Population = factor(Population, levels = c(".800", ".700", ".600", ".400", ".300", ".200")),
    Transitions = factor(Transitions, levels = c(1, 2), labels = c("Mover", "Stayer"))
  )

```

#### Step 5: Part 2 Calculate Violation Percentages per Condition

```{r}

#| label: "calculate-violation-percentages"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true


# Ensure no missing values in Any_Violation and create violation summary, including Population and Lambda
violation_summary <- final_data_with_actuals %>%
  mutate(Any_Violation = ifelse(is.na(Any_Violation), 0, Any_Violation)) %>%
  group_by(FileName, Population, N, Lambda) %>%  # Include Lambda in group_by
  summarize(
    Total_Rows = n(),  # Total rows per group
    Total_Violations = sum(Any_Violation, na.rm = TRUE),  # Total violations
    Percentage_Violations = (Total_Violations / Total_Rows) * 100  # Percentage of violations
  ) %>%
  ungroup() %>%
  select(-FileName)  # Remove FileName after ungrouping

# Ensure that all numeric columns are rounded and suppress scientific notation
options(scipen = 999)
violation_summary <- violation_summary %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)))

```

#### Step 5: Part 3 Summarize & Visualize Label Switching Percentage Results

```{r}
#| label: "summarize-violations"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Function to calculate the number of replications needed a priori, handling N and Population appropriately
calculate_needed_reps <- function(data) {
  data %>%
    # Convert N from character to numeric if it contains "N =" format
    mutate(
      N_numeric = as.numeric(gsub("N = ", "", N)),  # Remove "N = " prefix and convert to numeric
      Additional_Runs = (500 + Total_Violations) * (Percentage_Violations / 100),  # Correct calculation for additional runs
      Replications_Needed = ceiling(500 + Total_Violations + Additional_Runs + 20),  # Total replications needed + buffer of 20
      Replications_Needed = if_else(Replications_Needed < 500, 500, Replications_Needed)  # Ensure a minimum of 500 replications
    )
}

# Apply the updated function to calculate replications
violation_summary <- calculate_needed_reps(violation_summary)

# Ensure correct column names and assign TPs (logits) based on Population (for display and MplusAutomation)
violation_summary <- violation_summary %>%
  mutate(
    TPs = case_when(
      Population == ".800" ~ 1.385,
      Population == ".700" ~ 0.85,
      Population == ".600" ~ 0.41,
      Population == ".400" ~ -0.41,
      Population == ".300" ~ -0.85,
      Population == ".200" ~ -1.385,
      TRUE ~ NA_real_
    )
  ) %>%
  rename(
    `Transition Probability` = Population  # Rename Population to Transition Probability for display purposes
  )

# Now, create the table with formatted output
create_model_table <- function(data) {
  data %>%
    select(
      `Transition Probability`,  # Display transition probabilities
      TPs,  # Logit values for MplusAutomation
      `Sample Size` = N_numeric,  # Use the numeric version of N for clarity
      Lambda,  # Lambda column
      `Total Mplus Runs` = Total_Rows,  # Rename Total_Rows to Total Mplus Runs
      `Total Violations` = Total_Violations,  # Rename Total Violations to Total Violations
      `% of Violations` = Percentage_Violations,  # Rename Percentage Violations to % of Violations
      `Replications Needed` = Replications_Needed  # Add the column for replications needed
    ) %>%
    gt() %>%
    tab_header(
      title = "Monte Carlo Results:",
      subtitle = "Percentage of Cases with Label Switching and Replications Needed"
    ) %>%
    cols_align(
      align = "center",  # Center all columns
      columns = everything()
    ) %>%
    tab_options(
      data_row.padding = px(4)  # Set padding between rows
    ) %>%
    tab_style(
      style = cell_text(align = "center"),  # Center align the headers only
      locations = cells_column_labels(everything())  # Apply to headers only
    )
}

# Arrange the data by Transition Probability and Sample Size (numeric N)
violation_summary <- violation_summary %>%
  arrange(`Transition Probability`, N_numeric)

# Create and display the table
final_table <- create_model_table(violation_summary)

# Display the final table
final_table

```

# Part 2: Re Run Simulation

#### Objective

Rerun Simulation with the number of replications needed to deal with errors and/or label switching

```{r,message=FALSE, warning=FALSE, eval = FALSE}

#| label: "rilta-rilta2-simulation"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true



  # Define the Mplus object with the dynamic replications
rilta_rilta_func <- function(N_numeric, TPs, Lambda, Replications_Needed) {
  
  RILTA_RILTA <- mplusObject(
    TITLE = glue("Generate RILTA_RILTA_N_{N_numeric}_TP_{TPs}_TH_1_Lambda_{Lambda}"),

    MONTECARLO =
      glue("NAMES = u11-u15 u21-u25 u31-u35;
      GENERATE = u11-u15 u21-u25 u31-u35(1);
      CATEGORICAL = u11-u15 u21-u25 u31-u35;
      GENCLASSES = c1(2) c2(2) c3(2);
      CLASSES = c1(2) c2(2) c3(2);
      NOBSERVATIONS = {N_numeric};
      SEED = 07252005;
      NREPS = {Replications_Needed};
      !!SAVE = repM1*.dat;
      RESULTS = RILTA_RILTA_N_{N_numeric}_TP_{TPs}_TH_1_Lambda_{Lambda}.csv;"),

    ANALYSIS =
      "TYPE = MIXTURE;
      algorithm = integration;
      processors = 24;
      starts = 50 10;
      logcriterion=0.00001;
      mconv=0.00001;",

    MODELPOPULATION = glue("	
        %OVERALL%

        [c1#1-c3#1*0];
        c2#1 on c1#1*{TPs};
        c3#1 on c2#1*0;
      	
          f by u11-u15*{Lambda} (p1-p5)
               u21-u25*{Lambda} (p1-p5)
               u31-u35*{Lambda} (p1-p5);

        f@1;
        [f@0];
        
      MODEL POPULATION-c1:
        %c1#1%
     [u11$1*1 u12$1*1 u13$1*1 u14$1*1 u15$1*1] (p111-p115);

        %c1#2%
     [u11$1*-1 u12$1*-1 u13$1*-1 u14$1*-1 u15$1*-1] (p121-p125);

      MODEL POPULATION-c2:  
        %c2#1%
     [u21$1*1 u22$1*1 u23$1*1 u24$1*1 u25$1*1] (p111-p115);

        %c2#2%
     [u21$1*-1 u22$1*-1 u23$1*-1 u24$1*-1 u25$1*-1] (p121-p125);
     
      MODEL POPULATION-c3:  
        %c3#1%
     [u31$1*1 u32$1*1 u33$1*1 u34$1*1 u35$1*1] (p111-p115);

        %c3#2%
     [u31$1*-1 u32$1*-1 u33$1*-1 u34$1*-1 u35$1*-1] (p121-p125);
       "),
     

    MODEL =
      glue("	
        %OVERALL%
          [c1#1-c3#1*0](par1-par3);
        	c2#1 on c1#1*{TPs} (par11);
        	c3#1 on c2#1*0;
        	
          f by u11-u15*{Lambda} (p1-p5)
               u21-u25*{Lambda} (p1-p5)
               u31-u35*{Lambda} (p1-p5);
               
      	f@1;
        [f@0];

     MODEL c1:
        %c1#1%
     [u11$1*1 u12$1*1 u13$1*1 u14$1*1 u15$1*1] (p111-p115);

        %c1#2%
     [u11$1*-1 u12$1*-1 u13$1*-1 u14$1*-1 u15$1*-1] (p121-p125);

    MODEL c2: 	
        %c2#1%
     [u21$1*1 u22$1*1 u23$1*1 u24$1*1 u25$1*1] (p111-p115);

        %c2#2%
     [u21$1*-1 u22$1*-1 u23$1*-1 u24$1*-1 u25$1*-1] (p121-p125);
     
    MODEL c3:  
        %c3#1%
     [u31$1*1 u32$1*1 u33$1*1 u34$1*1 u35$1*1] (p111-p115);

        %c3#2%
     [u31$1*-1 u32$1*-1 u33$1*-1 u34$1*-1 u35$1*-1] (p121-p125);
	      "),
      

    MODELCONSTRAINT =
      if (TPs == 1.385) {
        glue("
        New(
        trans11*.80 trans12*.20 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.65 prob22*.35);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
      } 
             else if (TPs == .85) {
        glue("
        New(
        trans11*.70 trans12*.30 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.60 prob22*.4);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
            else  if (TPs == .41) {
        glue("
        New(
        trans11*.60 trans12*.40 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.55 prob22*.45);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
             else if (TPs == -.41) {
        glue("
        New(
        trans11*.40 trans12*.60 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.45 prob22*.55);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
             else if (TPs == -.85) {
        glue("
        New(
        trans11*.30 trans12*.70 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.40 prob22*.60);
        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;
        ")
              } 
        
        else if (TPs == -1.385) {
        glue("
         New(
        trans11*.20 trans12*.80 trans21*.5 trans22*.5
        prob11*.5 prob12*.5 prob21*.35 prob22*.65);

        trans11 = 1/(1+exp(-(par2+par11)));
        trans12 = 1-trans11;
        trans21 = 1/(1+exp(-par2));
        trans22 = 1- trans21;
        !marginal probabilities at T1 and T2:
        prob11 = 1/(1+exp(-par1));
        prob12 = 1 - prob11;
        prob21 = prob11*trans11+prob12*trans21;
        prob22 = 1- prob21;")
      }
  )

  # Run Mplus model
  RILTA_RILTA_Model<- mplusModeler(RILTA_RILTA, 
                                   dataout = here('Simulations', 'STUDY_1', '3 Time Points', '7_3T_RILTA_GEN_LTA_ANALYZED', glue("RILTA_RILTA_N_{N_numeric}_TP_{TPs}_TH_1_Lambda_{Lambda}.dat")),
                                   modelout = glue(here('Simulations', 'STUDY_1', '3 Time Points', '7_3T_RILTA_GEN_LTA_ANALYZED', "RILTA_RILTA_N_{N_numeric}_TP_{TPs}_TH_1_Lambda_{Lambda}.inp")),
                                   check = TRUE, run = TRUE, hashfilename = FALSE)
return(RILTA_RILTA_Model)
}

# Start the cluster
num_cores <- detectCores() - 1

# Step 2: Select the cluster type based on the system (PSOCK for Windows, FORK for macOS/Linux)
cluster_type <- ifelse(.Platform$OS.type == "windows", "PSOCK", "FORK")


cl <- makeCluster(num_cores, type = cluster_type)

# Export necessary objects to the cluster
clusterExport(cl, c("rilta_rilta_func", "violation_summary", "here", "glue", "mplusModeler", "mplusObject"))

# Ensure required libraries are loaded on each node
clusterEvalQ(cl, {
  library(MplusAutomation)
  library(glue)
  library(here)
})

result_list <- parLapply(cl, 1:nrow(violation_summary), function(i) {
  rilta_rilta_func(
    violation_summary$N_numeric[i], 
    violation_summary$TPs[i],  
    violation_summary$Lambda[i], 
    violation_summary$Replications_Needed[i]
  )
})

# Stop the cluster after the simulation
stopCluster(cl)



```

# CHECK FOR LABEL SWITCHING

### Step 6: Combine All CSV Files into One Data Frame

**Objective:** 

*Load all CSV files and combine them into a single data frame.*

```{r}
#| label: "combine-csv-files-parallel2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 1: Set the correct CSV directory
csv_directory <- here('Simulations', 'STUDY_1', '3 Time Points', '8_3T_RILTA_GEN_RILTA_ANALYZED_REP')

# Step 2: Source the child document
source(here('Child_Docs', 'data_scraping.R'))
```

### Step 7: Scrape Rows and Process Data

**Objective:** 

Extract data from the appropriate rows from each 9-row chunk and prepare the data for further processing.

```{r}
#| label: "scrape-rows-process-data-parallel2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 2: Process the data using the child script
source(here('Child_Docs', 'step2_3t_RILTA.R'))
#should returen df combined_data
```

### Step 8: Convert Logits to Probabilities and Add Actual (Population) Values

**Objective:** 

Convert the logits to probabilities and add the known actual values to each row.

```{r}
#| label: "convert-logits-to-probabilities2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 3 and 4: Process the data and return results
source(here('Child_Docs', 'step_3.R'))

# The objects `final_data_with_actuals` and `violators` should now be in the global environment
```

### **Step 9: Summarize Violations**

#### **Objective**

*Calculate the percentage of violations for each file, handling missing values appropriately.*

```{r}

#| label: "summarize-errors-and-violations2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true


library(parallel)


# Step 1: Extract Errors
extract_errors_from_file <- function(filepath) {
  lines <- readLines(filepath)
  results <- list()
  current_replication <- NULL
  error_keywords <- c("NON-POSITIVE DEFINITE", "C2#2 ON C1#1 \\(equality/label\\)", "C2#2 ON C1#2 \\(equality/label\\)", "C2#1 ON C1#2 \\(equality/label\\)", "C2#1 ON C1#1 \\(equality/label\\)")
  
  completed_line <- lines[grepl("Completed", lines, ignore.case = TRUE)]
  total_replications <- as.integer(str_match(completed_line, "Completed\\s+(\\d+)")[, 2])
  if (is.na(total_replications) || length(total_replications) == 0) total_replications <- 0
  
  results <- tibble(
    FileName = basename(filepath),
    Rep = 1:total_replications,  # Rename 'Replication' to 'Rep'
    ErrorFlag = 0,
    Message = "None"
  )
  
  current_replication <- NULL
  for (line in lines) {
    replication_match <- str_match(line, "REPLICATION (\\d+):")
    if (!is.na(replication_match[1])) {
      current_replication <- as.integer(replication_match[2])
    }
    if (!is.null(current_replication) && any(sapply(error_keywords, grepl, line, ignore.case = TRUE))) {
      results <- results %>%
        mutate(
          ErrorFlag = if_else(Rep == current_replication, 1, ErrorFlag),
          Message = if_else(Rep == current_replication,
                            paste0(Message, "; ", str_trim(line)),
                            Message
          )
        )
    }
  }
  
  results <- results %>%
    mutate(
      Message = if_else(Message == "None", "No Errors or Warnings", Message)
    )
  return(results)
}

extract_completed_replications <- function(filepath) {
  lines <- readLines(filepath)
  completed_line <- lines[grepl("Completed", lines, ignore.case = TRUE)]
  completed <- as.integer(str_match(completed_line, "Completed\\s+(\\d+)")[, 2])
  if (length(completed) == 0) completed <- 0
  tibble(FileName = basename(filepath), CompletedReplications = completed)
}

extract_requested_replications <- function(filepath) {
  lines <- readLines(filepath)
  requested_line <- lines[grepl("Requested", lines, ignore.case = TRUE)]
  requested <- as.integer(str_match(requested_line, "Requested\\s+(\\d+)")[, 2])
  if (length(requested) == 0) requested <- 0
  tibble(FileName = basename(filepath), RequestedReplications = requested)
}

# Function to calculate the replication summary
calculate_replication_summary <- function(all_reps) {
  # Aggregate file-level summary
  full_summary <- all_reps %>%
    group_by(FileName) %>%
    summarise(
      CompletedReplications = n_distinct(Rep),
      ErrorReplications = sum(ErrorFlag),
      RequestedReplications = max(Rep), # Assuming all replications are sequentially numbered
      .groups = "drop"
    ) %>%
    mutate(
      GoodReplications = CompletedReplications - ErrorReplications,
      ErrorRate = if_else(RequestedReplications > 0, (ErrorReplications / RequestedReplications) * 100, 0)
    )
  
  return(full_summary)
}

output_folder <- here('Simulations', 'STUDY_1', '3 Time Points', "8_3T_RILTA_GEN_RILTA_ANALYZED_REP")
file_list <- list.files(output_folder, pattern = "\\.out$", full.names = TRUE)

cluster_type <- ifelse(.Platform$OS.type == "windows", "PSOCK", "FORK")
num_cores <- detectCores() - 1
cl <- makeCluster(num_cores, type = cluster_type)

# Export functions to cluster
invisible(clusterExport(cl, c("extract_errors_from_file", "extract_completed_replications", "extract_requested_replications", "str_match")))

invisible(clusterEvalQ(cl, library(tidyverse)))

error_summary <- bind_rows(parLapply(cl, file_list, extract_errors_from_file)) %>%
  mutate(FileName = tolower(gsub("\\.out$", "", FileName)))  # Normalize FileName

completed_replications <- bind_rows(parLapply(cl, file_list, extract_completed_replications)) %>%
  mutate(FileName = tolower(gsub("\\.out$", "", FileName)))  # Normalize FileName

requested_replications <- bind_rows(parLapply(cl, file_list, extract_requested_replications)) %>%
  mutate(FileName = tolower(gsub("\\.out$", "", FileName)))  # Normalize FileName

# Stop the cluster
stopCluster(cl)

# After stopping the cluster, consolidate and summarize the data once
all_reps <- error_summary %>%
  left_join(completed_replications, by = "FileName") %>%
  left_join(requested_replications, by = "FileName")

# Add the 'Rep' column to ensure it's available for summarization
all_reps <- all_reps %>%
  mutate(Rep = 1:nrow(all_reps))  # Ensure Rep column is added

# Now calculate the replication summary with correct column order and summary
# Assuming the error_summary, completed_replications, and requested_replications
# are correctly populated with the right data including a 'Rep' column if needed.

# Adjusting the summarization for RequestedReplications
replication_summary <- all_reps %>%
  group_by(FileName) %>%
  summarise(
    CompletedReplications = n_distinct(Rep),
    RequestedReplications = first(RequestedReplications),  # Or use max(RequestedReplications) since all values are the same for each file
    ErrorReplications = sum(ErrorFlag),
    GoodReplications = CompletedReplications - ErrorReplications,
    ErrorRate = if_else(RequestedReplications > 0, (ErrorReplications / RequestedReplications) * 100, 0),
    .groups = 'drop'
  )

# Ensure correct column ordering
replication_summary <- replication_summary %>%
  select(FileName, CompletedReplications, RequestedReplications, ErrorReplications, GoodReplications, ErrorRate)



# Output the final number of rows to confirm data handling
cat("Number of rows in all_reps: ", nrow(all_reps), "\n")
cat("Number of rows in replication_summary: ", nrow(replication_summary), "\n")

```

#### Step 9 part 1: Initial Visualization

The purpose of this step is to render a visualization of the requested replications, completed replications, total errors, good replications, and error rate percentage.

```{r}
#| label: "visualization"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true



# Check for errors and create the table if applicable
#if (nrow(replication_summary) > 0 && any#(replication_summary$ErrorReplications > 0)) {
  replication_summary_table <- replication_summary %>%
    gt() %>%
    tab_header(
      title = "Replication Summary",
      subtitle = paste0("Folder: ", output_folder)
    ) %>%
    fmt_number(columns = c(CompletedReplications, RequestedReplications, ErrorReplications, GoodReplications, ErrorRate), decimals = 2) %>%
    cols_label(
      FileName = "File Name",
      CompletedReplications = "Completed Replications",
      RequestedReplications = "Requested Replications",
      ErrorReplications = "Replications with Errors",
      GoodReplications = "Good Replications",
      ErrorRate = "Error Rate (%)"
    ) %>%
    tab_options(
      table.font.size = "small",
      heading.title.font.size = "medium",
      heading.subtitle.font.size = "small"
    )

  print(replication_summary_table)
#} else {
 # message("No errors or warnings detected. No table to display.")
#}


```

#### Step 9: Part 2

The purpose of this code chunk is to

```{r}
# Filter for valid replications
clean_data <- all_reps %>%
  filter(ErrorFlag == 0) %>%  # Keep only rows without errors
  select(FileName, Rep)  # Select only required columns

# Summarize valid replications
cat("Number of valid replications per file:\n")
clean_data_summary <- clean_data %>%
  group_by(FileName) %>%
  summarise(ValidReplications = n(), .groups = "drop")


```

#### Step 9 Part 3: Create DF with Reps with errors Flagged

The purpose of this code chunk is to

```{r}

# Step 1: Summarize error information for each file
error_results_summary <- all_reps %>%
  group_by(FileName) %>%
  summarise(
    TotalReplications = n(),  # Count all replications
    ErrorReplications = sum(ErrorFlag == 1),  # Count error instances
    RemainingReplications = sum(ErrorFlag == 0),  # Count successful (non-error) replications
    .groups = "drop"
  )

# Step 2: Compute Derived Metrics
error_results_summary <- error_results_summary %>%
  mutate(
    ComputedAfterErrors = TotalReplications - ErrorReplications,  # Alternative name for RemainingReplications
    ErrorRate = if_else(
      TotalReplications > 0,
      (ErrorReplications / TotalReplications) * 100,
      0
    )  # Percentage of errors
  )

# Step 3: Validate Computations
validation_check <- error_results_summary %>%
  filter(ComputedAfterErrors != RemainingReplications)

if (nrow(validation_check) > 0) {
  warning("Mismatch detected between ComputedAfterErrors and RemainingReplications!")
  print(validation_check)
} else {
  message("Validation successful: All computations are consistent.")
}

# Now 'error_results_summary' includes TotalReplications, ErrorReplications, RemainingReplications, ComputedAfterErrors, and ErrorRate.


```

#### Step 9 Part 4: Visualize the data post errors

The purpose of this step is to visualize a check to make sure the number of replications remaining after deletion matches the number of replications calculated between the difference of tee number or replications completed and the number of replications with errrors.

```{r, eval=FALSE}
#| label: "visualize_data_post_errors"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true



# Render the error_results_summary table
error_results_summary%>%
  gt() %>%
  tab_header(
    title = "Replication Summary",
    subtitle = paste0("Folder: ", output_folder)
  ) %>%
  fmt_number(
    columns = c(TotalReplications, ErrorReplications, RemainingReplications, ComputedAfterErrors, ErrorRate),
    decimals = 2
  ) %>%
  cols_label(
    FileName = "File Name",
    TotalReplications = "Total Replications",
    ErrorReplications = "Error Replications",
    RemainingReplications = "Remaining Replications",
    ComputedAfterErrors = "Computed After Errors",
    ErrorRate = "Error Rate (%)"
  ) %>%
  tab_options(
    table.font.size = "small",
    heading.title.font.size = "medium",
    heading.subtitle.font.size = "small"
  )

```

### **Step 10: Summarize and Delete cases with Label Switching**

#### **Objective**

*Summarize cases with label switching and delete cases*

```{r}
# Remove '.out' from the 'FileName' column in both datasets
final_data_with_actuals <- final_data_with_actuals %>%
  mutate(FileName = str_remove(FileName, "\\.out$"))  # Remove '.out' at the end of the string

error_summary<- error_summary %>%
  mutate(FileName = str_remove(FileName, "\\.out$"))  # Remove '.out' at the end of the string

# Normalize the 'FileName' and 'Rep' columns in both datasets
final_data_with_actuals <- final_data_with_actuals %>%
  mutate(
    FileName = str_trim(tolower(FileName)),  # Trim spaces and convert to lowercase
    Rep = str_trim(tolower(Rep))             # Trim spaces and convert to lowercase
  )

error_summary <- error_summary %>%
  mutate(
    FileName = str_trim(tolower(FileName)),  # Trim spaces and convert to lowercase
    Rep = str_trim(tolower(Rep))             # Trim spaces and convert to lowercase
  )

# Step 1: Merge ErrorFlag into final_data_with_actuals based on both FileName and Rep
final_data_with_actuals <- final_data_with_actuals %>%
  left_join(error_summary %>% select(FileName, Rep, ErrorFlag), by = c("FileName", "Rep"))

# Step 2: Filter out rows where ErrorFlag is 1 (i.e., errors)
final_data_with_actuals <- final_data_with_actuals %>%
  filter(ErrorFlag != 1)

# Step 3: Verify the number of rows before and after deletion
cat("Rows in final_data_with_actuals (before deletion):", nrow(final_data_with_actuals), "\n")

```

#### Step 10: Part 1 Create column names

```{r}

#| label: "create-column-names-from-filename2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Add new columns based on the information in the FileName and set factors
final_data_with_actuals <- final_data_with_actuals %>%
  mutate(
    # Extract the sample size (N) from the FileName with the correct values
    N = case_when(
      grepl("n_4000", FileName) ~ 4,  # Correct value for N_4000
      grepl("n_500", FileName) ~ 1,   
      grepl("n_1000", FileName) ~ 2,  
      grepl("n_2000", FileName) ~ 3,  
      TRUE ~ NA_integer_
    ),
    # Map the TPs from the FileName to the appropriate Population labels
    Population = case_when(
      grepl("tp_1.385", FileName) ~ ".800",   # Use ".800" instead of "0.800"
      grepl("tp_0.85", FileName) ~ ".700",    # Use ".700" instead of "0.700"
      grepl("tp_0.41", FileName) ~ ".600",    # Use ".600" instead of "0.600"
      grepl("tp_-0.41", FileName) ~ ".400",   # Use ".400" instead of "0.400"
      grepl("tp_-0.85", FileName) ~ ".300",   # Use ".300" instead of "0.300"
      grepl("tp_-1.385", FileName) ~ ".200",  # Use ".200" instead of "0.200"
      TRUE ~ NA_character_
    ),
Lambda = case_when(
  grepl("a_1$", FileName) ~ "1",        # Match 'a_1' at the end of the string
  grepl("a_1\\.5", FileName) ~ "1.5",   # Match 'a_1.5' (escaping the decimal point)
  grepl("a_2$", FileName) ~ "2",        # Match 'a_2' at the end of the string
  grepl("a_2\\.5", FileName) ~ "2.5",   # Match 'a_2.5' (escaping the decimal point)
  grepl("a_3$", FileName) ~ "3",        # Match 'a_3' at the end of the string
  grepl("a_0", FileName) ~ "0",         # Match 'a_0'
  TRUE ~ NA_character_
),


    # Create the Transitions variable based on Population values before Population is a factor
    Transitions = case_when(
      Population %in% c(".200", ".300", ".400") ~ 1,  # Assign 1 for Population .200, .300, .400
      Population %in% c(".600", ".700", ".800") ~ 2,  # Assign 2 for Population .600, .700, .800
      TRUE ~ NA_integer_
    )
  ) %>%
  # Convert columns to factors, ordering N_4000 first in the factor levels
  mutate(
    N = factor(N, levels = c(4, 1, 2, 3), labels = c("N = 4000", "N = 500", "N = 1000", "N = 2000")),
    Population = factor(Population, levels = c(".800", ".700", ".600", ".400", ".300", ".200")),
    Transitions = factor(Transitions, levels = c(1, 2), labels = c("Mover", "Stayer"))
  )

```

#### Step 10; Part 2 Calculate Violation Percentages per Condition

```{r}

#| label: "calculate-violation-percentages2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true


# Ensure no missing values in Any_Violation and create violation summary, including Population and Lambda
violation_summary <- final_data_with_actuals %>%
  mutate(Any_Violation = ifelse(is.na(Any_Violation), 0, Any_Violation)) %>%
  group_by(FileName, Population, N, Lambda) %>%  # Include Lambda in group_by
  summarize(
    Total_Rows = n(),  # Total rows per group
    Total_Violations = sum(Any_Violation, na.rm = TRUE),  # Total violations
    Percentage_Violations = (Total_Violations / Total_Rows) * 100  # Percentage of violations
  ) %>%
  ungroup() %>%
  select(-FileName)  # Remove FileName after ungrouping

# Ensure that all numeric columns are rounded and suppress scientific notation
options(scipen = 999)
violation_summary <- violation_summary %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)))

```

#### Step 6: Part 3 Summarize & Visualize Label Switching Percentage Results

```{r}
#| label: "summarize-violations2"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Function to calculate the number of replications needed a priori, handling N and Population appropriately
calculate_needed_reps <- function(data) {
  data %>%
    # Convert N from character to numeric if it contains "N =" format
    mutate(
      N_numeric = as.numeric(gsub("N = ", "", N)),  # Remove "N = " prefix and convert to numeric
      Additional_Runs = (500 + Total_Violations) * (Percentage_Violations / 100),  # Correct calculation for additional runs
      Replications_Needed = ceiling(500 + Total_Violations + Additional_Runs + 20),  # Total replications needed + buffer of 20
      Replications_Needed = if_else(Replications_Needed < 500, 500, Replications_Needed)  # Ensure a minimum of 500 replications
    )
}

# Apply the updated function to calculate replications
violation_summary <- calculate_needed_reps(violation_summary)

# Ensure correct column names and assign TPs (logits) based on Population (for display and MplusAutomation)
violation_summary <- violation_summary %>%
  mutate(
    TPs = case_when(
      Population == ".800" ~ 1.385,
      Population == ".700" ~ 0.85,
      Population == ".600" ~ 0.41,
      Population == ".400" ~ -0.41,
      Population == ".300" ~ -0.85,
      Population == ".200" ~ -1.385,
      TRUE ~ NA_real_
    )
  ) %>%
  rename(
    `Transition Probability` = Population  # Rename Population to Transition Probability for display purposes
  )

# Now, create the table with formatted output
create_model_table <- function(data) {
  data %>%
    select(
      `Transition Probability`,  # Display transition probabilities
      TPs,  # Logit values for MplusAutomation
      `Sample Size` = N_numeric,  # Use the numeric version of N for clarity
      Lambda,  # Lambda column
      `Total Mplus Runs` = Total_Rows,  # Rename Total_Rows to Total Mplus Runs
      `Total Violations` = Total_Violations,  # Rename Total Violations to Total Violations
      `% of Violations` = Percentage_Violations,  # Rename Percentage Violations to % of Violations
      `Replications Needed` = Replications_Needed  # Add the column for replications needed
    ) %>%
    gt() %>%
    tab_header(
      title = "Monte Carlo Results:",
      subtitle = "Percentage of Cases with Label Switching and Replications Needed"
    ) %>%
    cols_align(
      align = "center",  # Center all columns
      columns = everything()
    ) %>%
    tab_options(
      data_row.padding = px(4)  # Set padding between rows
    ) %>%
    tab_style(
      style = cell_text(align = "center"),  # Center align the headers only
      locations = cells_column_labels(everything())  # Apply to headers only
    )
}

# Arrange the data by Transition Probability and Sample Size (numeric N)
violation_summary <- violation_summary %>%
  arrange(`Transition Probability`, N_numeric)

# Create and display the table
final_table <- create_model_table(violation_summary)

# Display the final table
final_table

```

### **Step 10: Delete Cases that Violate**

#### **Objective**

*Filter out cases with any violations, leaving only the clean data.*

```{r}
#| label: "delete-cases"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 1: Filter out cases with any violations (Any_Violation == 0)
filtered_data_with_no_violations <- final_data_with_actuals[final_data_with_actuals$Any_Violation == 0, ]

library(dplyr)


```

### STEP 12:

#### **Objective**

*Randomly select 500 replications per condition*

```{r}
# Set seed for reproducibility
library(dplyr)

# Set seed for reproducibility (although not needed for counting)
set.seed(07252005)

# Count the number of rows per FileName
count_per_filename <- filtered_data_with_no_violations %>%
  group_by(FileName) %>%
  summarize(Count = n(), .groups = "drop")

# Print the counts for each FileName
print(count_per_filename)

# Optionally, perform a check to ensure all FileName groups meet a certain threshold, e.g., 500 rows
sanity_check <- count_per_filename %>%
  filter(Count < 500)

if (nrow(sanity_check) > 0) {
  warning("Some FileName groups do not have at least 500 rows. Please verify the data.")
  print(sanity_check)
} else {
  message("All FileName groups have at least 500 rows or more.")
}




```

### 

### **Step 11: Compute Monte Carlo (MC) Values**

#### **Objective**

*Calculate Monte Carlo values for `TRANS11`, including population values, averages, standard errors, Mean Squared Error (MSE), coverage, and power.*

```{r}
#| label: "compute-mc-values"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

cleaned_data <- cleaned_data %>%
  mutate(Population = as.numeric(as.character(Population)))

# Calculate the Monte Carlo values, including Population (transition probability), N, 
# number of replications, Lambda, and averages for TRANS11 and SE11
mc_values <- cleaned_data %>%
  group_by(FileName, Population, N, Transitions, Lambda) %>%  # Include Lambda in the grouping
  summarize(
    average = round(mean(TRANS11, na.rm = TRUE), 3),
    average_SE = round(mean(SE_11, na.rm = TRUE), 3),
    population_sd = round(sd(TRANS11, na.rm = TRUE), 3),
    
    # MSE calculation: mean squared error between TRANS11 and Population
    MSE = round(mean((TRANS11 - Population)^2, na.rm = TRUE), 3),
    
    # Coverage calculation: check if Population lies within the confidence interval
    Coverage = round(mean((Population >= (TRANS11 - 1.96 * SE_11)) & (Population <= (TRANS11 + 1.96 * SE_11)), na.rm = TRUE), 3),
    
    # Power calculation: proportion of cases where TRANS11 is significant
    Power = round(mean(TRANS11 / SE_11 > 1.96, na.rm = TRUE), 3),
    
    # Reps_Used counts the number of replications (rows) used for each FileName
    Reps_Used = n()
  )

# Round the values to 3 decimal points
mc_values <- mc_values %>%
  mutate(across(starts_with("Avg_"), ~ round(.x, 3)))

```

### **Step 12: Calculate Dichotomous Variables and Bias**

#### **Objective**

*Calculate dichotomous variables for Power and Coverage, compute Parameter and SE Bias, and prepare subsets for movers and stayers.*

```{r}
#| label: "calculate-bias-dichotomous-variables"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 1: Calculate dichotomous variable for Power (1 if Power >= 0.8, else 0)
mc_values <- mc_values %>%
  mutate(Power_Dic = ifelse(Power >= 0.8, 1, 0))

# Step 2: Calculate dichotomous variable for Coverage (0 if outside [0.91, 0.98], else 1)
mc_values <- mc_values %>%
  mutate(Coverage_Dic = ifelse(Coverage > 0.98 | Coverage < 0.91, 0, 1))

# Step 3: Remove any groupings before further calculations
mc_values <- mc_values %>%
  ungroup()

# Step 4: Ensure numeric columns are correctly formatted and **convert Population only for calculations**
mc_values <- mc_values %>%
  mutate(
    # Keep average as numeric but **do not convert Population for display purposes**
    average = as.numeric(average),
    population_numeric = as.numeric(Population),  # Create a temporary numeric version of Population
    average_se = as.numeric(average_SE),
    population_sd = as.numeric(population_sd)
  )

# Step 5: Calculate Parameter Bias and SE Bias, rounding the results to 2 decimal places
mc_values <- mc_values %>%
  mutate(
    # Use population_numeric for the calculations, but **retain Population in the original format**
    Parameter_Bias = (average - population_numeric) / population_numeric * 100,  # Bias for the parameter
    SE_Bias = (average_se - population_sd) / population_sd * 100  # Bias for the standard error
  ) %>%
  mutate(across(c(Parameter_Bias, SE_Bias), ~ round(.x, 2)))  # Round to 2 decimal places

# Drop the temporary numeric population column if no longer needed
mc_values <- mc_values %>%
  select(-population_numeric)

```

### **Step 13: Subset Data for Movers and Stayers**

#### **Objective**

*Subset the Monte Carlo values data for transitions with movers and stayers based on the population value.*

```{r}

#| label: "subset-data-for-bias-plots"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true


# Assuming Population is numeric in all_data
all_data <- mc_values

# Convert N to a factor with the correct labels for plotting
all_data <- all_data %>%
  mutate(N = factor(N,
                    levels = c("N = 500", "N = 1000", "N = 2000", "N = 4000"),  # These are the existing labels
                    labels = c(`1` = "N = 500", 
                               `2` = "N = 1000", 
                               `3` = "N = 2000", 
                               `4` = "N = 4000")))

# Define the labels for N using expression() for italics, which will be used in plotting
n_labels <- c(
  `1` = expression(italic('N') ~ "= 500"),
  `2` = expression(italic('N') ~ "= 1000"),
  `3` = expression(italic('N') ~ "= 2000"),
  `4` = expression(italic('N') ~ "= 4000")
)

# Assign the labels to the levels
all_data$N <- factor(all_data$N, labels = n_labels)
# Now you can use `n_labels` in the plotting code


# Ensure that Population_Label is correctly prepared
all_data$Population_Label <- factor(all_data$Population, 
    levels = c(0.2, 0.3, 0.4, 0.6, 0.7, 0.8),  # Numeric levels without leading zeros
    labels = c(
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .200"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .300"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .400"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .600"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .700"),
        expression(bold(C[11]) ~ "\u2192" ~ bold(C[21]) ~ " = .800")
    )
)


# Subset for Transitions movers (already correctly defined as "Mover")
subset_mover <- subset(all_data, Transitions == "Mover")
subset_mover <- subset_mover %>%
  mutate(Lambda = as.numeric(as.character(Lambda)))

# Subset for Transitions stayers (already correctly defined as "Stayer")
subset_stayer <- subset(all_data, Transitions == "Stayer")
subset_stayer <- subset_stayer %>%
  mutate(Lambda = as.numeric(as.character(Lambda)))

```

### **Step 14: Combined Plot for Mover and Stayer**

#### **Objective**

*Create streamlined plots for both mover and stayer subsets using a common theme and labels.*

```{r}
#| label: "plot-bias"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Define common themes and aesthetics
common_theme <- theme_minimal() +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.minor.y = element_blank(),
    axis.text.x = element_text(size = 8),
    axis.ticks = element_line(color = "black", linewidth = 0.2),
    legend.position = "bottom",
    legend.title = element_blank(),
    text = element_text(family = "Times New Roman"),
    axis.title.x = element_text(margin = margin(t = 10, b = 10)),
    legend.margin = margin(t = -10),
    plot.caption = element_text(hjust = 0, margin = margin(t = 10))
  )

common_labels <- labs(
  x = "Lambda Loadings on the RI",
  y = "Bias (%)",
  color = "",
  title = ""
)


create_plot <- function(data, title_suffix) {
  # Detect which legend items to show
  present_categories <- c("Parameter Bias", "Standard Error Bias")  # Base categories
  if (any(data$Coverage_Dic == 0)) present_categories <- c(present_categories, "Coverage Failure")
  if (any(data$Power_Dic == 0)) present_categories <- c(present_categories, "Power Failure")

  # Define colors and shapes for different categories
  colors <- c("Parameter Bias" = "#7030A0", "Standard Error Bias" = "#C830CC", 
              "Coverage Failure" = "#7030A0", "Power Failure" = "black")
  shapes <- c("Parameter Bias" = 16, "Standard Error Bias" = 18, 
              "Coverage Failure" = 1, "Power Failure" = 4)

  # Filter colors and shapes based on detected categories
  filtered_colors <- colors[present_categories]
  filtered_shapes <- shapes[present_categories]

  ggplot(data = data, aes(x = Lambda, y = Parameter_Bias, color = "Parameter Bias", group = Population_Label)) +  
    geom_line(aes(group = Population_Label), linewidth = 0.3, linetype = "solid") +  
    geom_line(aes(y = SE_Bias, group = Population_Label, color = "Standard Error Bias"), linewidth = 0.3, linetype = "solid") +  
    geom_point(aes(y = Parameter_Bias), shape = 16, size = 1, fill = "#7030A0", alpha = 0.8) +  
    geom_point(aes(y = SE_Bias, color = "Standard Error Bias"), shape = 18, size = 1, fill = "#C830CC", alpha = 0.8) +  # Corrected
    geom_point(data = subset(data, Coverage_Dic == 0), aes(y = Parameter_Bias, color = "Coverage Failure"), shape = 1, size = 2, fill = "#7030A0", alpha = 1) +  
    geom_point(data = subset(data, Power_Dic == 0), aes(y = Parameter_Bias, color = "Power Failure"), shape = 4, size = 2, fill = "black", alpha = 1) + 
    scale_color_manual(
      values = filtered_colors, 
      labels = present_categories, 
      breaks = present_categories,
      guide = guide_legend(
        override.aes = list(shape = filtered_shapes)
      )
    ) +  
    labs(
      x = "Lambda Loadings on the RI",
      y = "Bias (%)",
      color = "",
      title = paste("RILTA Generated, LTA Analyzed with", title_suffix, "Transition Probabilities")
    ) +
    coord_cartesian(ylim = c(-40, 40)) +  
    facet_grid(Population_Label ~ N, scales = "free_x", labeller = label_parsed) +  
    scale_x_continuous(breaks = seq(0, 3, by = 0.5), labels = scales::number_format(accuracy = 0.1)) +  
    scale_y_continuous(breaks = seq(-40, 40, by = 10)) +  
    theme_minimal() +
    theme(
      panel.grid.major.x = element_blank(),
      panel.grid.minor.x = element_blank(),
      panel.grid.minor.y = element_blank(),
      axis.text.x = element_text(size = 8),
      axis.ticks = element_line(color = "black", linewidth = 0.2),
      legend.position = "bottom",
      legend.title = element_blank(),
      text = element_text(family = "Times New Roman"),
      axis.title.x = element_text(margin = margin(t = 10, b = 10)),
      legend.margin = margin(t = -10),
      plot.caption = element_text(hjust = 0, margin = margin(t = 10))
    ) +
    geom_hline(yintercept = c(-10, 10), linetype = "dashed", color = "#7030A0", linewidth = 0.3) +  
    geom_hline(yintercept = c(-5, 5), linetype = "dashed", color = "#C830CC", linewidth = 0.3)
}

```

#### Plot figure with Mover Transition Probabilities (.200, .300, .400)

```{r}
# Create and print plot for Mover
plot_mover <- create_plot(subset_mover, "Mover")
#| column: screen
#| fig-format: svg
print(plot_mover)


# Remove title for the saved version
plot_mover_no_title <- plot_mover + labs(title = NULL)

# Save Mover plot without title as .svg
ggsave(here('Simulations', 'STUDY_1', '3 Time Points', "zFigures", "x3t_rilta_rilta_plots", "plot_mover.svg"), plot = plot_mover_no_title, width = 6, height = 3, dpi = 300, device = "svg")
```

#### Plot figure with Mover Transition Probablities (.200, .300, .400)

```{r}
# Create and print plot for Stayer
plot_stayer <- create_plot(subset_stayer, "Stayer")
#| column: screen
#| fig-format: svg
print(plot_stayer)

# Remove title for the saved version
plot_stayer_no_title <- plot_stayer + labs(title = NULL)

# Save Stayer plot without title as .svg
ggsave(here('Simulations', 'STUDY_1', '3 Time Points', "zFigures", "x3t_rilta_rilta_plots", "plot_stayer.svg"), plot = plot_stayer_no_title, width = 6, height = 3, dpi = 300, device = "svg")
```

### **Step 15: Prepare Data for Heatmaps**

#### **Objective**

*Prepare data for heatmap creation by ensuring correct formatting for population values, and subsetting the data based on class proportions and sample sizes.*

```{r}

#| label: "prepare-data-for-heatmaps"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

# Step 1: Define a function to extract numeric values from the 'N' column
extract_numeric_from_N <- function(N) {
  # Use regex to extract the numeric part (e.g., from 'italic("N") ~ "= 1000"')
  as.numeric(gsub("[^0-9]", "", N))
}

# Step 2: Apply this function to both movers and stayers
movers <- all_data %>%
  filter(Transitions == "Mover") %>%
  mutate(N_numeric = extract_numeric_from_N(N)) %>%  # Create a new column with numeric N
  arrange(N_numeric, Population)

stayers <- all_data %>%
  filter(Transitions == "Stayer") %>%
  mutate(N_numeric = extract_numeric_from_N(N)) %>%  # Create a new column with numeric N
  arrange(N_numeric, Population)

# Step 3: Assign N_Label only once for each N within Movers and Stayers
movers <- movers %>%
  group_by(N) %>%
  mutate(N_Label = ifelse(row_number() == 2, as.character(N), "")) %>%
  ungroup()

stayers <- stayers %>%
  group_by(N) %>%
  mutate(N_Label = ifelse(row_number() == 2, as.character(N), "")) %>%
  ungroup()

# Step 4: Combine movers and stayers back into one dataset, keeping them separate by transitions
all_data_sorted <- bind_rows(movers, stayers)

# Step 5: Ensure Lambda is included and columns are factored
all_data_sorted <- all_data_sorted %>% 
  mutate(N_Label = factor(N, labels = c("N = 500", "N = 1000", "N = 2000", "N = 4000"))) %>% 
  mutate(Population = factor(Population,
                             labels = c(`.2` = ".200", `.3` = ".300", `.4` = ".400",
                                        `.6` = ".600", `.7` = ".700", `.8` = ".800")))

# Step 6: Select necessary columns, including Lambda
test_map <- select(all_data_sorted, N_Label, Population, average, Coverage, Power, Parameter_Bias, SE_Bias, Lambda)

# Step 7: Ordering the table based on the "Lambda" column
test_map <- test_map %>%
  arrange(as.numeric(Lambda))

# Define the population values as characters
population_values <- c(".200", ".300", ".400", ".600", ".700", ".800")

# Function to subset the data for a specific population value
subset_data <- function(data, pop_value) {
  subset <- data %>%
    filter(Population == pop_value)
  return(subset)
}

# Apply the function to each population value
subset_list <- lapply(population_values, function(x) subset_data(test_map, as.character(x)))

# Access the subsets for each population value
subset_02 <- subset_list[[1]]  # Subset for population value .200
subset_03 <- subset_list[[2]]  # Subset for population value .300
subset_04 <- subset_list[[3]]  # Subset for population value .400
subset_06 <- subset_list[[4]]  # Subset for population value .600
subset_07 <- subset_list[[5]]  # Subset for population value .700
subset_08 <- subset_list[[6]]  # Subset for population value .800

```

### **Step 16: Heatmap Creation and Rendering**

#### **Objective**

*Create heatmaps using the `gt` package and render each table separately for different subsets of the data.*

```{r}

#| label: "create-heatmap-function"
#| echo: true
#| message: false
#| warning: false
#| code-fold: true

create_table <- function(subset, transition_probability) {
  
  # Create the gt object and set initial formatting
  gt_table <- subset %>%
    gt() %>%
    opt_table_font(stack = "geometric-humanist") %>% 
    tab_header(
      title = paste("RILTA Generated & RILTA Analyzed with Transition Probability of", transition_probability)
    ) %>%
    cols_label(
      N_Label = "Sample Size",
      average = "Estimated<br>Probability",
      Coverage = "Coverage",
      Power = "Power",
      Parameter_Bias = "Parameter<br>Bias",
      SE_Bias = "Standard Error<br>Bias",
      .fn = md
    ) %>%
    tab_spanner(
      label = "Bias",
      columns = c("Parameter_Bias", "SE_Bias")) %>%
    tab_row_group(
      label = "Lambda RI Loading of 3 (λ)",  # Label for the first subgroup
      rows = c(21:24)  # Rows corresponding to the first subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 2.5 (λ)",  # Label for the second subgroup
      rows = c(17:20)  # Rows corresponding to the second subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 2 (λ)",  # Label for the third subgroup
      rows = c(13:16)  # Rows corresponding to the third subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 1.5 (λ)",  # Label for the fourth subgroup
      rows = c(9:12)  # Rows corresponding to the fourth subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 1 (λ)",  # Label for the fifth subgroup
      rows = c(5:8)  # Rows corresponding to the fifth subgroup
    ) %>%
    tab_row_group(
      label = "Lambda RI Loading of 0 (λ)",  # Label for the sixth subgroup
      rows = c(1:4)  # Rows corresponding to the sixth subgroup
    ) %>%
      tab_style(
    style = cell_text(
      font = "bold italic"  # Apply bold and italic styling
    ),
      locations = cells_row_groups()  # Apply style to row subheaders
    ) %>% 
    fmt_number(columns = c("Parameter_Bias", "SE_Bias"), decimals = 2) %>%  
    fmt_number(columns = 3, decimals = 3) %>%
    tab_options(
      table_body.hlines.color = "white",
      table.border.top.color = "black",
      table.border.bottom.color = "black",
      table_body.border.bottom.color = "black",
      heading.border.bottom.color = "black",
      column_labels.border.top.color = "black",
      column_labels.border.bottom.color = "black",
      row_group.border.bottom.color = "black" ,
      row_group.border.top.color = "black" 
    ) %>%
    cols_align(
      align = c("center"),
      columns = everything()
    )

  # Apply color highlighting for violations in Parameter Bias
  if (any(!(subset$Parameter_Bias >= -9.99 & subset$Parameter_Bias <= 9.99), na.rm = TRUE)) {
    gt_table <- gt_table %>%
      data_color(
        columns = "Parameter_Bias",
        rows = .data$Parameter_Bias < -9.99 | .data$Parameter_Bias > 9.99,  # Apply color only if outside the threshold
        method = "numeric",
        palette = c("#113386", "#DAE3FA", "#113386"),  # Darker blue for larger deviations
        domain = c(-40, 40)  # Adjust the domain to reflect the range of values
      ) %>%
      tab_footnote(
        footnote = md("Darker blue indicates larger deviations from zero *Parameter Bias* beyond the ±9.99 threshold."),
        locations = cells_column_labels(columns = "Parameter_Bias")
      )
  }

  # Apply color highlighting for violations in SE Bias
  if (any(!(subset$SE_Bias >= -4.99 & subset$SE_Bias <= 4.99), na.rm = TRUE)) {
    gt_table <- gt_table %>%
      data_color(
        columns = "SE_Bias",
        rows = .data$SE_Bias < -4.99 | .data$SE_Bias > 4.99,  # Apply color only if outside the threshold
        method = "numeric",
        palette = c("#B4186E", "#F9D5E9", "#B4186E"),  # Darker red for larger deviations
        domain = c(-80, 80)  # Adjust the domain for the SE_Bias range
      ) %>%
      tab_footnote(
        footnote = md("Darker red indicates larger deviations from zero *Standard Error Bias* beyond the ±4.99 threshold."),
        locations = cells_column_labels(columns = "SE_Bias")
      )
  }

  if (any(subset$Coverage < 0.93 | subset$Coverage > 0.979, na.rm = TRUE)) {
    gt_table <- gt_table %>%
      data_color(
        columns = "Coverage",
        rows = subset$Coverage < 0.93 | subset$Coverage > 0.979,
        method = "numeric",
        palette = c("#93C6B1", "white"),  # Green for coverage issues
        domain = c(0, 1)
      ) %>%
      tab_footnote(
        footnote = md("Green indicates failure to achieve adequate *Coverage*."),
        locations = cells_column_labels(columns = "Coverage")
      )
  }

  if (any(subset$Power < 0.8, na.rm = TRUE)) {
    gt_table <- gt_table %>%
      data_color(
        columns = "Power",
        rows = subset$Power < 0.8,
        method = "numeric",
        palette = c("#502CD1", "white"),  # Purple for power issues
        domain = c(0, 1)
      ) %>%
      tab_footnote(
        footnote = md("Purple indicates failure to achieve adequate *Power*."),
        locations = cells_column_labels(columns = "Power")
      )
  }

  
  return(gt_table)
}

```

#### Render tables for each transition probability condition TABLE FOR TRANSITION PROBABILITIES OF .200

```{r, warning = FALSE}
subset_02_table <- create_table(subset_02, ".200")
subset_02_table
subset_02_table |>  tab_options(table.width = pct(65)) |> gtsave(here('Simulations', 'STUDY_1', '3 Time Points', "zHeatmaps", "z3t_heatmaps","z2t_r_r_heatmaps", "2T_R_R_.200.png"))
```

#### TABLE FOR TRANSITION PROBABILITIES OF .300

```{r, warning = FALSE}
subset_03_table <- create_table(subset_03, ".300")
subset_03_table
subset_03_table |>  tab_options(table.width = pct(65)) |> gtsave(here('Simulations', 'STUDY_1', '3 Time Points', "zHeatmaps", "z3t_heatmaps","z2t_r_r_heatmaps","2T_R_R_.300.png"))
```

#### TABLE FOR TRANSITION PROBABILITIES OF .400

```{r, warning = FALSE}
subset_04_table <- create_table(subset_04, ".400")
subset_04_table
subset_04_table |>  tab_options(table.width = pct(65)) |> gtsave(here('Simulations', 'STUDY_1', '3 Time Points', "zHeatmaps", "z3t_heatmaps","z2t_r_r_heatmaps","2T_R_R_.400.png"))
```

#### TABLE FOR TRANSITION PROBABILITIES OF .600

```{r, warning = FALSE}
subset_06_table <- create_table(subset_06, ".600")
subset_06_table
subset_06_table |>  tab_options(table.width = pct(65)) |> gtsave(here('Simulations', 'STUDY_1', '3 Time Points', "zHeatmaps", "z3t_heatmaps","z2t_r_r_heatmaps","2T_R_R_.600.png"))
```

#### TABLE FOR TRANSITION PROBABILITIES OF .700

```{r, warning = FALSE}
subset_07_table <- create_table(subset_07, ".700")
subset_07_table
subset_07_table |>  tab_options(table.width = pct(65)) |> gtsave(here('Simulations', 'STUDY_1', '3 Time Points', "zHeatmaps", "z3t_heatmaps","z2t_r_r_heatmaps","2T_R_R_.700.png"))
```

#### TABLE FOR TRANSITION PROBABILITIES OF .800

```{r, warning = FALSE}
subset_08_table <- create_table(subset_08, ".800")
subset_08_table
subset_08_table |>  tab_options(table.width = pct(65)) |> gtsave(here('Simulations', 'STUDY_1', '3 Time Points', "zHeatmaps", "z3t_heatmaps","z2t_r_r_heatmaps","2T_R_R_.800.png"))
```
